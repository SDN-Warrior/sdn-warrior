<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
	<channel>
		<title>Posts on SDN-Warrior | Daniel Krieger</title>
		<link>https://sdn-warrior.org/posts/</link>
		<description>Recent content in Posts on SDN-Warrior | Daniel Krieger</description>
		<generator>Hugo -- gohugo.io</generator>
		<language>en-us</language>
		<copyright>Daniel Krieger</copyright>
		<lastBuildDate>Wed, 27 Nov 2024 19:54:18 +0100</lastBuildDate>
		<atom:link href="https://sdn-warrior.org/posts/index.xml" rel="self" type="application/rss+xml" />
		
		<item>
			<title>MAC Learning is your friend</title>
			<link>https://sdn-warrior.org/posts/mac-learning/</link>
			<pubDate>Wed, 27 Nov 2024 19:54:18 +0100</pubDate>
			
			<guid>https://sdn-warrior.org/posts/mac-learning/</guid>
			<description><![CDATA[%!s(<nil>)]]></description>
			<content type="html"><![CDATA[<h2 id="intro">Intro</h2>
<p>When working with nested ESXi environments, understanding the interplay between MAC Learning, Promiscuous Mode, and Forged Transmits is critical. These settings significantly affect how traffic flows in virtualized networks, especially in scenarios involving virtualized hypervisors or advanced network configurations.</p>
<ul>
<li>
<p>MAC Learning: Think of it as a switch-like behavior in your virtual environment. It optimizes network traffic by ensuring that each virtual machine (VM) receives only the packets meant for its MAC address.
Without MAC learning, when the ESXi VM&rsquo;s vNIC connects to a switch port, it only contains a static MAC address.
<a href="https://docs.vmware.com/en/VMware-vSphere/8.0/vsphere-networking/GUID-E0246B3D-9FB1-4976-8217-5C085863EA9A.html">(more Information about MAC learning)</a></p>
</li>
<li>
<p>Promiscuous Mode: On the other hand, this allows a VM or virtual switch to capture all traffic on a port group, whether addressed to it or not. It&rsquo;s a useful feature for troubleshooting and monitoring but comes with potential security and performance implications.</p>
</li>
<li>
<p>Forged Transmits: Forged Transmits plays a complementary role in this configuration. It ensures that traffic originating from a VM with a source MAC address different from its assigned MAC address is allowed to leave the virtual switch. This is crucial in nested environments.</p>
</li>
</ul>
<h2 id="lab-environment">Lab environment</h2>
<p>In this lab environment, I am using two Minisforum MS-01 workstations, each equipped with ESXi 8.0.3 as the hypervisor. These compact systems provide a balance of performance and energy efficiency, fitting perfectly into my goal of maintaining a powerful yet quiet setup.</p>
<p>Each workstation is interconnected via dual 10 Gb/s network links, ensuring high-speed communication with minimal latency. This setup is particularly advantageous for simulating complex network scenarios and nested virtualization environments.</p>
<p>On each workstation, a nested ESXi host is deployed. These nested hosts act as virtualized hypervisors for a future VCF deployment.</p>
<h2 id="the-problems-ive-caused">The problems I&rsquo;ve caused</h2>
<p>In my previous lab setups, Promiscuous Mode was my go-to solution for nested virtualization. It was reliable, simple to configure, and worked flawlessly for years. While I was aware of the security risks associated with it, in a controlled homelab environment, those risks were not a significant concern.</p>
<p>However, everything changed when I upgraded my lab to dual 10 Gb/s network links and, powered by the i9 CPU, gained the ability to run multiple nested ESXi hosts on a single physical machine.
One of the first challenges I encountered was during the configuration of a vSAN port group for my nested ESXi hosts. This port group was configured to use Active/Active load balancing across both 10 Gb/s uplinks on the MS-01 workstations.
Almost immediately, I noticed unexpected performance issues. Nested VMs were experiencing slow network speeds, and vSAN operations were significantly hindered. Initially, I struggled to pinpoint the root cause. Given my past success with Promiscuous Mode, I didn’t suspect it could be contributing to the problem.</p>
<p><a href="https://cybernils.net/2024/11/26/the-effect-of-using-mac-learning-in-esxi-nested-labs/">This article</a> by my fellow vExpert colleague Nils Kristiansen inspired me to delve deeper into the topic.</p>
<h2 id="why-promiscuous-mode-became-a-problem">Why Promiscuous Mode Became a Problem</h2>
<p>The performance degradation stemmed from how traffic was handled with Promiscuous Mode in a dual-uplink, Active/Active configuration:</p>
<ul>
<li>
<p>Broadcasting Traffic Across Both Uplinks: Promiscuous Mode caused the virtual switch to deliver all traffic to every uplink, regardless of the destination. With two high-speed uplinks in an Active/Active configuration, this created excessive overhead, saturating the uplinks and causing packet drops.</p>
</li>
<li>
<p>vSAN’s High Sensitivity to Latency: vSAN traffic is highly dependent on low latency and consistent performance. The unnecessary broadcast of packets interfered with its ability to operate efficiently.</p>
</li>
<li>
<p>Nested Virtualization Amplified the Problem: Nested ESXi hosts added another layer of complexity. The inner VMs were sending and receiving traffic that the parent ESXi host’s virtual switch struggled to handle efficiently under Promiscuous Mode.</p>
</li>
</ul>
<h2 id="ok-but-how-bad-is-the-performance">OK, but how bad is the performance?</h2>
<p>To quantify the performance issues, I turned to iPerf3, a reliable tool for measuring network throughput that is conveniently included in ESXi 8. Using iPerf3, I conducted a series of tests to better understand the extent of the performance degradation.</p>
<h3 id="performance-measurement-1-both-physical-nics-active-nested-hosts-on-the-same-physical-host">Performance Measurement 1: Both Physical NICs Active, Nested Hosts on the Same Physical Host</h3>
<p>For the first test, I configured both pNICs (10 Gb/s) as active in an Active/Active load balancing setup and placed both nested ESXi hosts on the same physical host. Additionally, Promiscuous Mode was enabled on the port group to ensure traffic could flow properly between the nested hosts.</p>
<p><img src="test1.png" alt="Test 1"></p>
<h3 id="results">Results</h3>
<pre tabindex="0"><code>[ ID] Interval           Transfer     Bitrate         Retr
[  5]   0.00-10.00  sec  1.35 GBytes  1.16 Gbits/sec    0            sender  
[  5]   0.00-10.00  sec  1.35 GBytes  1.16 Gbits/sec                 receiver 
</code></pre><h3 id="performance-measurement-2-single-nic-active-nested-hosts-on-the-same-physical-host">Performance Measurement 2: Single NIC Active, Nested Hosts on the Same Physical Host</h3>
<p>For the second test, I modified the setup to use only one physical NIC (pNIC) while keeping both nested ESXi hosts on the same physical host. Promiscuous Mode was still enabled on the port group to ensure traffic routing between the nested hosts. By disabling the second uplink, the traffic path was simplified, reducing potential conflicts.</p>
<p><img src="test2.png" alt="Test 2"></p>
<h3 id="results-1">Results</h3>
<pre tabindex="0"><code>[ ID] Interval           Transfer     Bitrate         Retr
[  5]   0.00-10.00  sec  11.4 GBytes  9.82 Gbits/sec    0            sender
[  5]   0.00-10.01  sec  11.4 GBytes  9.80 Gbits/sec                 receiver
</code></pre><h3 id="performance-measurement-3-mac-learning-and-forged-transmits-dual-uplinks-nested-hosts-on-the-same-physical-host">Performance Measurement 3: MAC Learning and Forged Transmits, Dual Uplinks, Nested Hosts on the Same Physical Host</h3>
<p>For the third test, I switched to using MAC Learning and Forged Transmits, while keeping both physical NICs (pNICs) active in the Active/Active load balancing configuration. Both nested ESXi hosts were still located on the same physical host. This configuration was designed to optimize traffic handling without relying on Promiscuous Mode</p>
<p><img src="test3.png" alt="Test 3"></p>
<h3 id="results-2">Results</h3>
<pre tabindex="0"><code>[ ID] Interval           Transfer     Bitrate         Retr  
[  5]   0.00-10.00  sec  24.2 GBytes  20.8 Gbits/sec    0            sender  
[  5]   0.00-10.01  sec  24.2 GBytes  20.8 Gbits/sec                 receiver  
</code></pre><h3 id="performance-measurement-4-mac-learning-and-forged-transmits-single-uplink-nested-hosts-on-the-same-physical-host">Performance Measurement 4: MAC Learning and Forged Transmits, Single Uplink, Nested Hosts on the Same Physical Host</h3>
<p>For the fourth test, I used a single uplink (pNIC) with both nested ESXi hosts on the same ESXi server. MAC Learning and Forged Transmits were enabled to optimize traffic handling.
The throughput was 20.7 Gbits/sec, almost identical to Test 3. This confirms that, since the traffic did not need to traverse the physical network infrastructure, the single uplink configuration with MAC Learning and Forged Transmits performed just as efficiently, without the overhead of Promiscuous Mode.</p>
<p><img src="test4.png" alt="Test 4"></p>
<h3 id="results-3">Results</h3>
<pre tabindex="0"><code>[ ID] Interval           Transfer     Bitrate         Retr  
[  5]   0.00-10.00  sec  24.2 GBytes  20.8 Gbits/sec    0            sender  
[  5]   0.00-10.01  sec  24.2 GBytes  20.8 Gbits/sec                 receiver  
</code></pre><h3 id="further-performance-measurements-and-security-considerations">Further Performance Measurements and Security Considerations</h3>
<p>Additional performance tests revealed that the difference between Promiscuous Mode and MAC Learning was minimal or even non-existent when the nested hosts were placed on two different physical hosts.
The traffic between the nested VMs did not significantly differ whether Promiscuous Mode or MAC Learning was enabled, indicating that both configurations performed similarly in a multi-host environment.</p>

    <aside class="admonition warning">
        <div class="admonition-title">
            <div class="icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-alert-circle">
      <circle cx="12" cy="12" r="10"></circle>
      <line x1="12" y1="8" x2="12" y2="12"></line>
      <line x1="12" y1="16" x2="12.01" y2="16"></line>
   </svg></div><b>It&#39;s important to note the security implications of using Promiscuous Mode</b>
        </div>
        <div class="admonition-content">Enabling Promiscuous Mode on a network interface allows all traffic to be sent to the VM, even traffic not intended for it, which can expose sensitive data or potentially allow malicious activity.
Because of this security concern, Promiscuous Mode should only be used temporarily, and for troubleshooting purposes, in production environments.
It is recommended to disable it once the issue is resolved to maintain a secure network setup.</div>
    </aside>
<h3 id="side-effect-of-promiscuous-mode-duplicate-packets">Side Effect of Promiscuous Mode: Duplicate Packets</h3>
<p>Enabling Promiscuous Mode on a network interface can lead to duplicate packets when both the source and destination are on the same ESXi host. In this mode, the virtual machine receives all traffic, including its own outbound packets, causing unnecessary duplication. This can result in performance degradation due to increased CPU usage and network inefficiencies.</p>
<pre tabindex="0"><code>[root@esxnuc04:/usr/lib/vmware/vsan/bin] vmkping -I vmk1 192.168.69.203
PING 192.168.69.203 (192.168.69.203): 56 data bytes
64 bytes from 192.168.69.203: icmp_seq=0 ttl=64 time=0.356 ms
64 bytes from 192.168.69.203: icmp_seq=0 ttl=64 time=0.423 ms (DUP!)
64 bytes from 192.168.69.203: icmp_seq=0 ttl=64 time=0.426 ms (DUP!)
64 bytes from 192.168.69.203: icmp_seq=0 ttl=64 time=0.429 ms (DUP!)
64 bytes from 192.168.69.203: icmp_seq=1 ttl=64 time=0.249 ms
64 bytes from 192.168.69.203: icmp_seq=1 ttl=64 time=0.274 ms (DUP!)
64 bytes from 192.168.69.203: icmp_seq=1 ttl=64 time=0.277 ms (DUP!)
64 bytes from 192.168.69.203: icmp_seq=1 ttl=64 time=0.281 ms (DUP!)
64 bytes from 192.168.69.203: icmp_seq=2 ttl=64 time=0.261 ms
</code></pre><h2 id="sidequest-using-iperf-on-esxi-803">Sidequest: Using iPerf on ESXi 8.0.3</h2>
<p>To use iPerf for network performance testing on ESXi 8.0.3, you&rsquo;ll need to follow a few steps to enable and configure the necessary settings.</p>
<ul>
<li>Step 1: Disable the ESXi firewall temporarily
First, disable the ESXi firewall to allow the iPerf tool to operate without restrictions:</li>
</ul>
<pre tabindex="0"><code>esxcli network firewall set --enabled false
</code></pre><ul>
<li>Step 2: Allow executing iPerf
Next, set the system to allow execution of non-installed binaries (such as iPerf), which are not part of the default ESXi installation:</li>
</ul>
<pre tabindex="0"><code>localcli system settings advanced set -o /User/execInstalledOnly -i 0
</code></pre><ul>
<li>Step 3: Execute iPerf (Client example)
Once you&rsquo;ve set the necessary configuration, you can execute iPerf to test the network performance. Use the following command to run iPerf as a client (-c) and specify the target IP address (e.g., 192.168.69.203):</li>
</ul>
<pre tabindex="0"><code>./usr/lib/vmware/vsan/bin/iperf3 -c 192.168.69.203
</code></pre><ul>
<li>Step 4: Re-enable the firewall
Once you’ve finished testing, remember to re-enable the firewall for security reasons:</li>
</ul>
<pre tabindex="0"><code>esxcli network firewall set --enabled true
</code></pre><ul>
<li>Step 5: Restrict execution of non-installed binaries
To revert the system to its default behavior and restrict the execution of non-installed binaries, run the following command:</li>
</ul>
<pre tabindex="0"><code>localcli system settings advanced set -o /User/execInstalledOnly -i 1
</code></pre><h2 id="why-mac-learning-and-forged-transmits-replace-promiscuous-mode-in-nested-environments">Why MAC Learning and Forged Transmits Replace Promiscuous Mode in Nested Environments</h2>
<p>In a typical nested ESXi environment, each inner VM sends packets with its unique MAC address, which the virtual switch on the parent ESXi host does not recognize by default. This creates a challenge because:</p>
<ul>
<li>Without Promiscuous Mode, the switch drops packets destined for or originating from these MAC addresses.</li>
<li>Without Forged Transmits, packets from inner VMs with &ldquo;forged&rdquo; source MAC addresses are also dropped.</li>
</ul>
<h3 id="by-enabling-mac-learning-and-forged-transmits">By enabling MAC Learning and Forged Transmits:</h3>
<p><em><strong>MAC Learning</strong></em> ensures that the virtual switch learns the inner VMs’ MAC addresses dynamically, so it can correctly forward traffic to them without requiring Promiscuous Mode.
<em><strong>Forged Transmits</strong></em> ensures that traffic from inner VMs with different source MAC addresses is allowed to leave the parent VM&rsquo;s vNIC.</p>

    <aside class="admonition info">
        <div class="admonition-title">
            <div class="icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-info">
      <circle cx="12" cy="12" r="10"></circle>
      <line x1="12" y1="16" x2="12" y2="12"></line>
      <line x1="12" y1="8" x2="12.01" y2="8"></line>
   </svg></div><b>Conclusion</b>
        </div>
        <div class="admonition-content">The combination of MAC Learning and Forged Transits removes the need for promiscuous mode, while maintaining:
Better performance, as traffic is only sent where needed.
Stronger security, since traffic is not broadcast unnecessarily.</div>
    </aside>
<p>MAC Learning with Forged Transmits is a significant performance gamechanger, especially when running multiple nested VMs on a single physical ESXi host.
However, it&rsquo;s important to note that MAC Learning with Forged Transmits requires a Distributed Switch. If you&rsquo;re using a Standard Switch, you&rsquo;ll still need to rely on Promiscuous Mode to achieve similar functionality.</p>
]]></content>
		</item>
		
		<item>
			<title>Unraid - A Storage Journey</title>
			<link>https://sdn-warrior.org/posts/unraid-storage/</link>
			<pubDate>Tue, 19 Nov 2024 23:00:00 +0100</pubDate>
			
			<guid>https://sdn-warrior.org/posts/unraid-storage/</guid>
			<description><![CDATA[%!s(<nil>)]]></description>
			<content type="html"><![CDATA[<h2 id="my-custom-unraid-storage-build---flexibility-simplicity-and-future-proofing">My Custom Unraid Storage Build - Flexibility, Simplicity, and Future-Proofing</h2>
<p>As a passionate homelaber, I enjoy not only using technology but also understanding and customizing it to suit my needs. My Unraid storage system is one of my longest-running projects, continuously evolving to meet the demands of my homelab.</p>
<p>After thorough research, I decided to go with Unraid – an operating system renowned for its simplicity, flexibility, and scalability. These key features were the deciding factors for me:</p>
<ul>
<li>Easy Expansion: Unraid allows combining drives of different sizes and expanding the array later without replacing all disks at once.</li>
<li>Docker Integration: The ability to run Docker containers directly on Unraid unlocks immense potential for personal projects and applications.</li>
<li>Versatility: Whether it’s managing data, running a media server, or hosting virtual machines, Unraid offers the freedom to adapt the system to your needs.</li>
</ul>
<p>In this blog post, I’ll share my experience and guide you through how I’ve planned, built, and continuously improved my Unraid storage system. It’s a perfect solution for anyone seeking a scalable, cost-effective setup without sacrificing performance or ease of use.</p>
<h2 id="the-beginning-my-first-steps-with-unraid">The Beginning: My First Steps with Unraid</h2>
<p>Unraid is a commercial product that initially caught my attention due to its unique approach to storage management. Historically, Unraid licenses were available for a one-time purchase, providing lifetime access to its features. Today, however, users can choose between a subscription model or a lifetime license, offering flexibility depending on individual needs.</p>
<p>One of the standout features of Unraid is that it boots directly from a USB stick. This design choice not only simplifies installation but also makes it incredibly easy to replace hardware. Simply move the USB stick to a new machine, and the system is ready to run without the need for extensive reconfiguration.</p>
<p>My first Unraid “server” was far from conventional: a Lenovo notebook powered by an old Intel Dual-Core processor. To build my initial array, I used external USB disks – a true makeshift setup but perfect for testing the waters. For three weeks, this setup served as my proof of concept (POC), allowing me to explore Unraid’s capabilities and ensure it met my needs before committing to more suitable hardware.</p>
<p>This early experimentation confirmed that Unraid was the right choice for my homelab, and I soon began planning and acquiring the components for my first proper build.</p>
<h2 id="building-a-3-tier-performance-storage-system">Building a 3-Tier Performance Storage System</h2>
<p>As my Unraid setup evolved, I implemented a 3-tier performance storage system to meet the varying demands of my homelab. Each tier is tailored for a specific purpose, optimizing the balance between speed, capacity, and efficiency:</p>
<ul>
<li>Tier 1: The Unraid Array (Slow Storage)
At the foundation of my storage system is the Unraid Array, which serves as the slowest but most capacious tier. Unlike traditional RAID, an Unraid Array does not stripe data across disks. Instead, each disk holds individual files, while parity disks provide fault tolerance for data recovery. This unique design allows mixing drives of different sizes, making upgrades straightforward and cost-effective.
My Unraid Array is hosted in an external USB 3.2 storage shelf, which presents each drive individually to Unraid. The shelf delivers enough bandwidth to operate all six 6TB enterprise SATA drives at full speed, ensuring reliable performance even during intensive data access.</li>
</ul>

    <aside class="admonition warning">
        <div class="admonition-title">
            <div class="icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-alert-circle">
      <circle cx="12" cy="12" r="10"></circle>
      <line x1="12" y1="8" x2="12" y2="12"></line>
      <line x1="12" y1="16" x2="12.01" y2="16"></line>
   </svg></div><b>A Quick Warning About Using Unraid with USB Disk Shelves</b>
        </div>
        <div class="admonition-content"><p>It’s important to note that Unraid does not officially recommend running the array on a USB disk shelf, as USB connections can sometimes lead to instability or performance issues. However, my personal experience has shown that it can work reliably with the right hardware.</p>
<p>In my setup, I use a TerraMaster D6-320 USB 3.2 disk shelf, paired with a high-quality USB controller like those found in devices such as Intel NUCs. This combination has proven stable and capable of delivering full-speed performance for all six enterprise SATA drives in my array.
While this setup works well for me, I recommend testing thoroughly in your environment to ensure stability and compatibility before committing to a similar configuration.</p>
</div>
    </aside>
<ul>
<li>
<p>Tier 2: Consumer NVMe Drives (Fast Cache and Docker Storage)
The second performance tier consists of consumer-grade NVMe drives, configured in a btrfs pool within Unraid. This configuration not only allows advanced features like snapshots but also supports RAID levels within the pool, providing a balance between performance and redundancy.
This tier is designed to handle tasks requiring high-speed I/O, such as hosting Docker containers. Keeping Docker data on the NVMe tier ensures that the Unraid Array doesn’t need to spin up unnecessarily, prolonging the life of the disks and improving system responsiveness.
The NVMe drives also serve as a fast cache for incoming data. Files uploaded to the NAS during the day are stored on the NVMe tier and then moved to the slower Unraid Array overnight—except for Docker data, which always remains on the NVMe storage to maintain optimal performance.
Unraid’s flexibility makes it easy to decide whether specific shares or data should stay on the NVMe pool or be automatically moved to the Array on a scheduled basis. This level of control ensures you can optimize storage placement to suit your workload, balancing speed and capacity seamlessly.</p>
</li>
<li>
<p>Tier 3: Enterprise NVMe via iSCSI (Fast and Durable Storage)
The top tier features a 4TB enterprise NVMe drive, designed for high-speed and durable performance under constant load. This storage tier is shared with my homelab servers via iSCSI Multichannel, utilizing a 2x10Gb Intel X710 NIC for redundancy and maximum throughput.
This tier provides fast, reliable storage for workloads that demand consistent performance, such as virtual machines or other critical applications in my homelab. By leveraging enterprise-grade hardware and robust networking, this storage layer ensures low-latency access and can handle the demands of continuous use without compromising reliability.</p>
</li>
</ul>
<h2 id="current-setup">Current Setup</h2>
<p>My Unraid server is built on a Intel NUC Extreme 11th Gen i7 with 64GB of RAM, offering a powerful and compact platform for my homelab. The storage setup includes 2x 1TB consumer-grade NVMe drives for fast cache and 4TB enterprise-grade NVMe for ultra-reliable, high-performance storage.</p>
<p>The Unraid Array has a total capacity of 42TB, with 33.4TB usable for data storage. This provides ample space for both my active projects and long-term storage needs.</p>
<p>On the software side, I host 29 Docker container services and 4 virtual machines, including critical services such as my Active Directory (AD), Certificate Authority (CA), and a Veeam Proxy for file backups. This setup allows for a highly efficient and flexible environment that supports a wide range of use cases while maintaining reliable performance.</p>
<p><img src="unraid2.jpg" alt="Gui"></p>
<h2 id="performance">Performance</h2>
<p>The performance of my Unraid setup was measured using CrystalDiskMark with a 16GB test file (on a Windows 11 VM) to evaluate both sequential and random read and write speeds, as well as IOPS (Input/Output Operations Per Second) of my iSCSI Storage (Tier 3). The results highlight the impressive capabilities of the system:</p>
<p>Read Performance:</p>
<ul>
<li>Sequential Read (Q8T1): 1.993 GB/s | IOPS: 1900.35</li>
<li>Sequential Read (Q1T1): 0.782 GB/s | IOPS: 746.21</li>
<li>Random Read 4K (Q32T1): 0.322 GB/s | IOPS: 78,651.61</li>
<li>Random Read 4K (Q1T1): 0.021 GB/s | IOPS: 5,149.17</li>
</ul>
<p>Write Performance:</p>
<ul>
<li>Sequential Write (Q8T1): 1.247 GB/s | IOPS: 1,189.37</li>
<li>Sequential Write (Q1T1): 0.802 GB/s | IOPS: 764.48</li>
<li>Random Write 4K (Q32T1): 0.298 GB/s | IOPS: 72,776.61</li>
<li>Random Write 4K (Q1T1): 0.036 GB/s | IOPS: 8,835.45</li>
</ul>
<p>These performance metrics demonstrate both the high throughput and responsiveness of the NVMe storage.
The sequential read and write speeds are excellent for large file transfers, while the random IOPS (especially at Q32T1) indicate the drive’s ability to handle a high volume of small, random data requests.
Despite the lower random read/write speeds at Q1T1, the system still shows strong overall performance for a variety of tasks.</p>
<h2 id="understanding-the-crystaldiskmark-test-parameters">Understanding the CrystalDiskMark Test Parameters</h2>
<p>In CrystalDiskMark, several key parameters define how the storage device is tested. Here’s a breakdown of what each test represents:</p>
<p>Q8T1: This stands for Queue Depth 8, Thread 1. It simulates a scenario where 8 data requests are queued up, but only 1 thread (or process) is handling those requests. This type of test is useful for measuring the performance of the storage device when handling multiple sequential tasks at once, but not with excessive parallelism.</p>
<p>Q1T1: This stands for Queue Depth 1, Thread 1. Here, only 1 data request is in the queue, and a single thread handles it. This test represents the performance when a single request is being processed at a time, simulating typical user scenarios where one operation is occurring without significant multitasking.</p>
<p>Q32T1: This stands for Queue Depth 32, Thread 1. In this case, there are 32 queued data requests with a single thread handling them. This test simulates heavy workloads where many data requests are pending, but only one thread is processing them. It can show how the device handles stress under larger, more sustained read operations.</p>
<h3 id="sequential-vs-random-read-tests">Sequential vs. Random Read Tests</h3>
<p>Sequential Read: This test measures how fast the storage device can read large, contiguous chunks of data, like streaming a video or transferring large files. It simulates real-world scenarios where large files need to be read from the storage at a steady rate.</p>
<p>Sequential Read (Q8T1): 1.993 GB/s – This high performance indicates the drive can handle multiple large file read operations quickly, with 8 data requests queued up.
Sequential Read (Q1T1): 0.782 GB/s – This is slower than the Q8T1 test because only one request is processed at a time, simulating less intensive operations.</p>
<p>Random Read: This test measures the performance when the drive has to read small, non-contiguous chunks of data from different parts of the storage. This type of test is more representative of workloads like database operations or running small applications that frequently access different data blocks.</p>
<p>Random Read 4K (Q32T1): 0.322 GB/s – With 32 queued requests and 4KB blocks, this performance shows how the system handles multiple random reads.
Random Read 4K (Q1T1): 0.021 GB/s – Here, only one small request is being handled at a time, leading to slower performance because random 4K reads are typically slower due to the overhead of accessing many different locations on the disk.</p>
<p>These tests give a complete picture of the drive&rsquo;s performance under different scenarios: from high-speed, sequential reads (large files) to more intensive, random access (small files), and with varying levels of workload concurrency.</p>
<h2 id="bom-bill-of-materials">BOM (Bill of Materials)</h2>
<ul>
<li>NUC11DBBi7 , Version M17027-404</li>
<li>2x 32 GB RAM Kingston KF3200C20S4 SODIMM DDR4 Synchronous 3200 MHz</li>
<li>TerraMaster D6-320 USB 3.2(Gen2)</li>
<li>3x TOSHIBA_MG09ACA18TE 18 TB Enterprise SATA</li>
<li>1x TOSHIBA_MG08ADA600E 6 TB Enterpise SATA (to change)</li>
<li>2x Samsung 970 EVO Plus 1TB</li>
<li>1x Samsung MZ1L23T8HBLA-00A07 4 TB Enterprise NVMe 110mm</li>
<li>1x Intel X710 2x 10 Gb/s</li>
<li>1x Good USB Stick (32GB)</li>
</ul>
<p><img src="das.jpg" alt="DAS Disk Array">
<img src="unraid.jpg" alt="Unraid"></p>
<h3 id="fun-fact-my-unraid-server-has-underglow-lighting">Fun fact: My Unraid server has underglow lighting!</h3>
]]></content>
		</item>
		
		<item>
			<title>How to get most out of your Nuc </title>
			<link>https://sdn-warrior.org/posts/nuc/</link>
			<pubDate>Sun, 17 Nov 2024 11:57:43 +0100</pubDate>
			
			<guid>https://sdn-warrior.org/posts/nuc/</guid>
			<description><![CDATA[%!s(<nil>)]]></description>
			<content type="html"><![CDATA[<h2 id="first-things-first">First things first</h2>
<p>Get a second NIC. The Intel NUC Pro has an IO expansion and supports an additional NIC.
Unfortunately, these are relatively difficult to get in Germany, but it&rsquo;s worth the effort.

    <aside class="admonition info">
        <div class="admonition-title">
            <div class="icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-info">
      <circle cx="12" cy="12" r="10"></circle>
      <line x1="12" y1="16" x2="12" y2="12"></line>
      <line x1="12" y1="8" x2="12.01" y2="8"></line>
   </svg></div><b>Search for</b>
        </div>
        <div class="admonition-content">ASUS NUC LAN and USB Expansion Module (90AR0000-P00010)</div>
    </aside></p>
<p>
<figure><picture>
          <source srcset="/NIC_hu6903087498817470336.webp" type="image/webp">
          <source srcset="/NIC_hu4204005753805133783.jpg" type="image/jpeg">
          <img src="/NIC_hu6903087498817470336.webp"alt="Image of an IO expansion"  width="1200"  height="800" />
        </picture><figcaption>
            <p>IO expansion</p>
          </figcaption></figure>
vSphere 8 supports the cards natively and you don&rsquo;t have to install any drivers.
It also supports jumbo frames, which is relevant for NSX Labs.
It is recommended to use a 2.5 GB managed switch. I am using a Mikrotik with the wonderful name <code>CRS326-4C +20G+2Q</code>.</p>

    <aside class="admonition tip">
        <div class="admonition-title">
            <div class="icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-sun">
      <circle cx="12" cy="12" r="5"></circle>
      <line x1="12" y1="1" x2="12" y2="3"></line>
      <line x1="12" y1="21" x2="12" y2="23"></line>
      <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
      <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
      <line x1="1" y1="12" x2="3" y2="12"></line>
      <line x1="21" y1="12" x2="23" y2="12"></line>
      <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
      <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
   </svg></div><b>Practical tip</b>
        </div>
        <div class="admonition-content">My experience with 2.5 Gb/s Lan has shown that it makes sense to set the ports to a fixed speed in the hypervisor and on the switch, otherwise I kept having network failures.</div>
    </aside>
<h2 id="memory-tiering">Memory Tiering</h2>
<p>Memory Tiering is very new in ESXi vSphere 8.0U3 and is still a Tech Preview.
With memory tiering, you can use up to 400% of the physical RAM. This requires a fast NVMe.
I would recommend a PCIe4 NVMe with at least 5000 MB/s read/write.
Memory Tiering stores very cold (unused RAM pages) and cold RAM pages (less than 20% used) on the NVMe (Memory Tier).
There is a wonderful <a href="https://www.vmware.com/explore/video-library/video/6360757998112" title="Explore USA">Explore Session</a> on this.</p>
<p>To enable memory tiering, you have to enter the following commands via the ESX Cli:</p>
<ul>
<li>This command turns on memory tiering</li>
</ul>
<pre tabindex="0"><code>esxcli system settings kernel set -s MemoryTiering -v TRUE
</code></pre><ul>
<li>This command selects the NVMe</li>
</ul>
<pre tabindex="0"><code>esxcli system tierdevice create -d /vmfs/devices/disks/&lt;Your NVME&gt;
</code></pre><ul>
<li>Enter the factor here (0-400%).</li>
</ul>
<pre tabindex="0"><code>esxcli system settings advanced set -o /Mem/TierNvmePct -i 400
</code></pre><p>After a reboot, you have the selected amount of additional memory.</p>

    <aside class="admonition warning">
        <div class="admonition-title">
            <div class="icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-alert-circle">
      <circle cx="12" cy="12" r="10"></circle>
      <line x1="12" y1="8" x2="12" y2="12"></line>
      <line x1="12" y1="16" x2="12.01" y2="16"></line>
   </svg></div><b>Attention</b>
        </div>
        <div class="admonition-content">The selected disk is no longer available to the ESXi.
The minimum capacity must match the selected factor.
If the disk is larger, it will still be used entirely for memory tiering.
My recommendation is to use 1 TB NVMe with 64 GB of physical RAM and 400% as the factor.
ESXi will use the NVMe evenly so that the disk doesn&rsquo;t break as quickly.</div>
    </aside>
<h2 id="using-pe-cores">Using P/E Cores</h2>
<p>Intel has introduced the big.little CPU architecture from the 12th generation of their consumer CPUs. This leads to some problems with ESXi. If the efficiency cores are activated, the ESXi starts with a PSOD (Purble Screen of Death).
Fortunately, there are a few workarounds here.</p>
<ul>
<li>Disable the E cores in the BIOS</li>
</ul>
<p>This means that you can use hyperthreading and the P Cores. However, you are clearly wasting potential here. That&rsquo;s why we don&rsquo;t want to.</p>
<ul>
<li>Use P and E cores and sacrifice hyperthreading for them</li>
</ul>
<p>My tests showed that I got significantly more performance out of my 13th generation i7 if I didn&rsquo;t use hyperthreading and only used “real” CPU cores, even if the E cores have a lower clock rate.
<a href="https://williamlam.com/2023/01/video-of-esxi-install-workaround-for-fatal-cpu-mismatch-on-feature-for-intel-12th-gen-cpus-and-newer.html">William Lam</a> has written very detailed blog articles about this, I link to him here for more information, as this article was actually only intended to be a short summary.</p>
<p>We actually only need two ESX CLI commands to make it all work.</p>
<ul>
<li>With this command, we prevent the PSOD from occurring when the ESXi boots.</li>
</ul>
<pre tabindex="0"><code>esxcli system settings kernel set -s cpuUniformityHardCheckPanic -v FALSE
</code></pre><ul>
<li>With this command, we prevent the ESXi from getting a PSOD when the VMs are switched on.</li>
</ul>
<pre tabindex="0"><code>esxcli system settings kernel set -s ignoreMsrFaults -v TRUE
</code></pre>
    <aside class="admonition tip">
        <div class="admonition-title">
            <div class="icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-sun">
      <circle cx="12" cy="12" r="5"></circle>
      <line x1="12" y1="1" x2="12" y2="3"></line>
      <line x1="12" y1="21" x2="12" y2="23"></line>
      <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
      <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
      <line x1="1" y1="12" x2="3" y2="12"></line>
      <line x1="21" y1="12" x2="23" y2="12"></line>
      <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
      <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
   </svg></div><b>Practical tip</b>
        </div>
        <div class="admonition-content">When reinstalling an ESXi server, I always switch off the E Cores, which saves me from having to manipulate the boot loader. After I have allowed memory tiering and the E/P Cores via the ESX CLI, I switch the E/P Cores back on in the BIOS.</div>
    </aside>
<p>If everything is correct, an ESX NUC of the 13th generation looks like this.

<figure><picture>
          <source srcset="/nuc_hu16244612542378356649.webp" type="image/webp">
          <source srcset="/nuc_hu988947436421053682.jpg" type="image/jpeg">
          <img src="/nuc_hu16244612542378356649.webp"alt="NUC i7"  width="1098"  height="458" />
        </picture><figcaption>
            <p>NUC i7 13th Gen with Memory Tiering and P/E Cores</p>
          </figcaption></figure></p>
]]></content>
		</item>
		
		<item>
			<title>Homelab V4</title>
			<link>https://sdn-warrior.org/posts/labv4/</link>
			<pubDate>Sat, 16 Nov 2024 20:00:00 +0000</pubDate>
			
			<guid>https://sdn-warrior.org/posts/labv4/</guid>
			<description><![CDATA[%!s(<nil>)]]></description>
			<content type="html"><![CDATA[<h2 id="ready-for-vcf">Ready for VCF</h2>
<p>I have done a huge redesign of my Homelab.
To better test VCF scenarios, 3 new Minisforum MS-01 have been added.
These have a 13th generation i9 and are equipped with fast NVMes for memory tiering.
They also have 2x10G and 2x2.5G networking on board for various VM workloads.
Furthermore, I converted my storage from NFS to iSCSI with multipathing, which gets even more performance out of my self-built Unraid.
I manage about 2 GB/s read / 1.2 GB GB/s write and 78K IOPS (Random 4K with 32Q) in a Windows 11 VM.</p>

<figure><picture>
          <source srcset="/bench1_hu2966482509308598586.webp" type="image/webp">
          <source srcset="/bench1_hu2660173491905647916.jpg" type="image/jpeg">
          <img src="/bench1_hu2966482509308598586.webp"alt="Disk Performance iSCSI Multipathing"  width="483"  height="351" />
        </picture><figcaption>
            <p>Disk Performance iSCSI Multipathing</p>
          </figcaption></figure>

<figure><picture>
          <source srcset="/bench2_hu3593698816181082095.webp" type="image/webp">
          <source srcset="/bench2_hu10297252823232288012.jpg" type="image/jpeg">
          <img src="/bench2_hu3593698816181082095.webp"alt="IOPS iSCSI Multipathing"  width="483"  height="356" />
        </picture><figcaption>
            <p>IOPS iSCSI Multipathing</p>
          </figcaption></figure>
<p>Pretty impressive for my setup. I still have to customize the rack a bit so that I can add the 10G Mikrotik switch and clean up the VLANs from old labs.
I&rsquo;m already planning a further expansion stage though.\</p>
]]></content>
		</item>
		
		<item>
			<title>NSX Integration Fortigate</title>
			<link>https://sdn-warrior.org/posts/nsx-integration-fortigate/</link>
			<pubDate>Mon, 26 Aug 2024 19:49:23 +0100</pubDate>
			
			<guid>https://sdn-warrior.org/posts/nsx-integration-fortigate/</guid>
			<description><![CDATA[%!s(<nil>)]]></description>
			<content type="html"><![CDATA[<h2 id="nsx-integration-for-fortinet-fortigate-firewalls">NSX integration for Fortinet Fortigate Firewalls</h2>
<p>Modern SDN solutions are flexible, fast and effective. The rules of the classic perimeter firewall should be exactly the same. To make life easier, Fortinet has an NSX integration that allows us to write our perimeter rules to dynamic NSX groups.</p>
<h2 id="first-things-first">First things first</h2>
<p>The Fortinet NSX integration works via a so-called external connector. For this purpose, the Fortigate contacts the NSX Manager at regular intervals and updates the previously imported groups.
This allows us to use dynamic groups that were previously created in NSX using tags, for example.</p>
<p>First we need to configure our connector. To do this, go to the Fortigate at <em><strong>Security Fabric / External Connectors</strong></em> and click on <em><strong>Create New</strong></em>.</p>
<p><img src="01.webp" alt="Fortigate Dialog"></p>
<p>Here we need to enter our NSX Manager, if we have an NSX Manager Cluster then of course the Cluster VIP or FQDN is needed.
We can define an update interval, this determines how long it takes to update the groups on the Fortigate.
In my lab I chose 30 seconds, depending on the environment lower or higher values may make sense. In a productive environment, the certificate should always be verified.
In my homelab environment I deliberately turned this off.</p>
<p><img src="02_External-Connector.webp" alt="External Connector"></p>
<h2 id="importing-the-dynamic-nsx-groups">Importing the dynamic NSX groups</h2>
<p>The groups need to be imported via the Fortigate CLI. This is relatively easy to do for all groups and specifically for individual groups.
Groups imported this way will be automatically updated in the future. If new groups are configured in NSX, they must be imported via the CLI if they play a role in the rules.</p>

    <aside class="admonition info">
        <div class="admonition-title">
            <div class="icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-info">
      <circle cx="12" cy="12" r="10"></circle>
      <line x1="12" y1="16" x2="12" y2="12"></line>
      <line x1="12" y1="8" x2="12.01" y2="8"></line>
   </svg></div><b>Fortigate CLI</b>
        </div>
        <div class="admonition-content"><code>execute nsx group import &lt;SDN Connector&gt; &lt;VDOM&gt; &lt;group&gt;</code></div>
    </aside>
<p>If you want to import all NSX groups, you need to omit the group name in the CLI call. In the screenshot you can see me importing the <em><strong>dFG_AlpineLinux</strong></em> NSX group.
This uses an NSX tag to combine all VMs of type Alpine Linux into one security group.</p>
<p><img src="03_Group-Import.png" alt="Group-Import"></p>
<p>In the Fortigate, you can now find the group under <em><strong>Policy &amp; Objects / Addresses</strong></em> and use it like any other group in firewall policies. The NSX groups can be used not only for firewall rules, but also for policy-based routing via the SD-WAN feature.</p>
<p><img src="04_FW-Groups.webp" alt="Firewall Groups"></p>
<p>In my example, I am prohibiting the Alpine Linux VMs from accessing the Internet. The current realised group assignment can be checked at any time via <em><strong>Policy &amp; Objects&gt; / Addresses</strong></em> and a double click on the group.</p>
<p><img src="05_matched-adress.webp" alt="Matched Adrewss"></p>
<h2 id="delete-groups">Delete groups</h2>
<p>Groups need to be deleted manually. The easiest way to do this is via the Fortigate CLI. To do this, execute the following CLI command:</p>

    <aside class="admonition info">
        <div class="admonition-title">
            <div class="icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-info">
      <circle cx="12" cy="12" r="10"></circle>
      <line x1="12" y1="16" x2="12" y2="12"></line>
      <line x1="12" y1="8" x2="12.01" y2="8"></line>
   </svg></div><b>Fortigate CLI</b>
        </div>
        <div class="admonition-content"><code>execute nsx group delete &lt;VDOM&gt; &lt;filter&gt;</code></div>
    </aside>
<p>If you want to delete all groups, you can simply leave the filter empty. If a group is used in a firewall policy, it cannot be deleted and you will receive a message that the group is in use.</p>
<h2 id="testing-the-solution">Testing the solution</h2>
<p>To do this, I log on to the Alpine2 VM and check the current IP. The VM has currently been assigned 172.31.2.10. We can also find this on the Fortigate in our dFG_AlpineLinux group. I am trying to send an ICMP to the Internet, which is blocked by the Fortigate firewall as expected.</p>
<p><img src="06_test-1.webp" alt="First Test"></p>
<p>Next, I remove the Alpine Linux tag in the NSX, which ensures that the VM is no longer realised in the dFG_Alpine Linux group on the Fortigate after 30 seconds at the latest.</p>
<p><img src="07_test-2.webp" alt="Second Test"></p>
<p>Finally, I repeated my ping test. As expected, Internet access is now without any problems.</p>
<p><img src="08_icmp.png" alt="Test Number three"></p>
<h2 id="remarks">Remarks</h2>
<p>If the connection to NSX Manager is interrupted, group membership remains at the last synchronised state. This means that in highly dynamic environments, too much or too little traffic may be allowed or blocked. For this reason, the SDN connection should always be monitored. All group changes are saved in the Log SDN Connector Log of the Fortigate.</p>
<h2 id="use-cases">Use cases</h2>
<p>One conceivable scenario would be to enable a dynamic firewall for VMs that are allowed to access the Internet. This can be done in NSX using a tag and a group. Every VM that does not have a tag and is therefore not in the group will be blocked at the Fortigate perimeter firewall.</p>
<p><img src="09_firewall_rule.webp" alt="Firewall Rules"></p>
<p>The firewall rule allows everything that does not go into RFC1918 networks (private IP range). Of course, this is only a simple example and more complex setups are possible.</p>
<h2 id="additional-information">Additional information</h2>
<p><a href="https://docs.fortinet.com/document/fortigate/7.4.4/administration-guide/753961/public-and-private-sdn-connectors">Fortinet Documentation: Public and private SDN connectors</a></p>
]]></content>
		</item>
		
		<item>
			<title>NSX Identity Firewall – A Case Study With the Flavour VDI</title>
			<link>https://sdn-warrior.org/posts/nsx-idfw-vdi/</link>
			<pubDate>Fri, 02 Aug 2024 08:34:23 +0100</pubDate>
			
			<guid>https://sdn-warrior.org/posts/nsx-idfw-vdi/</guid>
			<description><![CDATA[%!s(<nil>)]]></description>
			<content type="html"><![CDATA[
    <aside class="admonition info">
        <div class="admonition-title">
            <div class="icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-info">
      <circle cx="12" cy="12" r="10"></circle>
      <line x1="12" y1="16" x2="12" y2="12"></line>
      <line x1="12" y1="8" x2="12.01" y2="8"></line>
   </svg></div><b>Disclaimer</b>
        </div>
        <div class="admonition-content">There are of course other ways of using the Identity Firewall. This is the way I have used it with my customers so far. Of course, the whole thing is colored by personal preferences, experiences and customer requirements so take this as an idea for your own implementations.</div>
    </aside>
<h2 id="intro">Intro</h2>
<p>A customer of mine has the use case that his entire environment must be micro-segmented. Of course, this does not stop at the VDI environment. Since my customer uses non-persistent VDIs (the VMs are deleted after each logoff), no tags can be used for the security groups. After deleting the VM, the tag would also be removed and a new VDI would have no tags and would thus be isolated. This can be solved with automation or with the Identity Firewall (a combination of both solutions is also conceivable and makes sense). In the first step, my customer opted for the Identity Firewall because it allows generic VDIs to be used and authorisations to resources can be conveniently controlled via the customer AD. In addition, each user/user group receives individual firewall rules, which corresponds to the need-to-know principle.</p>
<h3 id="you-can-use-the-identity-firewall-in-2-ways">You can use the Identity Firewall in 2 ways:</h3>
<ul>
<li>Variant 1 would be with VMware Tools and Guest Introspection.</li>
<li>Variant 2 would be with log scraping and, for example, Aria Operations for Logs as a log server.</li>
</ul>
<p>I use variant 1 because the implementation means less effort in my customer setup and we only want to protect the VDIs with Idendity Firewall.</p>
<h3 id="limitations">Limitations</h3>
<ul>
<li>No User /Group ID Support for Federation.</li>
<li>No direct integration with VDI and RDSH.</li>
<li>User-ID based rules are supported for only firewall rules.</li>
<li>No User-ID based policy for IDS/IPS and TLS Inspection.</li>
<li>Multi-User (RDSH) does not support Server Message Block (SMP) protocol.</li>
</ul>
<h3 id="supported-protocols">Supported protocols</h3>
<ul>
<li>Single user (VDI, or Non-RDSH Server) use case support – TCP, UDP</li>
<li>Multi-User (RDSH) use case support – TCP, UDP</li>
</ul>
<h3 id="supported-clients">Supported clients</h3>
<ul>
<li>Windows 8,10,11</li>
<li>Windows Server 2012 – 2022</li>
</ul>
<h3 id="what-is-needed">What is needed?</h3>
<ul>
<li>NSX in the VDI cluster</li>
<li>AD infrastructure</li>
<li>VMware tools</li>
<li>VMware Aria Operations for Logs (optional)</li>
</ul>
<h2 id="first-things-first">First things first</h2>
<p>Identity based groups can only be used as a source for a firewall rule. In addition, the rules only come into effect after a successful logon to the client. Thus, normal dFW rules must be written for all communication that happens before the user logs on. This applies, for example, to Windows AD communication. The NSX Manager synchronises cyclically with the domain controllers. The default interval is 180 minutes. If changes are made to the group membership at short notice, a manual sync can be performed. Alternatively, the sync interval can also be shortened or extended. Using network introspection, the NSX Manager recognises when a user logs on to a client and can perform matching with the AD groups and thus add the client dynamically to the security group of the IDFW firewall rule.</p>

<figure><picture>
          <source srcset="/idfw/00_idfw_hu13340240812954672917.webp" type="image/webp">
          <source srcset="/idfw/00_idfw_hu2021098600553032142.jpg" type="image/jpeg">
          <img src="/idfw/00_idfw_hu13340240812954672917.webp"alt="Idendity Firewall function"  width="1782"  height="1164" />
        </picture><figcaption>
            <p>Idendity Firewall function</p>
          </figcaption></figure>
<h2 id="getting-started">Getting started</h2>
<p>Firstly, I start by customising the golden images of the VDI and installing NSX Guest Introspection. These are not installed by default and have to be installed explicitly. You can find them under VMware Device Driver – NSX Network Introspection. File Introspection is installed automatically.</p>

<figure><picture>
          <source srcset="/idfw/01_vmwaretools_hu10766174744744931261.webp" type="image/webp">
          <source srcset="/idfw/01_vmwaretools_hu2019103790225014772.jpeg" type="image/jpeg">
          <img src="/idfw/01_vmwaretools_hu10766174744744931261.webp"alt="VMware Tools"  width="555"  height="417" />
        </picture><figcaption>
            <p>VMware Tools</p>
          </figcaption></figure>
<p>Once Guestintrospection has been successfully installed, we no longer need to do anything on our Windows clients. Next, the domain must be integrated.</p>
<p>This is done in the NSX Manager under System -&gt; Identity Firewall AD. Several domains can be entered so that multi-tenant setups can also be realised. The NSX Manager requires firewall activations and must be able to reach the domain controllers via LDAP or LDAPS. I strongly recommend the use of LDAPS. These settings can also be used to perform a manual sync or check the synchronisation status.</p>

<figure><a href="02_nsx_idfw.jpeg"><picture>
          <source srcset="/idfw/02_nsx_idfw_hu5496888141303419910.webp" type="image/webp">
          <source srcset="/idfw/02_nsx_idfw_hu10295445750369093658.jpeg" type="image/jpeg">
          <img src="/idfw/02_nsx_idfw_hu5496888141303419910.webp"alt="Idetity firewall AD settings"  width="1452"  height="465" />
        </picture></a><figcaption>
            <p>Idetity firewall AD settings (click to enlarge)</p>
          </figcaption></figure>
<p>Under LDAP Server you can set several domain controllers for the previously set up domain. The protocol used is also selected here. I use the Domain Administrator in my lab. In a productive environment, an LDAP bind user should always be used.</p>

<figure><a href="03.jpeg"><picture>
          <source srcset="/idfw/03_hu16733942988963753118.webp" type="image/webp">
          <source srcset="/idfw/03_hu2839745583667809871.jpeg" type="image/jpeg">
          <img src="/idfw/03_hu16733942988963753118.webp"alt="LDAP Server settings"  width="1160"  height="460" />
        </picture></a><figcaption>
            <p>LDAP Server settings (click to enlarge)</p>
          </figcaption></figure>

    <aside class="admonition attention">
        <div class="admonition-title">
            <div class="icon"><svg xmlns="http://www.w3.org/2000/svg" class="feather feather-link" width="24" height="24" viewBox="0 0 24 24"
      fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
      <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path>
      <path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path>
   </svg></div><b>Attention</b>
        </div>
        <div class="admonition-content">If LDAPS is used, NSX imports the SHA thumbprint of the domain controller certificate. As the certificate is usually renewed automatically after 2 years at the latest, NSX loses the trust with the domain controller. In this case, the trust must be established manually. To do this, delete the thumbprint and reconnect to the bind user.  It has proven to be practical to monitor certificate expiry times and to enter at least 2 domain controllers that exchange their certificates with a 4-week time lag. If the trust fails completely, no more identity rules are applied and the default firewall rule comes into effect. In practice, this should be an any/any default drop and log rule.</div>
    </aside>
<p>After we have successfully setup and synchronised our domain, we only need to activate the Identity Firewall. By default, this feature is disabled (it is a free feature that is available with the NSX Firewall VCF add-on). To activate the Identity Firewall, go to Security -&gt; Distributed Firewall -&gt; Settings -&gt; Identity Firewall Settings and activate the identity firewall service button. Then we select the cluster on which we want to activate the service. Now we can get started.</p>

<figure><a href="04.webp"><picture>
          <source srcset="/idfw/04_hu12560464345806440685.webp" type="image/webp">
          <source srcset="/idfw/04_hu11069272111777865810.jpg" type="image/jpeg">
          <img src="/idfw/04_hu12560464345806440685.webp"alt="Distributed Firewall Settings"  width="1699"  height="792" />
        </picture></a><figcaption>
            <p>Distributed Firewall Settings (click to enlarge)</p>
          </figcaption></figure>
<h2 id="identity-firewall-rules">Identity Firewall Rules</h2>
<p>A general recommendation that applies to all firewalls is to think about a naming concept. At my customer, we have different name prefixes for the various security groups in NSX or in the other firewalls. For my part, I prefer the following naming convention, for example:</p>
<p>Distributed firewall groups start with dFW_XXX, an LDAP backed security group with dFWU_XXX (the U stands for user). For a group that contains an NSX segment it would be dFWS_XXX and for the gateway firewall a gWF_XXX and so on.</p>
<p>So we create our first LDAP user group. As with any group, we can do this either when creating the rules or in the inventory under Groups. The process is the same as for a normal Distributed Firewall Group, except that we don’t use tags but Distinguished Names, which can be conveniently selected or filtered from the synchronised AD elements.</p>

<figure><a href="05.webp"><picture>
          <source srcset="/idfw/05_hu16308509706422833611.webp" type="image/webp">
          <source srcset="/idfw/05_hu13581943069617237554.jpg" type="image/jpeg">
          <img src="/idfw/05_hu16308509706422833611.webp"alt="Security Groups"  width="1164"  height="977" />
        </picture></a><figcaption>
            <p>Security Groups (click to enlarge)</p>
          </figcaption></figure>
<p>I also need at least one segment group and at least one group containing my target servers. The target servers are assigned to their groups via tags. The same applies to the segment group. To do this, I set one or more tags on the overlay segment and use this tag as a condition for group membership.</p>

<figure><a href="06.webp"><picture>
          <source srcset="/idfw/06_hu566842021385844307.webp" type="image/webp">
          <source srcset="/idfw/06_hu1670820342853564742.jpg" type="image/jpeg">
          <img src="/idfw/06_hu566842021385844307.webp"alt="Member selection"  width="1156"  height="964" />
        </picture></a><figcaption>
            <p>Member selection (click to enlarge)</p>
          </figcaption></figure>
<p>Our goal will be that we authorise our users assigned to dFWU_UserGroup1 to access our fileserver with SMB and the users of the group dFWU_UserGroup2 must not receive any authorisation. I have two domain users in my lab, User1 is in the AD group assigned to dFWU_UserGroup1 and User2 is only assigned to dFWU_UserGroup2.</p>
<h2 id="creating-the-firewall-rules">Creating the firewall rules</h2>
<p>For each identity firewall rule that allows traffic from a group of users to a destination, there must be a corresponding distributed firewall rule that allows traffic from a group of computers to the same destination specified in the identity firewall rule. We therefore need two firewall rules.</p>

<figure><a href="07.webp"><picture>
          <source srcset="/idfw/07_hu14371369714500250916.webp" type="image/webp">
          <source srcset="/idfw/07_hu7421683099691664086.jpg" type="image/jpeg">
          <img src="/idfw/07_hu14371369714500250916.webp"alt="Firewall Rules"  width="1630"  height="226" />
        </picture></a><figcaption>
            <p>Firewall Rules (click to enlarge)</p>
          </figcaption></figure>
<p>The first rule is pretty straight forward, as source we have our dFWU_UserGroup1, the target is our dFG_Fileserver and the service is SMB. The Applied To Field is even more important than usual for the Identity Firewall. We may only apply this rule to our VDIs. Since my customer has different pools that are named according to a specific naming scheme, I can further restrict the scope based on the computer name. Each pool has different rules and we only want the rules to be realised on VMs where they are needed. The second rule is a bit more interesting. As a source, we have our VDI segment or segments. As in the first rule, the target is our file server. Logically, the service is also the same.</p>

    <aside class="admonition attention">
        <div class="admonition-title">
            <div class="icon"><svg xmlns="http://www.w3.org/2000/svg" class="feather feather-link" width="24" height="24" viewBox="0 0 24 24"
      fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
      <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path>
      <path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path>
   </svg></div><b>Attention</b>
        </div>
        <div class="admonition-content">It is important that the Apply To field can only be on the file servers or on the target that we want to enable. If we were to use the dFG_VDI_GroupA or the dFGS_VDI group here, for example, then the entire Identity Firewall Rule is cancelled out!</div>
    </aside>
<h2 id="testing-and-verifying">Testing and verifying</h2>
<p>The test is considered successful if User1 on the TestVDI can establish a successful SMB connection to the file server. If User2 is used instead of User1, the traffic to the file server must be blocked by the firewall.</p>
<p>For testing, I log in to my VDI with the credentials of User1 and perform a TestNetConnection with Powershell. This is a simple and quick way to test TCP connections. I also open a share on the file server.</p>

<figure><a href="08.webp"><picture>
          <source srcset="/idfw/08_hu11227237490731539297.webp" type="image/webp">
          <source srcset="/idfw/08_hu11206926558301644657.jpg" type="image/jpeg">
          <img src="/idfw/08_hu11227237490731539297.webp"alt="Network test user 1"  width="1072"  height="627" />
        </picture></a><figcaption>
            <p>Network test user 1 (click to enlarge)</p>
          </figcaption></figure>
<p>The test was successful, both the TNC command and the actual opening of the file share worked. Now I’m running the same test on the same VDI (after it was recreated, because non-persistent VDI), only this time I’m using User2, which has no explicit firewall rules and is therefore blocked by my default cleanup rule. As expected, the traffic was successfully blocked.</p>

<figure><a href="09.png"><picture>
          <source srcset="/idfw/09_hu2338535551923653157.webp" type="image/webp">
          <source srcset="/idfw/09_hu1977019721799240822.jpg" type="image/jpeg">
          <img src="/idfw/09_hu2338535551923653157.webp"alt="Network test user 2"  width="993"  height="499" />
        </picture></a><figcaption>
            <p>Network test user 2 (click to enlarge)</p>
          </figcaption></figure>
<h2 id="lessons-learned">Lessons learned</h2>
<p>This is where I would like to add a few more thoughts on the subject. Troubleshooting is more difficult in practice than I thought. Tools such as NSX Traceflow cannot be used because you cannot add an AD user to the request. This means that the traffic in the traceflow is always dropped or the identity rule is maybe configured incorrectly.</p>
<p>But there is light at the end of the tunnel. In NSX 4.X there is a session view of the active IDFW user session under Security -&gt; Security Overview -&gt; Configuration. All active sessions, UserIDs and VMs are displayed here, as well as the source of the information.</p>

<figure><a href="10.png"><picture>
          <source srcset="/idfw/10_hu10916967050054961034.webp" type="image/webp">
          <source srcset="/idfw/10_hu10504170639179592038.jpg" type="image/jpeg">
          <img src="/idfw/10_hu10916967050054961034.webp"alt="Active Sessions"  width="979"  height="278" />
        </picture></a><figcaption>
            <p>Active Sessions (click to enlarge)</p>
          </figcaption></figure>
<p>Next tip would be to always check the sync status with the AD. Ask your AD admin when the user was added to the group. If the user has several accounts, ask for the user name used. Experience has shown that this is where most problems occur.</p>
<p>Use a syslog server and check exactly with which rule ID the traffic was discarded. Have all deny rules logged.</p>
<p>Not all rules can be implemented as Identity Firewall Rules. The Windows domain basic communication can only be enabled via a classic set of rules, as no Identity Firewall rules are active for the VM without an active user session.</p>
<h2 id="important-things-to-know">Important things to know</h2>
<p>Never install Guest Introspection on a target. If a user has remote desktop permissions on the target and guest introspection is active there, then the target receives all of the user’s firewall rules. This can lead to unwanted firewall permissions.</p>
<p>If targets outside of NSX are addressed, such as a NAS or legacy infrastructure, a second rule is not required (unless the gateway firewall is also used). In this case, the distributed firewall will only check the traffic at the source VM.</p>
<p>Any change on a domain, including a domain name change, will trigger a full sync with Active Directory. Because a full sync can take a long time, i recommend syncing during off-peak or non-business hours.</p>
<p>MutiUser setups only work with RDSH (Remote Desktop Session Hosts) which requires a special configuration. Otherwise, if several users are logged on to a client at the same time, this leads to unwanted behavior and, in the worst case, to unwanted firewall permissions.</p>
<h2 id="conclusion">Conclusion</h2>
<p>The Identity Firewall is a wonderful extension of the Distributed firewall and should be treated as such. Used correctly, it provides a very nice way to manage and delegate firewall permissions dynamically and centrally for individual users or user groups. It enables generic VDIs that can be used for different purposes depending on the user. This can reduce the number of VDI pools required, which in turn makes it easier to manage the customers VDI environment. The RBAC concept is even more tightly bound to the firewall policies and AD tiering can also be enforced via the firewall. And best of all, this great feature is included in the NSX Firewall license. I would recommend every NSX firewall administrator to take a closer look at the Identity Firewall.</p>
<h2 id="additional-resources">Additional resources</h2>
<p><a href="https://docs.vmware.com/en/VMware-NSX/4.1/administration/GUID-9CD3FC21-9ED4-4FB3-9E19-67A7C4D1F53E.html">VMware Docs Idendity Firewall</a></p>
]]></content>
		</item>
		
		<item>
			<title>NSX 4.X Certificate exchange of the NSX Manager</title>
			<link>https://sdn-warrior.org/posts/nsx-cert-exchange/</link>
			<pubDate>Fri, 05 Apr 2024 23:22:00 +0100</pubDate>
			
			<guid>https://sdn-warrior.org/posts/nsx-cert-exchange/</guid>
			<description><![CDATA[%!s(<nil>)]]></description>
			<content type="html"><![CDATA[<h1 id="nsx-4x-certificate-exchange-of-the-nsx-manager">NSX 4.X Certificate exchange of the NSX Manager</h1>
<h1 id="certificate-creation">Certificate creation</h1>
<p>First of all, we need a CSR request. This can be created with OPENSSL. It is important that the key is also exported. You can either create 4 individual certificates (VIP and the three manager nodes) or a SAN certificate with all DNS and IP names of the manager nodes. The easiest way is to carry out the request on a manager node. To do this, I create an openssl config file with VIM.</p>
<pre tabindex="0"><code>[req]
default_bits = 4096
default_md = sha256
days = 365
distinguished_name = req_distinguished_name
req_extensions = v3_req
prompt = no
 
[req_distinguished_name]
C   = DE
ST  = RLP
L   = NW
O   = Land RLP
OU  = sdnwarrior
CN  = nsxm0001.lab.home
emailAddress = mail@lab.home
 
[v3_req]
subjectAltName = @sans
 
[sans]
DNS.1 = nsxm0001.lab.home
DNS.2 = nsxm0002.lab.home
DNS.3 = nsxm0003.lab.home
DNS.4 = nsxm0004.lab.home
IP.1 = 192.168.12.110
IP.2 = 192.168.12.111
IP.3 = 192.168.12.112
IP.4 = 192.168.12.113
</code></pre><p>The CSR is generated with the following command:</p>
<pre tabindex="0"><code>openssl req -new -newkey rsa:4096 -nodes -keyout nsxm0001.key -out nsxm0001.csr -config opnssl.cnf
</code></pre><p>Two files are generated, a private key file and the actual request, which must be submitted to the CA.</p>

    <aside class="admonition attention">
        <div class="admonition-title">
            <div class="icon"><svg xmlns="http://www.w3.org/2000/svg" class="feather feather-link" width="24" height="24" viewBox="0 0 24 24"
      fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
      <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path>
      <path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path>
   </svg></div><b>Attention</b>
        </div>
        <div class="admonition-content">The CA must issue the certificate with the extension basicConstraints = cA:FALSE, otherwise the certificate cannot be used. With a Windows CA, this must be explicitly permitted in the template. If the extension is missing, the certificate validation will fail with an error message that the certificate key does not match the certificate.</div>
    </aside>
<h2 id="import-certificate">Import certificate</h2>
<p>The certificate can be imported in the NSX Manager under System &gt; Certificates &gt; Import. Here it must be ensured that the service certificate slider is set to NO. The complete certificate chain is also required. The certificate chain must be in the industry standard order of ‘certificate – intermediate – root.</p>

<figure><picture>
          <source srcset="/nsx-cert/01_hu8747713429439574210.webp" type="image/webp">
          <source srcset="/nsx-cert/01_hu1064617936367254404.jpg" type="image/jpeg">
          <img src="/nsx-cert/01_hu8747713429439574210.webp"alt="NSX Cert"  width="582"  height="924" />
        </picture><figcaption>
            <p>Import NSX Cert</p>
          </figcaption></figure>
<p>After the import, the certificate can be validated using an API request.
API calls may vary depending on the NSX-T versions, in my example NSX version 4.1.2.3 is used.</p>
<pre tabindex="0"><code>GET https://&lt;nsx-mgr&gt;/api/v1/trust- management/certificates/&lt;cert-id&gt;?action=validate
</code></pre><h2 id="exchange-of-certificates">Exchange of certificates</h2>
<p>An API request must be executed for each manager node and for the VIP. This requires the certificate ID and the manager node ID. Both can be copied from the WebGUI or requested via API Get Requests.</p>
<p>The following API call is used to exchange the Manager Node certificate:</p>
<pre tabindex="0"><code>POST /api/v1/trust-management/certificates/&lt;cert- id&gt;?action=apply_certificate&amp;service_type=API&amp;node_id=&lt;node- id&gt;
</code></pre><p>The following API call is used to exchange the cluster VIP certificate:</p>
<pre tabindex="0"><code>POST /api/v1/trust-management/certificates/&lt;cert- id&gt;?action=apply_certificate&amp;service_type=MGMT_CLUSTER
</code></pre><p>After replacing the certificates, you should close all browser windows and log in to the NSX Manager again. The certificate should now have been successfully replaced.</p>
<h2 id="further-resources">Further resources:</h2>
<p><a href="https://docs.vmware.com/en/VMware-NSX-T-Data-Center/3.2/administration/GUID-50C36862-A29D-48FA-8CE7-697E64E10E37.html">VMware Administration Handbook</a></p>
]]></content>
		</item>
		
	</channel>
</rss>
