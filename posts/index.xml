<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
	<channel>
		<title>Posts on SDN-Warrior | Daniel Krieger</title>
		<link>https://sdn-warrior.org/posts/</link>
		<description>Recent content in Posts on SDN-Warrior | Daniel Krieger</description>
		<generator>Hugo -- gohugo.io</generator>
		<language>en-us</language>
		<copyright>Daniel Krieger</copyright>
		<lastBuildDate>Fri, 20 Dec 2024 02:00:04 +0100</lastBuildDate>
		<atom:link href="https://sdn-warrior.org/posts/index.xml" rel="self" type="application/rss+xml" />
		
		<item>
			<title>IPAM Automation with NetBox, Ansible, and Microsoft Windows DNS Serve</title>
			<link>https://sdn-warrior.org/posts/ipam-automation/</link>
			<pubDate>Fri, 20 Dec 2024 02:00:04 +0100</pubDate>
			
			<guid>https://sdn-warrior.org/posts/ipam-automation/</guid>
			<description><![CDATA[IPAM Automation with Netbox and Ansible]]></description>
			<content type="html"><![CDATA[<h2 id="introduction">Introduction</h2>
<p>Managing IP addresses and DNS records manually can be a daunting task, especially in dynamic IT environments. This blog post demonstrates how to leverage NetBox, Ansible, and Microsoft Windows DNS Server to automate IP Address Management (IPAM) and DNS record updates, making your infrastructure more efficient and reliable.</p>
<h2 id="why-automate-ipam-and-dns">Why Automate IPAM and DNS?</h2>
<ul>
<li>Consistency: Automation minimizes human errors and ensures uniformity.</li>
<li>Efficiency: Automating repetitive tasks saves time and allows teams to focus on strategic activities.</li>
<li>Scalability: As networks grow, automated solutions adapt more easily than manual processes.</li>
</ul>
<h2 id="my-goal">My goal</h2>
<ul>
<li>Get a free IP address is dynamically fetched from a defined subnet in NetBox.</li>
<li>The IP address is immediately assigned to the specified FQDN in NetBox.</li>
<li>A corresponding Host A record is created in your Windows DNS Server.</li>
</ul>
<h2 id="prerequisites">Prerequisites</h2>
<p>Before diving into the implementation, ensure the following:</p>
<ul>
<li>A functional NetBox instance configured with appropriate IPAM data.</li>
<li>A Microsoft Windows DNS Server with administrative access.</li>
<li>Ansible installed and configured on a control node.</li>
<li>API access credentials for NetBox.</li>
<li>pywinrm Python module</li>
<li>PowerShell Remoting</li>
</ul>
<h2 id="ansible-project">Ansible Project</h2>
<p>For this automation project, I structured my workflow into multiple steps to keep it organized and modular. I use an ansible.cfg file to integrate and manage my inventory. At the core of the setup is a master playbook, which orchestrates the entire automation process.</p>
<p>To simplify and separate concerns, I divided the tasks into two sub-playbooks:</p>
<p>NetBox playbook: Handles all interactions with NetBox, such as fetching available IPs or updating DNS-related metadata.
DNS playbook: Focuses on managing DNS records on my Microsoft Windows DNS Server.
This approach not only makes the automation workflow easier to manage but also allows me to test and modify individual components independently while maintaining a clear overview of the entire process through the master playbook.</p>
<h2 id="getting-started">Getting Started</h2>
<p>To begin, I will list the files and their roles in this automation project. While these files are currently stored in my local Gitea instance, I’m considering creating a public Git repository for future projects to make them more accessible and easier to share.</p>
<h3 id="inventoryyml">inventory.yml</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-yaml" data-lang="yaml"><span class="line"><span class="cl"><span class="nt">all</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">hosts</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">dnsserver.lab.home</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">ansible_host</span><span class="p">:</span><span class="w"> </span><span class="l">dc.lab.home </span><span class="w"> </span><span class="c"># IP-Adresse or Hostname of Windows-DNS-Servers</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">ansible_user</span><span class="p">:</span><span class="w"> </span><span class="l">administrator </span><span class="w"> </span><span class="c"># Username </span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">ansible_password</span><span class="p">:</span><span class="w"> </span><span class="l">xxx </span><span class="w"> </span><span class="c"># Password</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">ansible_connection</span><span class="p">:</span><span class="w"> </span><span class="l">winrm </span><span class="w"> </span><span class="c"># connection</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">ansible_winrm_transport</span><span class="p">:</span><span class="w"> </span><span class="l">basic </span><span class="w"> </span><span class="c"># auth</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">ansible_winr_server_cert_validation</span><span class="p">:</span><span class="w"> </span><span class="l">ignore</span><span class="w"> </span><span class="c">#don&#39;t check the certificate</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">ansible_port</span><span class="p">:</span><span class="w"> </span><span class="m">5986</span><span class="w"> </span><span class="c">#winrm https port</span><span class="w">
</span></span></span></code></pre></div><h3 id="ansiblecfg">ansible.cfg</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-ini" data-lang="ini"><span class="line"><span class="cl"><span class="k">[defaults]</span>
</span></span><span class="line"><span class="cl"><span class="na">inventory</span> <span class="o">=</span> <span class="s">inventory.yml</span>
</span></span></code></pre></div><h3 id="register_ipyml">register_ip.yml</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-yaml" data-lang="yaml"><span class="line"><span class="cl"><span class="nn">---</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">Validate input variables</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">fail</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">msg</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;You must provide &#39;netbox_token&#39;, &#39;prefix&#39;, and &#39;dns_name&#39; as extra-vars.&#34;</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">when</span><span class="p">:</span><span class="w"> </span><span class="l">netbox_token == &#34;&#34; or prefix == &#34;&#34; or dns_name == &#34;&#34;</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">Get the prefix ID from NetBox</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">uri</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">url</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;{{ netbox_url }}/api/ipam/prefixes/?prefix={{ prefix }}&#34;</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">method</span><span class="p">:</span><span class="w"> </span><span class="l">GET</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">headers</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">Authorization</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;Token {{ netbox_token }}&#34;</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">Accept</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;application/json&#34;</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">return_content</span><span class="p">:</span><span class="w"> </span><span class="kc">yes</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">register</span><span class="p">:</span><span class="w"> </span><span class="l">prefix_data</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">Fail if the prefix does not exist</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">fail</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">msg</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;Prefix {{ prefix }} does not exist in NetBox.&#34;</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">when</span><span class="p">:</span><span class="w"> </span><span class="l">prefix_data.json.results | length == 0</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">Get available IPs in the prefix</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">uri</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">url</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;{{ netbox_url }}/api/ipam/prefixes/{{ prefix_data.json.results[0].id }}/available-ips/&#34;</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">method</span><span class="p">:</span><span class="w"> </span><span class="l">GET</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">headers</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">Authorization</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;Token {{ netbox_token }}&#34;</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">Accept</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;application/json&#34;</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">return_content</span><span class="p">:</span><span class="w"> </span><span class="kc">yes</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">register</span><span class="p">:</span><span class="w"> </span><span class="l">available_ips</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">Fail if no available IPs are found</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">fail</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">msg</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;No available IPs found in prefix {{ prefix }}.&#34;</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">when</span><span class="p">:</span><span class="w"> </span><span class="l">available_ips.json | length == 0</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">Assign the first available IP</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">uri</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">url</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;{{ netbox_url }}/api/ipam/ip-addresses/&#34;</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">method</span><span class="p">:</span><span class="w"> </span><span class="l">POST</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">headers</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">Authorization</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;Token {{ netbox_token }}&#34;</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">Accept</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;application/json&#34;</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">Content-Type</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;application/json&#34;</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">body</span><span class="p">:</span><span class="w"> </span><span class="p">&gt;</span><span class="sd">
</span></span></span><span class="line"><span class="cl"><span class="sd">      {
</span></span></span><span class="line"><span class="cl"><span class="sd">        &#34;address&#34;: &#34;{{ available_ips.json[0].address }}&#34;,
</span></span></span><span class="line"><span class="cl"><span class="sd">        &#34;status&#34;: &#34;active&#34;,
</span></span></span><span class="line"><span class="cl"><span class="sd">        &#34;description&#34;: &#34;Created by Ansible&#34;,
</span></span></span><span class="line"><span class="cl"><span class="sd">        &#34;dns_name&#34;: &#34;{{ dns_name }}&#34;
</span></span></span><span class="line"><span class="cl"><span class="sd">      }</span><span class="w">      
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">body_format</span><span class="p">:</span><span class="w"> </span><span class="l">json</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">status_code</span><span class="p">:</span><span class="w"> </span><span class="m">201</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">return_content</span><span class="p">:</span><span class="w"> </span><span class="kc">yes</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">register</span><span class="p">:</span><span class="w"> </span><span class="l">ip_assignment</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">Extract host and zone from DNS name</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">set_fact</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">dns_host</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;{{ dns_name.split(&#39;.&#39;)[0] }}&#34;</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">dns_zone</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;{{ dns_name.split(&#39;.&#39;, 1)[1] }}&#34;</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">assigned_ip</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;{{ ip_assignment.json.address.split(&#39;/&#39;)[0] }}&#34;</span><span class="w">
</span></span></span></code></pre></div><h3 id="add_dns_recordyml">add_dns_record.yml</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-yaml" data-lang="yaml"><span class="line"><span class="cl"><span class="nn">---</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">Add DNS A Record</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">win_shell</span><span class="p">:</span><span class="w"> </span><span class="p">|</span><span class="sd">
</span></span></span><span class="line"><span class="cl"><span class="sd">    Add-DnsServerResourceRecordA -Name &#34;{{ zdns_host }}&#34; -ZoneName &#34;{{ zdns_zone }}&#34; -IPv4Address &#34;{{ zassigned_ip }}&#34;</span><span class="w">    
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">args</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">executable</span><span class="p">:</span><span class="w"> </span><span class="l">powershell</span><span class="w">
</span></span></span></code></pre></div><h3 id="mp_dnsyml-my-masterplaybook">mp_dns.yml (my masterplaybook)</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-yaml" data-lang="yaml"><span class="line"><span class="cl"><span class="nn">---</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">Register IP in NetBox</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">hosts</span><span class="p">:</span><span class="w"> </span><span class="l">localhost</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">gather_facts</span><span class="p">:</span><span class="w"> </span><span class="kc">no</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">vars</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">prefix</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;{{ prefix }}&#34;</span><span class="w">  </span><span class="c"># variables</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">dns_name</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;{{ dns_name }}&#34;</span><span class="w">  </span><span class="c"># variables</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">netbox_url</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;http://netbox.lab.home&#34;</span><span class="w">  </span><span class="c">#NetBox-URL</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">netbox_token</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;xxx&#34;</span><span class="w">  </span><span class="c"># Ntebox API token</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">tasks</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">Run NetBox IP Registration Playbook</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">include_tasks</span><span class="p">:</span><span class="w"> </span><span class="l">register_ip.yml</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">vars</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span><span class="nt">prefix</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;{{ prefix }}&#34;</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span><span class="nt">dns_name</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;{{ dns_name }}&#34;</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        
</span></span></span><span class="line"><span class="cl"><span class="w"> 
</span></span></span><span class="line"><span class="cl"><span class="w"></span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">Add DNS A Record</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">hosts</span><span class="p">:</span><span class="w"> </span><span class="l">dnsserver.lab.home</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">gather_facts</span><span class="p">:</span><span class="w"> </span><span class="kc">no</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">vars</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">ansible_winrm_server_cert_validation</span><span class="p">:</span><span class="w"> </span><span class="l">ignore</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">zassigned_ip</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;{{ hostvars[&#39;localhost&#39;][&#39;sip&#39;] }}&#34;</span><span class="w"> </span><span class="c"># variables</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">zdns_host</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;{{ hostvars[&#39;localhost&#39;][&#39;sdns&#39;] }}&#34;</span><span class="w">   </span><span class="c"># variables</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">zdns_zone</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;{{ hostvars[&#39;localhost&#39;][&#39;szone&#39;] }}&#34;</span><span class="w">  </span><span class="c"># variables</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">tasks</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">Include DNS Record Playbook</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">include_tasks</span><span class="p">:</span><span class="w"> </span><span class="l">add_dns_record.yml</span><span class="w">
</span></span></span></code></pre></div><h2 id="how-the-playbooks-work">How the Playbooks Work</h2>
<p>The process is coordinated by a master playbook (mp_dns.yml) and relies on sub-playbooks for discrete tasks.</p>
<h3 id="master-playbook-mp_dnsyml">Master Playbook (mp_dns.yml)</h3>
<p>The master playbook serves as the central control file. It performs the following steps:</p>
<p>Registers an IP Address in NetBox: This step invokes the register_ip.yml sub-playbook to allocate an available IP address within a specified prefix and associate it with the given DNS name in NetBox.</p>
<ul>
<li>
<p>Sets Facts:
After obtaining the IP address and DNS details from NetBox, it uses set_fact to store these values in variables (sip, sdns, szone) for use in the next task.</p>
</li>
<li>
<p>Adds a DNS A Record:
The second phase connects to the DNS server and calls the add_dns_record.yml sub-playbook to create a DNS A record using the information retrieved from NetBox.</p>
</li>
</ul>
<h3 id="sub-playbook-register_ipyml">Sub-Playbook: register_ip.yml</h3>
<p>This playbook interacts with NetBox&rsquo;s API to:</p>
<ul>
<li>Validate input variables like the NetBox token, prefix, and DNS name.</li>
<li>Retrieve the prefix and find available IPs.</li>
<li>Assign the first available IP to the provided DNS name and register it in NetBox.</li>
</ul>
<p>The playbook sends a POST request to the NetBox API to assign an available IP address to the provided DNS name. The response is returned in JSON format and parsed to extract the necessary variables for the DNS record creation.</p>
<p>The JSON response is parsed to extract key values:</p>
<ul>
<li>dns_host and dns_zone are derived by splitting the FQDN.</li>
<li>assigned_ip captures the raw IP address, omitting the CIDR notation.</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-yaml" data-lang="yaml"><span class="line"><span class="cl"><span class="w">  </span><span class="nt">dns_host</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;{{ dns_name.split(&#39;.&#39;)[0] }}&#34;</span><span class="w">  </span><span class="c"># Extracts the hostname (e.g., &#34;myhost&#34; from &#34;myhost.lab.local&#34;)</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">dns_zone</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;{{ dns_name.split(&#39;.&#39;, 1)[1] }}&#34;</span><span class="w">  </span><span class="c"># Extracts the zone (e.g., &#34;lab.local&#34;)</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">assigned_ip</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;{{ ip_assignment.json.address.split(&#39;/&#39;)[0] }}&#34;</span><span class="w">  </span><span class="c"># Removes the subnet mask (e.g., &#34;192.168.1.10/24&#34; to &#34;192.168.1.10&#34;)</span><span class="w">
</span></span></span></code></pre></div><p>This parsing ensures the required details are extracted for creating the DNS record in subsequent tasks, linking NetBox&rsquo;s IP allocation to the DNS configuration seamlessly.</p>
<h3 id="sub-playbook-add_dns_recordyml">Sub-Playbook: add_dns_record.yml</h3>
<p>This playbook uses PowerShell (win_shell) to execute the Add-DnsServerResourceRecordA cmdlet on the Windows DNS server. It creates a DNS A record with the assigned IP, host, and zone.</p>
<h3 id="why-use-host_vars">Why Use host_vars?</h3>
<p>hostvars is a built-in Ansible variable that provides access to variables from other hosts in the inventory. This is particularly useful when you need to share or reference facts or variables gathered from one host on another host.
The NetBox-related tasks (e.g., registering IP addresses and extracting DNS details) are performed on localhost since they interact with external APIs and don’t require remote server execution.
Variables like sip, sdns, and szone are set as facts on localhost during the first phase of the playbook execution.
The <em><strong>hostvars[&rsquo;localhost&rsquo;]</strong></em> construct is used to retrieve these facts and make them available to the subsequent tasks running on the DNS server (dnsserver.lab.home).</p>
<h3 id="variable-assignments">Variable Assignments:</h3>
<ul>
<li>zassigned_ip: This retrieves the IP address (sip) assigned to the host from the NetBox interaction on localhost.</li>
<li>zdns_host: This extracts the host portion of the DNS name (sdns) derived from the FQDN split.</li>
<li>zdns_zone: This fetches the DNS zone (szone), also derived from the FQDN split.</li>
</ul>
<p>This approach ensures that:</p>
<ul>
<li>Data derived or computed in one phase (NetBox-related tasks) is seamlessly passed to the next phase (DNS-related tasks).</li>
<li>sThe DNS playbook (add_dns_record.yml) running on the DNS server has access to the correct IP, host, and zone information without redundant processing.</li>
</ul>

    <aside class="admonition info">
        <div class="admonition-title">
            <div class="icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-info">
      <circle cx="12" cy="12" r="10"></circle>
      <line x1="12" y1="16" x2="12" y2="12"></line>
      <line x1="12" y1="8" x2="12.01" y2="8"></line>
   </svg></div><b>Info</b>
        </div>
        <div class="admonition-content"><p>One of the biggest challenges I faced in this project was understanding why I couldn’t directly use the variables returned from NetBox in the DNS-related tasks. I initially tried to pass these variables directly, but the playbook failed because the DNS tasks were executed on a different host (dnsserver.lab.home) than the one that retrieved the data (localhost).</p>
<p>The solution involved using hostvars to reference the facts set on localhost. This took the most time to figure out, as I didn’t immediately realize that variables gathered on one host are not automatically accessible on another. Once I understood how hostvars works, everything started to fall into place.</p>
</div>
    </aside>
<h2 id="ok-enough-code-and-explanations-lets-see-it-in-action">Ok, Enough Code and Explanations, Let’s See It in Action</h2>
<p>Starting the playbook:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-plaintext" data-lang="plaintext"><span class="line"><span class="cl">ansible-playbook mp_dns.yml -e &#34;prefix=192.168.2.0/24 dns_name=hello-world.lab.home&#34;
</span></span></code></pre></div>
<figure><a href="output.png"><picture>
          <source srcset="/ipam-automation/output_hu17735082299749069346.webp" type="image/webp">
          <source srcset="/ipam-automation/output_hu2134898282273478624.jpg" type="image/jpeg">
          <img src="/ipam-automation/output_hu17735082299749069346.webp"alt="Ansible output"  width="1003"  height="610" />
        </picture></a><figcaption>
            <p>Ansible output (click to enlarge)</p>
          </figcaption></figure>

<figure><a href="netbox.png"><picture>
          <source srcset="/ipam-automation/netbox_hu16384549009619117197.webp" type="image/webp">
          <source srcset="/ipam-automation/netbox_hu10090990778334734330.jpg" type="image/jpeg">
          <img src="/ipam-automation/netbox_hu16384549009619117197.webp"alt="Netbox"  width="1447"  height="489" />
        </picture></a><figcaption>
            <p>Netbox (click to enlarge)</p>
          </figcaption></figure>

<figure><a href="dns.png"><picture>
          <source srcset="/ipam-automation/dns_hu12209689906728532248.webp" type="image/webp">
          <source srcset="/ipam-automation/dns_hu11255499409238053450.jpg" type="image/jpeg">
          <img src="/ipam-automation/dns_hu12209689906728532248.webp"alt="DNS"  width="404"  height="455" />
        </picture></a><figcaption>
            <p>DNS (click to enlarge)</p>
          </figcaption></figure>
<h2 id="conclusion">Conclusion</h2>
<p>This project represents just the first step toward a fully automated IPAM and DNS management workflow. While the current solution works well in my lab environment, there is plenty of room for improvement and expansion.</p>
<p>Key Takeaways:</p>
<ul>
<li>
<p>Modular Design: Starting with a modular playbook structure ensures flexibility for future enhancements and easier debugging.</p>
</li>
<li>
<p>Lab vs. Production: This setup is tailored for a lab environment. For production systems, avoid using highly privileged accounts like the local administrator on the DNS server. A more secure approach with role-based access control (RBAC) should be implemented in future iterations.</p>
</li>
<li>
<p>Continuous Improvement: I acknowledge that the playbook is not perfect. Over time, I plan to refine and optimize it, addressing any current shortcomings and making it more robust for complex workflows.</p>
</li>
</ul>
<p><em><strong>Automation is a journey</strong></em>, and I’m excited to see how this project evolves. Stay tuned for updates and new features in future versions!</p>
]]></content>
		</item>
		
		<item>
			<title>From Zero to Automation: How I Used ChatGPT to Create My First Ansible Playbook</title>
			<link>https://sdn-warrior.org/posts/first-steps-ansible/</link>
			<pubDate>Tue, 17 Dec 2024 22:36:18 +0100</pubDate>
			
			<guid>https://sdn-warrior.org/posts/first-steps-ansible/</guid>
			<description><![CDATA[How I Used ChatGPT to Create My First Ansible Playbook]]></description>
			<content type="html"><![CDATA[<h2 id="introduction">Introduction</h2>
<p>I recently decided to automate the startup and shutdown of my lab environments—both standard and nested labs. While the idea sounded simple, it quickly turned into an interesting challenge. Having never written an Ansible Playbook before, I turned to ChatGPT for help.</p>
<h2 id="why-chatgpt">Why ChatGPT?</h2>
<p>Let’s be honest: starting with Ansible can feel overwhelming, especially if you&rsquo;re new to it. My last experience with something remotely similar was years ago, working with PowerShell scripts or even earlier with .NET 3 (yes, I’m &ldquo;that old&rdquo;).</p>
<p>The task itself seemed straightforward at first:</p>
<ul>
<li>Write a playbook to power VMs on and off in a controlled manner.</li>
<li>Integrate both my standard lab and nested lab (e.g., my VCF setup with its own vCenter).</li>
</ul>
<p>However, the challenge revealed itself quickly:</p>
<ul>
<li>Controlling VMs via my main vCenter is relatively easy.</li>
<li>But what about nested labs where each nested setup has its own vCenter?</li>
</ul>
<p>This is where ChatGPT became a game changer.</p>
<h2 id="the-approach">The Approach</h2>
<h3 id="starting-from-zero">Starting from Zero</h3>
<p>I described my setup and goals to ChatGPT:</p>
<p>Automate VM startup/shutdown.
Handle dependencies like nested vCenters that control their own VMs.
ChatGPT provided a clear starting point, explaining how to structure an Ansible playbook. Step by step, it introduced me to tasks, loops, and the required VMware modules.</p>
<h3 id="iterating-through-challenges">Iterating Through Challenges</h3>
<p>The major challenge was managing nested environments:</p>
<p>Powering on the parent vCenter first.
Waiting until it’s responsive.
Then triggering the startup sequence for the nested VMs managed by that vCenter.
Through multiple iterations, ChatGPT helped refine the logic.</p>
<h3 id="not-always-smooth-sailing">Not Always Smooth Sailing</h3>
<p>To be honest, ChatGPT’s suggestions weren’t always perfect. More than once, I found myself in a dead end. I had to point out repeatedly that the same solution, presented for the third time, simply didn’t work. This is the reality of working with AI: it doesn’t replace expertise, but it certainly accelerates the process.</p>
<p>While ChatGPT couldn’t solve everything on its own, it significantly simplified finding the right solution. Instead of starting from scratch or digging through documentation for hours, I could focus on testing and refining the playbook.</p>
<h2 id="current-progress-what-i-achieved-in-two-evenings">Current Progress: What I Achieved in Two Evenings</h2>
<p>After a couple of evenings, with a few hours of experimenting and iterating with ChatGPT, I managed to create four modular Ansible playbooks. These playbooks are designed to handle two key scenarios for starting and stopping VMs:</p>
<p>Two Playbooks for Environments with vCenter</p>
<p>These playbooks are for my standard (non-nested) lab environments, where I can rely on vCenter to manage the VMs.
With vCenter in place, controlling VMs is relatively straightforward, as vCenter provides a central interface to handle power states.
Two Playbooks for Environments without vCenter</p>
<p>These playbooks handle environments where no vCenter is available, such as nested labs or standalone ESXi hosts.
In nested labs, the challenge arises because VMs and their dependencies are controlled individually, without the convenience of a central management interface.
By separating the logic into modular playbooks, I ensured flexibility and reusability across my different lab setups. Whether I’m dealing with my regular homelab VMs or complex nested environments like my VCF setup, I can now efficiently start and stop VMs with a single command.</p>
<h3 id="inventory-files-the-backbone-of-the-setup">Inventory Files: The Backbone of the Setup</h3>
<p>To make the playbooks flexible and reusable, I created inventory YAML files for each lab. Out of habit, I named them something like vcfvm_vars.yml or vcfesx_vars.yml. These files act as the variable storage for each lab environment.</p>
<p>There are two types of inventory files:</p>
<p>For Nested VMs:</p>
<p>Includes variables specific to nested lab setups, such as nested vCenter credentials, VM names, and their dependencies.
For Non-Nested VMs:</p>
<p>Stores details for standard VMs managed directly via the main vCenter.</p>
<h3 id="nested-vcf-example-controlled-boot-and-shutdown">Nested VCF Example: Controlled Boot and Shutdown</h3>
<p>In my VCF setup, which is fully nested, the playbook must follow a strict sequence:</p>
<p>Startup:</p>
<p>Start the nested ESXi hosts first.
Wait for their availability.
Then start the nested management VMs, such as NSX Manager, SDDC Manager, and vCenter.
Shutdown:</p>
<p>Stop the management VMs first.
Once the management layer is powered down, shut down the nested ESXi hosts.
This controlled sequence ensures the nested environment behaves predictably.</p>
<h3 id="inventory-file-for-esxi-hosts">Inventory File for ESXi Hosts</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-plaintext" data-lang="plaintext"><span class="line"><span class="cl"># vcfesx_vars.yml
</span></span><span class="line"><span class="cl">vcenter_hostname: &#34;vcsa.lab.home&#34;
</span></span><span class="line"><span class="cl">vcenter_username: &#34;administrator@vsphere.local&#34;
</span></span><span class="line"><span class="cl">vcenter_password: &#34;your_pw&#34;
</span></span><span class="line"><span class="cl">vcenter_datacenter: &#34;Homelab&#34;
</span></span><span class="line"><span class="cl">validate_certs: false
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">vm_names:
</span></span><span class="line"><span class="cl">  - &#34;sfo01-m01-esx01&#34;
</span></span><span class="line"><span class="cl">  - &#34;sfo01-m01-esx02&#34;
</span></span><span class="line"><span class="cl">  - &#34;sfo01-m01-esx03&#34;
</span></span></code></pre></div><h3 id="inventory-file-for-nested-vms">Inventory File for Nested VMs</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-plaintext" data-lang="plaintext"><span class="line"><span class="cl"># vcfvm_vars.yml
</span></span><span class="line"><span class="cl">validate_certs: false
</span></span><span class="line"><span class="cl">esxi_hosts:
</span></span><span class="line"><span class="cl">  - &#34;sfo01-m01-esx01.lab.home&#34;
</span></span><span class="line"><span class="cl">  - &#34;sfo01-m01-esx02.lab.home&#34;
</span></span><span class="line"><span class="cl">  - &#34;sfo01-m01-esx03.lab.home&#34;
</span></span><span class="line"><span class="cl">esxi_username: &#34;root&#34;
</span></span><span class="line"><span class="cl">esxi_password: &#34;your_pw&#34;
</span></span><span class="line"><span class="cl">esxi_datacenter: &#34;sfo-m01-dc01&#34;
</span></span><span class="line"><span class="cl">vm_names:
</span></span><span class="line"><span class="cl">  - &#34;vcfvcsa&#34;
</span></span><span class="line"><span class="cl">  - &#34;vcfnsx01a&#34;
</span></span><span class="line"><span class="cl">  - &#34;vcf01&#34;
</span></span></code></pre></div><h3 id="power-on-playbook-for-non-nested-vms">Power-On Playbook for Non-Nested VMs</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-plaintext" data-lang="plaintext"><span class="line"><span class="cl">---
</span></span><span class="line"><span class="cl">- name: Start specific VMs in vCenter
</span></span><span class="line"><span class="cl">  hosts: localhost
</span></span><span class="line"><span class="cl">  gather_facts: no
</span></span><span class="line"><span class="cl">  collections:
</span></span><span class="line"><span class="cl">    - community.vmware
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  tasks:
</span></span><span class="line"><span class="cl">    - name: Load variables from file
</span></span><span class="line"><span class="cl">      include_vars: &#34;{{ vars_file }}&#34;
</span></span><span class="line"><span class="cl">    - name: Connect to vCenter and start VMs
</span></span><span class="line"><span class="cl">      community.vmware.vmware_guest_powerstate:
</span></span><span class="line"><span class="cl">        hostname: &#34;{{ vcenter_hostname }}&#34;
</span></span><span class="line"><span class="cl">        username: &#34;{{ vcenter_username }}&#34;
</span></span><span class="line"><span class="cl">        password: &#34;{{ vcenter_password }}&#34;
</span></span><span class="line"><span class="cl">        validate_certs: &#34;{{ validate_certs }}&#34;
</span></span><span class="line"><span class="cl">        name: &#34;{{ item }}&#34;
</span></span><span class="line"><span class="cl">        state: powered-on
</span></span><span class="line"><span class="cl">      loop: &#34;{{ vm_names }}&#34;
</span></span><span class="line"><span class="cl">      register: power_state_result
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    - name: Display power state result
</span></span><span class="line"><span class="cl">      debug:
</span></span><span class="line"><span class="cl">        msg: &#34;VM {{ item.item }} wurde erfolgreich gestartet.&#34;
</span></span><span class="line"><span class="cl">      when: item.instance.hw_power_status == &#34;poweredOn&#34;
</span></span><span class="line"><span class="cl">      loop: &#34;{{ power_state_result.results }}&#34;
</span></span><span class="line"><span class="cl">      loop_control:
</span></span><span class="line"><span class="cl">        label: &#34;{{ item.item }}&#34;
</span></span></code></pre></div><ol>
<li>
<p><em><strong>include_vars:</strong></em> Loads a variable file, such as vcfvm_vars.yml, which makes the playbook modular and reusable.</p>
</li>
<li>
<p><em><strong>community.vmware.vmware_guest_powerstate:</strong></em> Uses the <em><strong>vmware_guest_powerstate</strong></em> module to control the power state of VMs in a vCenter-managed environment.</p>
</li>
<li>
<p><em><strong>The state:</strong></em> powered-on option ensures VMs are powered on.</p>
</li>
<li>
<p><em><strong>register: power_state_result:</strong></em> Captures the result of the task execution for each VM, including its power state.</p>
</li>
<li>
<p><em><strong>debug with when:</strong></em> Checks the power state of each VM and displays a success message if the VM was successfully powered on.</p>
</li>
</ol>
<h3 id="power-on-playbook-for-nested-vms-on-multiple-esxi-hosts">Power-On Playbook for Nested VMs on Multiple ESXi Hosts</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-plaintext" data-lang="plaintext"><span class="line"><span class="cl">---
</span></span><span class="line"><span class="cl">- name: Power on multiple VMs on multiple ESXi hosts
</span></span><span class="line"><span class="cl">  hosts: localhost
</span></span><span class="line"><span class="cl">  gather_facts: no
</span></span><span class="line"><span class="cl">  collections:
</span></span><span class="line"><span class="cl">    - community.vmware
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  tasks:
</span></span><span class="line"><span class="cl">    - name: Load variables from file
</span></span><span class="line"><span class="cl">      include_vars: &#34;{{ vars_file }}&#34;
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    - name: Get VM power status for each VM on each ESXi host
</span></span><span class="line"><span class="cl">      community.vmware.vmware_guest_info:
</span></span><span class="line"><span class="cl">        hostname: &#34;{{ item.0 }}&#34;
</span></span><span class="line"><span class="cl">        username: &#34;{{ esxi_username }}&#34;
</span></span><span class="line"><span class="cl">        password: &#34;{{ esxi_password }}&#34;
</span></span><span class="line"><span class="cl">        datacenter: &#34;{{ esxi_datacenter }}&#34;
</span></span><span class="line"><span class="cl">        validate_certs: &#34;{{ validate_certs }}&#34;
</span></span><span class="line"><span class="cl">        name: &#34;{{ item.1 }}&#34;
</span></span><span class="line"><span class="cl">      with_nested:
</span></span><span class="line"><span class="cl">        - &#34;{{ esxi_hosts }}&#34;
</span></span><span class="line"><span class="cl">        - &#34;{{ vm_names }}&#34;
</span></span><span class="line"><span class="cl">      register: vm_info_results
</span></span><span class="line"><span class="cl">      ignore_errors: true
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    - name: Filter VMs that are poweredOff
</span></span><span class="line"><span class="cl">      set_fact:
</span></span><span class="line"><span class="cl">        powered_off_vms: &#34;{{ vm_info_results.results | selectattr(&#39;failed&#39;, &#39;equalto&#39;, false)
</span></span><span class="line"><span class="cl">                           | selectattr(&#39;instance.hw_power_status&#39;, &#39;equalto&#39;, &#39;poweredOff&#39;) }}&#34;
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    - name: Power on VMs if they are poweredOff
</span></span><span class="line"><span class="cl">      community.vmware.vmware_guest_powerstate:
</span></span><span class="line"><span class="cl">        hostname: &#34;{{ item.item.0 }}&#34;
</span></span><span class="line"><span class="cl">        username: &#34;{{ esxi_username }}&#34;
</span></span><span class="line"><span class="cl">        password: &#34;{{ esxi_password }}&#34;
</span></span><span class="line"><span class="cl">        datacenter: &#34;{{ esxi_datacenter }}&#34;
</span></span><span class="line"><span class="cl">        validate_certs: &#34;{{ validate_certs }}&#34;
</span></span><span class="line"><span class="cl">        name: &#34;{{ item.item.1 }}&#34;
</span></span><span class="line"><span class="cl">        state: powered-on
</span></span><span class="line"><span class="cl">      loop: &#34;{{ powered_off_vms }}&#34;
</span></span><span class="line"><span class="cl">      loop_control:
</span></span><span class="line"><span class="cl">        label: &#34;Host: {{ item.item.0 }} | VM: {{ item.item.1 }}&#34;
</span></span><span class="line"><span class="cl">      register: poweron_results
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    - name: Wait for VMs to be powered on
</span></span><span class="line"><span class="cl">      community.vmware.vmware_guest_info:
</span></span><span class="line"><span class="cl">        hostname: &#34;{{ item.item.item.0 }}&#34;
</span></span><span class="line"><span class="cl">        username: &#34;{{ esxi_username }}&#34;
</span></span><span class="line"><span class="cl">        password: &#34;{{ esxi_password }}&#34;
</span></span><span class="line"><span class="cl">        datacenter: &#34;{{ esxi_datacenter }}&#34;
</span></span><span class="line"><span class="cl">        validate_certs: &#34;{{ validate_certs }}&#34;
</span></span><span class="line"><span class="cl">        name: &#34;{{ item.item.item.1 }}&#34;
</span></span><span class="line"><span class="cl">      loop: &#34;{{ poweron_results.results }}&#34;
</span></span><span class="line"><span class="cl">      loop_control:
</span></span><span class="line"><span class="cl">        label: &#34;Host: {{ item.item.item.0 }} | VM: {{ item.item.item.1 }}&#34;
</span></span><span class="line"><span class="cl">      register: vm_status
</span></span><span class="line"><span class="cl">      until: vm_status.instance.hw_power_status == &#34;poweredOn&#34;
</span></span><span class="line"><span class="cl">      retries: 20
</span></span><span class="line"><span class="cl">      delay: 15
</span></span><span class="line"><span class="cl">      when: item.failed == false
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    - name: Display power on result
</span></span><span class="line"><span class="cl">      debug:
</span></span><span class="line"><span class="cl">        msg: &#34;VM {{ item.item.item.1 }} on Host {{ item.item.item.0 }} has been successfully powered on.&#34;
</span></span><span class="line"><span class="cl">      loop: &#34;{{ poweron_results.results }}&#34;
</span></span><span class="line"><span class="cl">      loop_control:
</span></span><span class="line"><span class="cl">        label: &#34;Host: {{ item.item.item.0 }} | VM: {{ item.item.item.1 }}&#34;
</span></span></code></pre></div><ol>
<li>
<p><em><strong>vmware_guest_info</strong></em> Retrieves the power state of each VM on each ESXi host.</p>
</li>
<li>
<p><em><strong>set_fact</strong></em> Filters out only those VMs that are powered off.</p>
</li>
<li>
<p><em><strong>vmware_guest_powerstate</strong></em> Powers on each VM that is in a &ldquo;poweredOff&rdquo; state.</p>
</li>
<li>
<p><em><strong>wait_for with retries</strong></em> Ensures that the VMs are fully powered on before proceeding.</p>
</li>
<li>
<p><em><strong>debug</strong></em> Displays a confirmation message for each successfully powered-on VM.</p>
</li>
</ol>
<h3 id="master-playbook">Master Playbook</h3>
<p>to orchestrate the two Power-On playbooks in the correct order. I kept your current 60-second pause timer as a placeholder for checking ESXi server readiness but structured everything neatly for clarity. A 60-second pause ensures that the ESXi hosts have enough time to initialize. Why a Pause? Without an active feedback mechanism to confirm the ESXi servers are ready, this static wait acts as a temporary workaround and will replaced later.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-plaintext" data-lang="plaintext"><span class="line"><span class="cl">---
</span></span><span class="line"><span class="cl">- name: Power on Nested ESXi Hosts
</span></span><span class="line"><span class="cl">  import_playbook: poweron_vcsa.yml
</span></span><span class="line"><span class="cl">  vars:
</span></span><span class="line"><span class="cl">    vars_file: &#34;vcfesx_vars.yml&#34;
</span></span><span class="line"><span class="cl">  # Executes the playbook to power on the nested ESXi hosts.
</span></span><span class="line"><span class="cl">  # Variables specific to ESXi servers are loaded from &#34;vcfesx_vars.yml&#34;.
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">- name: Wait for 60 seconds before powering on nested VMs
</span></span><span class="line"><span class="cl">  hosts: localhost
</span></span><span class="line"><span class="cl">  gather_facts: no
</span></span><span class="line"><span class="cl">  tasks:
</span></span><span class="line"><span class="cl">    - name: Pause for 60 seconds
</span></span><span class="line"><span class="cl">      pause:
</span></span><span class="line"><span class="cl">        seconds: 60
</span></span><span class="line"><span class="cl">      # A static wait time to ensure ESXi hosts are ready.
</span></span><span class="line"><span class="cl">      # This will be improved in the future with dynamic checks.
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">- name: Power on Nested Management VMs
</span></span><span class="line"><span class="cl">  import_playbook: poweron_esx.yml
</span></span><span class="line"><span class="cl">  vars:
</span></span><span class="line"><span class="cl">    vars_file: &#34;vcfvm_vars.yml&#34;
</span></span><span class="line"><span class="cl">  # Executes the playbook to power on nested VMs like NSX Manager, SDDC Manager, and vCenter.
</span></span><span class="line"><span class="cl">  # Variables specific to management VMs are loaded from &#34;vcfvm_vars.yml&#34;.
</span></span></code></pre></div><h3 id="starting-the-vms-via-the-ansible-master-playbook">Starting the VMs via the Ansible Master Playbook</h3>
<p>Starting my VCF nested lab has never been easier. With the Ansible Master Playbook, it’s as simple as running a single command on my Ansible server:</p>
<pre tabindex="0"><code>ansible-playbook mp_poweron_vcf.yml
</code></pre><p>Within approximately 5-10 minutes (depending on the overall load on my lab), the entire VCF environment is up and ready to use—without any further manual intervention.</p>
<p>The beauty of this setup lies in its flexibility:</p>
<p>New labs can be easily added by simply creating a new inventory file and a customized master playbook.
The core logic remains untouched, making it a scalable and modular solution for automating additional environments.
This approach not only saves time but also ensures consistency when starting up complex nested labs like my VCF setup.</p>

<figure><a href="ansible.png"><picture>
          <source srcset="/first-steps-ansible/ansible_hu6172869207777496753.webp" type="image/webp">
          <source srcset="/first-steps-ansible/ansible_hu10846971240981565405.jpg" type="image/jpeg">
          <img src="/first-steps-ansible/ansible_hu6172869207777496753.webp"alt="Ansible Log"  width="1718"  height="1056" />
        </picture></a><figcaption>
            <p>Ansible Output (click to enlarge)</p>
          </figcaption></figure>
<p>The log output of my Ansible playbook contains failed messages during the task: Get VM power status for each VM on each ESXi host
These failures occur because each ESXi host is queried for specific VMs (like vcf01) that may not exist on that particular host. This is both normal and expected behavior.</p>
<p>Why?
Due to DRS (Distributed Resource Scheduler), I can never be certain which nested ESXi host a particular VM was last running on. By iterating through all ESXi hosts, the playbook ensures that the power status of every VM is eventually retrieved, regardless of where it was previously located.</p>
<h3 id="shutdown-playbook-graceful-power-off-of-vms">Shutdown Playbook: Graceful Power-Off of VMs</h3>
<p>The shutdown process follows the same principles as the power-on playbook but in reverse order. Instead of starting VMs, it ensures a graceful shutdown while verifying their power state. I won&rsquo;t describe every task in detail, but here’s a quick overview:</p>
<p>Logic Similar to Power-On:</p>
<ul>
<li>VMs are iterated across multiple ESXi hosts.</li>
<li>Only VMs that are currently powered on are gracefully shut down.</li>
</ul>
<p>Graceful Shutdown with Validation:</p>
<ul>
<li>VMs are shut down using shutdown-guest to trigger the guest OS shutdown process.</li>
<li>A retry loop with retries: 20 and delay: 15 ensures that the playbook actively checks until the VMs reach the poweredOff state.</li>
</ul>
<p>Harmless Errors Handled:</p>
<ul>
<li>As with the power-on playbook, the ignore_errors: true directive handles expected failures gracefully (e.g., querying for VMs on ESXi hosts where they are not located).</li>
</ul>
<h3 id="shutdown-nested-vms">Shutdown Nested VMs</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-plaintext" data-lang="plaintext"><span class="line"><span class="cl">- name: Graceful shutdown of multiple VMs on multiple ESXi hosts
</span></span><span class="line"><span class="cl">  hosts: localhost
</span></span><span class="line"><span class="cl">  gather_facts: no
</span></span><span class="line"><span class="cl">  collections:
</span></span><span class="line"><span class="cl">    - community.vmware
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  tasks:
</span></span><span class="line"><span class="cl">    - name: Load variables from file
</span></span><span class="line"><span class="cl">      include_vars: &#34;{{ vars_file }}&#34;
</span></span><span class="line"><span class="cl">      
</span></span><span class="line"><span class="cl">    - name: Get VM power status for each VM on each ESXi host
</span></span><span class="line"><span class="cl">      community.vmware.vmware_guest_info:
</span></span><span class="line"><span class="cl">        hostname: &#34;{{ item.0 }}&#34;
</span></span><span class="line"><span class="cl">        username: &#34;{{ esxi_username }}&#34;
</span></span><span class="line"><span class="cl">        password: &#34;{{ esxi_password }}&#34;
</span></span><span class="line"><span class="cl">        datacenter: &#34;{{ esxi_datacenter }}&#34;
</span></span><span class="line"><span class="cl">        validate_certs: &#34;{{ validate_certs }}&#34;
</span></span><span class="line"><span class="cl">        name: &#34;{{ item.1 }}&#34;
</span></span><span class="line"><span class="cl">      with_nested:
</span></span><span class="line"><span class="cl">        - &#34;{{ esxi_hosts }}&#34;
</span></span><span class="line"><span class="cl">        - &#34;{{ vm_names }}&#34;
</span></span><span class="line"><span class="cl">      register: vm_info_results
</span></span><span class="line"><span class="cl">      ignore_errors: true
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    - name: Filter VMs that are poweredOn
</span></span><span class="line"><span class="cl">      set_fact:
</span></span><span class="line"><span class="cl">        powered_on_vms: &#34;{{ vm_info_results.results | selectattr(&#39;failed&#39;, &#39;equalto&#39;, false)
</span></span><span class="line"><span class="cl">                           | selectattr(&#39;instance.hw_power_status&#39;, &#39;equalto&#39;, &#39;poweredOn&#39;) }}&#34;
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    - name: Shut down VMs if they are poweredOn
</span></span><span class="line"><span class="cl">      community.vmware.vmware_guest_powerstate:
</span></span><span class="line"><span class="cl">        hostname: &#34;{{ item.item.0 }}&#34;
</span></span><span class="line"><span class="cl">        username: &#34;{{ esxi_username }}&#34;
</span></span><span class="line"><span class="cl">        password: &#34;{{ esxi_password }}&#34;
</span></span><span class="line"><span class="cl">        datacenter: &#34;{{ esxi_datacenter }}&#34;
</span></span><span class="line"><span class="cl">        validate_certs: &#34;{{ validate_certs }}&#34;
</span></span><span class="line"><span class="cl">        name: &#34;{{ item.item.1 }}&#34;
</span></span><span class="line"><span class="cl">        state: shutdown-guest
</span></span><span class="line"><span class="cl">        force: false
</span></span><span class="line"><span class="cl">      loop: &#34;{{ powered_on_vms }}&#34;
</span></span><span class="line"><span class="cl">      loop_control:
</span></span><span class="line"><span class="cl">        label: &#34;Host: {{ item.item.0 }} | VM: {{ item.item.1 }}&#34;
</span></span><span class="line"><span class="cl">      register: shutdown_results
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    - name: Wait for VMs to be powered off
</span></span><span class="line"><span class="cl">      community.vmware.vmware_guest_info:
</span></span><span class="line"><span class="cl">        hostname: &#34;{{ item.item.item.0 }}&#34;
</span></span><span class="line"><span class="cl">        username: &#34;{{ esxi_username }}&#34;
</span></span><span class="line"><span class="cl">        password: &#34;{{ esxi_password }}&#34;
</span></span><span class="line"><span class="cl">        datacenter: &#34;{{ esxi_datacenter }}&#34;
</span></span><span class="line"><span class="cl">        validate_certs: &#34;{{ validate_certs }}&#34;
</span></span><span class="line"><span class="cl">        name: &#34;{{ item.item.item.1 }}&#34;
</span></span><span class="line"><span class="cl">      loop: &#34;{{ shutdown_results.results }}&#34;
</span></span><span class="line"><span class="cl">      loop_control:
</span></span><span class="line"><span class="cl">        label: &#34;Host: {{ item.item.item.0 }} | VM: {{ item.item.item.1 }}&#34;
</span></span><span class="line"><span class="cl">      register: vm_status
</span></span><span class="line"><span class="cl">      until: vm_status.instance.hw_power_status == &#34;poweredOff&#34;
</span></span><span class="line"><span class="cl">      retries: 20
</span></span><span class="line"><span class="cl">      delay: 15
</span></span><span class="line"><span class="cl">      when: item.failed == false
</span></span></code></pre></div><h3 id="shutdown-playbook-for-virtual-esxi-servers-using-vcenter">Shutdown Playbook for Virtual ESXi Servers Using vCenter</h3>
<p>This playbook is very similar to the nested VM shutdown playbook, but since I can rely on the vCenter, I don’t need to iterate through all ESXi servers. This simplifies the process and improves efficiency.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-plaintext" data-lang="plaintext"><span class="line"><span class="cl">---
</span></span><span class="line"><span class="cl">- name: Graceful shutdown of specific VMs if powered on
</span></span><span class="line"><span class="cl">  hosts: localhost
</span></span><span class="line"><span class="cl">  gather_facts: no
</span></span><span class="line"><span class="cl">  collections:
</span></span><span class="line"><span class="cl">    - community.vmware
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  tasks:
</span></span><span class="line"><span class="cl">    - name: Load variables from file
</span></span><span class="line"><span class="cl">      include_vars: &#34;{{ vars_file }}&#34;
</span></span><span class="line"><span class="cl">    - name: Get VM information
</span></span><span class="line"><span class="cl">      community.vmware.vmware_guest_info:
</span></span><span class="line"><span class="cl">        hostname: &#34;{{ vcenter_hostname }}&#34;
</span></span><span class="line"><span class="cl">        username: &#34;{{ vcenter_username }}&#34;
</span></span><span class="line"><span class="cl">        password: &#34;{{ vcenter_password }}&#34;
</span></span><span class="line"><span class="cl">        datacenter: &#34;{{ vcenter_datacenter }}&#34;
</span></span><span class="line"><span class="cl">        validate_certs: &#34;{{ validate_certs }}&#34;
</span></span><span class="line"><span class="cl">        name: &#34;{{ item }}&#34;
</span></span><span class="line"><span class="cl">      loop: &#34;{{ vm_names }}&#34;
</span></span><span class="line"><span class="cl">      register: vm_info_results
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    - name: Shut down VMs gracefully if powered on
</span></span><span class="line"><span class="cl">      community.vmware.vmware_guest_powerstate:
</span></span><span class="line"><span class="cl">        hostname: &#34;{{ vcenter_hostname }}&#34;
</span></span><span class="line"><span class="cl">        username: &#34;{{ vcenter_username }}&#34;
</span></span><span class="line"><span class="cl">        password: &#34;{{ vcenter_password }}&#34;
</span></span><span class="line"><span class="cl">        validate_certs: &#34;{{ validate_certs }}&#34;
</span></span><span class="line"><span class="cl">        name: &#34;{{ item.item }}&#34;
</span></span><span class="line"><span class="cl">        state: shutdown-guest
</span></span><span class="line"><span class="cl">        force: false
</span></span><span class="line"><span class="cl">      when: item.instance.hw_power_status == &#34;poweredOn&#34;
</span></span><span class="line"><span class="cl">      loop: &#34;{{ vm_info_results.results }}&#34;
</span></span><span class="line"><span class="cl">      register: shutdown_results
</span></span><span class="line"><span class="cl">      loop_control:
</span></span><span class="line"><span class="cl">        label: &#34;{{ item.item }}&#34;
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    - name: Wait for VMs to be powered off
</span></span><span class="line"><span class="cl">      community.vmware.vmware_guest_info:
</span></span><span class="line"><span class="cl">        hostname: &#34;{{ vcenter_hostname }}&#34;
</span></span><span class="line"><span class="cl">        username: &#34;{{ vcenter_username }}&#34;
</span></span><span class="line"><span class="cl">        password: &#34;{{ vcenter_password }}&#34;
</span></span><span class="line"><span class="cl">        datacenter: &#34;{{ vcenter_datacenter }}&#34;
</span></span><span class="line"><span class="cl">        validate_certs: &#34;{{ validate_certs }}&#34;
</span></span><span class="line"><span class="cl">        name: &#34;{{ item.item }}&#34;
</span></span><span class="line"><span class="cl">      register: vm_status
</span></span><span class="line"><span class="cl">      until: vm_status.instance.hw_power_status == &#34;poweredOff&#34;
</span></span><span class="line"><span class="cl">      retries: 20
</span></span><span class="line"><span class="cl">      delay: 15
</span></span><span class="line"><span class="cl">      loop: &#34;{{ vm_info_results.results }}&#34;
</span></span><span class="line"><span class="cl">      when: item.instance.hw_power_status == &#34;poweredOn&#34;
</span></span><span class="line"><span class="cl">      loop_control:
</span></span><span class="line"><span class="cl">        label: &#34;{{ item.item }}&#34;
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    - name: Display shutdown result
</span></span><span class="line"><span class="cl">      debug:
</span></span><span class="line"><span class="cl">        msg: &#34;VM {{ item.item }} ist erfolgreich heruntergefahren oder war bereits ausgeschaltet.&#34;
</span></span><span class="line"><span class="cl">      loop: &#34;{{ vm_info_results.results }}&#34;
</span></span><span class="line"><span class="cl">      loop_control:
</span></span><span class="line"><span class="cl">        label: &#34;{{ item.item }}&#34;
</span></span></code></pre></div><p>Use of vCenter:</p>
<ul>
<li>
<p>The playbook uses vCenter directly to manage the shutdown process, which avoids manually iterating through all ESXi hosts.
Graceful Shutdown:</p>
</li>
<li>
<p>The shutdown-guest option triggers a clean shutdown of the guest operating system running on the virtual ESXi servers.
Dynamic Verification:</p>
</li>
<li>
<p>The playbook dynamically filters the powered-on ESXi VMs and waits until their power state is confirmed as poweredOff.
Efficiency:</p>
</li>
<li>
<p>By leveraging vCenter and a loop with retries, the process is both clean and efficient.</p>
</li>
</ul>
<h3 id="master-shutdown-playbook">Master Shutdown Playbook</h3>
<p>To orchestrate the shutdown of the nested VCF lab and its virtual ESXi servers, we’ll create a master playbook similar to the Power-On master playbook. The inventory files remain the same as those used for the Power-On process, ensuring consistency and avoiding duplication.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-plaintext" data-lang="plaintext"><span class="line"><span class="cl">- name: Poweroff Nested VMs   
</span></span><span class="line"><span class="cl">  import_playbook: shutdown_esx.yml
</span></span><span class="line"><span class="cl">  vars:
</span></span><span class="line"><span class="cl">    vars_file: &#34;vcfvm_vars.yml&#34;
</span></span><span class="line"><span class="cl">- name: Poweroff Nested ESXi
</span></span><span class="line"><span class="cl">  import_playbook: shutdown_vcsa.yml
</span></span><span class="line"><span class="cl">  vars:
</span></span><span class="line"><span class="cl">    vars_file: &#34;vcfesx_vars.yml&#34;
</span></span></code></pre></div><p>Unlike the Power-On master playbook, the shutdown process does not require a pause or workaround. This is because during the shutdown, we can actively check if the respective VMs have already powered off using a loop. This makes the process cleaner and more efficient.</p>

    <aside class="admonition attention">
        <div class="admonition-title">
            <div class="icon"><svg xmlns="http://www.w3.org/2000/svg" class="feather feather-link" width="24" height="24" viewBox="0 0 24 24"
      fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
      <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path>
      <path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path>
   </svg></div><b>Attention</b>
        </div>
        <div class="admonition-content"><p>The playbooks presented in this article were generated with the help of AI and subsequently adjusted to work in my specific environment. While they function as intended for my use case, I strongly recommend exercising caution and thoroughly testing these playbooks in your own environment before implementing or relying on them.</p>
<p>Automation can be powerful, but every infrastructure is unique—always test in a controlled setting first!</p>
</div>
    </aside>
<h2 id="conclusion-is-chatgpt-useful-for-ansible">Conclusion: Is ChatGPT Useful for Ansible?</h2>
<p>From my perspective, the answer is both yes and no.</p>
<p>ChatGPT gave me a solid starting point and explained a lot of the foundational concepts, which was extremely helpful as a beginner with Ansible. However, it wasn’t perfect—there were several significant errors in the generated playbooks, and more than once, the AI proposed the same incorrect solution repeatedly.</p>
<p>Despite these challenges, I still found the process enjoyable. With some manual corrections and adjustments, I was able to create playbooks that worked for my specific environment. Within just a few hours, I achieved a usable result—something that would have taken considerably longer without ChatGPT&rsquo;s assistance.</p>
<p>Ultimately, while ChatGPT cannot replace expertise or thorough testing, it’s a powerful tool to accelerate development and simplify learning, especially when working with automation tools like Ansible.</p>
]]></content>
		</item>
		
		<item>
			<title>Homelab V5</title>
			<link>https://sdn-warrior.org/posts/homelab-v5/</link>
			<pubDate>Sat, 14 Dec 2024 02:00:26 +0100</pubDate>
			
			<guid>https://sdn-warrior.org/posts/homelab-v5/</guid>
			<description><![CDATA[My Homelab Journey: From Unraid Beginnings to Version 5]]></description>
			<content type="html"><![CDATA[<h2 id="my-homelab-journey-from-unraid-beginnings-to-version-5">My Homelab Journey: From Unraid Beginnings to Version 5</h2>
<p>Building and optimizing a homelab has always been a passion of mine. Since its inception, my homelab has gone through several iterations, constantly evolving to meet my goals of achieving maximum performance while minimizing power consumption, noise, and physical space requirements. Here is a snapshot of my journey, culminating in the current Version 5 of my homelab.</p>
<h2 id="the-beginning-unraid-with-custom-hardware">The Beginning: Unraid with Custom Hardware</h2>
<p>My homelab journey began with a custom-built Unraid server featuring an Intel i3 11th Generation processor and 64 GB of RAM. This setup acted as an all-in-one solution for storage, virtualization, and container workloads. I even conducted simple nested vSphere tests on this server during its early days. Today, the server is still in use as a storage and Docker host, although I have replaced the underlying hardware four times to keep up with evolving requirements.</p>

<figure><picture>
          <source srcset="/labv5/unraid_hu12871636074369548549.webp" type="image/webp">
          <source srcset="/labv5/unraid_hu3133507235213664422.jpeg" type="image/jpeg">
          <img src="/labv5/unraid_hu12871636074369548549.webp"alt="Homelab v1"  width="960"  height="1280" />
        </picture><figcaption>
            <p>My first Homelab</p>
          </figcaption></figure>
<p>The rack that I used from Version 1 through Version 4 of my homelab housed only 2 switches, a Pi3 and an old HP Elitedesk Client in V1, but it had to be replaced to accommodate the changes in Version 5.</p>
<h2 id="evolution-to-version-5">Evolution to Version 5</h2>
<p>Over the years, I continuously refined and upgraded the homelab. With my role at Evoila GmbH, my expectations for both myself and my homelab grew significantly. It quickly became clear that I needed different hardware to meet these new demands, especially as I aimed to conduct more extensive labs with NSX.</p>
<p>To start, I added a simple 3-node NUC cluster using 11th Generation Intel i5 processors. Additionally, I replaced the switches in my setup with multiple multispeed switches from Zyxel and QNAP. At the time, there were limited options on the market for 2.5 Gbps switches with management capabilities, resulting in a somewhat heterogeneous configuration.</p>
<p>Each iteration brought new hardware, better software configurations, and more ambitious goals. Over time, more technologies found their way into my lab, including a Fortinet FortiGate F40, BGP routing, and 10G switches. These advancements eventually culminated in Version 4 of my lab. However, as the lab grew, the rack ran out of space for further development, prompting the need for a complete rebuild, which led to the creation of Version 5. Now, in its fifth version, the lab has transformed into a powerful and efficient setup comprising.</p>
<h2 id="my-lab-philosophy">My Lab Philosophy</h2>
<p>My primary goal has always been to achieve the best possible performance with minimal power consumption, noise, and space requirements. To this end, I have standardized my homelab on Intel’s 13th Generation CPUs, which strike a great balance between power efficiency and computational capability.</p>
<h2 id="lab-overview">Lab Overview</h2>
<p>In Lab Version 5, I have three clusters:</p>
<h3 id="management-cluster">Management Cluster:</h3>
<p>This cluster is powered by an Intel NUC i3 13th Generation, which serves as the always-on management node. The ESXi server in this cluster hosts several key VMs:</p>
<ul>
<li>A Windows 11 VM with tools like Hugo, Go, and GitHub for managing this blog.</li>
<li>LogInsight and FortiAnalyzer.</li>
<li>A vCenter server.</li>
<li>An mDNS Repeater for Smart Home integration.</li>
<li>Homebridge for managing smart devices.</li>
<li>NetBox for network documentation.</li>
<li>A Veeam server for backups.</li>
</ul>
<p>Additionally, two VMs on my Unraid server contribute to the management cluster:</p>
<ul>
<li>A Root CA.</li>
<li>A Domain Controller, which primarily supports the labs.</li>
</ul>
<p>The Unraid server also runs several containers, including:</p>
<ul>
<li>DNS servers.</li>
<li>An Excalidraw instance.</li>
<li>Various other tools.</li>
</ul>
<p>For redundancy, I run a backup DNS server on a Raspberry Pi 3 to ensure DNS functionality during storage maintenance. I use AdGuard Home as my primary DNS server, which blocks ads and forwards DNS queries to my lab.home domain managed by the Active Directory server.</p>
<h3 id="compute-cluster">Compute Cluster:</h3>
<p>My compute cluster consists of three Intel NUCs of the 13th Generation, each equipped with 64 GB of RAM and a 2TB NVMe drive. Due to the P/E core architecture, these NUCs do not support Hyperthreading but offer 12 cores (4P + 8E). Based on my experience, the performance with E cores enabled is better than using 4 P cores with Hyperthreading. Each NUC features dual 2.5G network interfaces and is connected to my iSCSI storage. This cluster runs standard nested labs, such as my NSX lab and AVI load balancer labs. The performance is sufficient for many labs, making it a reliable and frequently used part of my setup.</p>
<p>The Compute Cluster in Lab Version 5 has the following total resources:</p>
<ul>
<li>192 GB RAM across 3 NUCs</li>
<li>6 TB NVMe storage (2 TB per NUC)</li>
<li>8 TB shared storage</li>
<li>36 CPU cores (12 cores per NUC)</li>
</ul>
<h3 id="performance-cluster">Performance Cluster:</h3>
<p>My performance cluster consists of four MinisForum MS-01 units, each featuring an Intel i9 processor with 14 cores (6P + 8E), 64 GB of physical RAM, and a 400% memory tiering configuration. Each MS-01 includes 2 TB of local NVMe storage and an additional 1 TB PCIe4 NVMe drive for memory tiering. With onboard dual 10GbE networking, the MS-01 units are ideal for demanding labs, such as a complete VCF deployment including an HCX proof of concept where I live-migrated VMs between my NSX lab and the VCF lab. The MS-01 units are also used for vSAN labs. Additionally, Intel vPro support allows for efficient remote management. This cluster provides:</p>
<ul>
<li>56 CPU cores (14 cores per MS-01).</li>
<li>1280 GB of RAM (64 GB physical per unit with memory tiering).</li>
<li>8 TB of local NVMe storage (2 TB per unit).</li>
<li>4 TB of NVMe storage for memory tiering (1 TB per unit).</li>
<li>8 TB shared Storage</li>
</ul>
<h3 id="network">Network</h3>
<p>My network consists of multiple MikroTik switches. The centerpiece is my ToR (Top of Rack) switch, the MikroTik CRS309, which can route at line speed thanks to hardware offloading. This switch hosts all lab-relevant gateways and networks, ensuring they don&rsquo;t need to be routed through my Fortinet FortiGate F40. The servers themselves are connected to two access switches: the NUCs via dual 2.5Gbps connections, and the MS-01 units via 10Gbps connections per switch.
I also have a service router (RB5009) that establishes a VPN tunnel to a fellow homelabber. Through this connection, I can utilize his Kubernetes resources, and we&rsquo;ve even tested the NSX Application Platform (NAPP) together. <a href="https://marschall.systems/">Visit marschall.systems</a></p>

<figure><a href="plan.png"><picture>
          <source srcset="/labv5/plan_hu4235498138432559503.webp" type="image/webp">
          <source srcset="/labv5/plan_hu7818108223675192909.jpg" type="image/jpeg">
          <img src="/labv5/plan_hu4235498138432559503.webp"alt="Network setup"  width="6206"  height="3968" />
        </picture></a><figcaption>
            <p>Network setup (click to enlarge)</p>
          </figcaption></figure>
<p>My network employs dynamic routing, with eBGP as the primary protocol peering all critical components. My BGP NSX lab peers directly with my ToR switch, ensuring high efficiency and seamless integration with the rest of the network. For my labs, I utilize both OSPF and BGP. My OSPF lab runs on two virtual ArubaCX switches and a VyOS router, which has an IP in my standard client network and provides internet access to the OSPF lab via NAT.</p>

<figure><a href="bgp.png"><picture>
          <source srcset="/labv5/bgp_hu13323852698658684218.webp" type="image/webp">
          <source srcset="/labv5/bgp_hu5788182371385841057.jpg" type="image/jpeg">
          <img src="/labv5/bgp_hu13323852698658684218.webp"alt="Network bgp setup"  width="3111"  height="2917" />
        </picture></a><figcaption>
            <p>BGP setup (click to enlarge)</p>
          </figcaption></figure>
<h3 id="storage">Storage</h3>
<p>As my primary storage solution, I use my Unraid server. After implementing several iSCSI optimizations <a href="https://sdn-warrior.org/posts/iscsi-tuning/">(my iSCSI Blog post)</a> and installing the iSCSI Target plugin <a href="https://sdn-warrior.org/posts/unraid-storage/">(my Unraid Blog post)</a>, the server provides a performant iSCSI storage capable of achieving around 2000 MB/s for both read and write operations.</p>

    <aside class="admonition info">
        <div class="admonition-title">
            <div class="icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-info">
      <circle cx="12" cy="12" r="10"></circle>
      <line x1="12" y1="16" x2="12" y2="12"></line>
      <line x1="12" y1="8" x2="12.01" y2="8"></line>
   </svg></div><b>Info</b>
        </div>
        <div class="admonition-content">An iSCSI Target in Linux refers to a service or daemon that enables a Linux server to present storage devices over the network using the iSCSI protocol. This allows other machines, known as iSCSI Initiators, to connect to and use these storage devices as if they were local drives.</div>
    </aside>
<h2 id="firewall">Firewall</h2>
<p>As my firewall, I use a Fortinet FortiGate F40, which I’ve had for two years. I am fortunate to have access to an NFR/LAB license through my employer at an affordable price. The FortiGate F40 handles both firewalling and IDS/IPS functionality. Additionally, I operate it in a dual-stack configuration and leverage its SD-WAN feature to load balance two WAN connections: 5G and DSL.</p>
<h2 id="lessons-learned-and-future-goals">Lessons Learned and Future Goals</h2>
<ul>
<li>
<p>Performance vs. Efficiency: Achieving the right balance between performance and efficiency requires meticulous planning and experimentation. Each hardware choice and configuration tweak contributes to the overall success of the setup.</p>
</li>
<li>
<p>Automation: In the future, I plan to incorporate more automation into my homelab. To achieve this, I have started experimenting with Terraform to streamline deployments and configurations.</p>
</li>
<li>
<p>Scaling Smartly: As my lab has grown, managing power, cooling, and network configurations has become increasingly important.</p>
</li>
<li>
<p>Continuous Improvement: My homelab is a perpetual work in progress. With each iteration, I discover new ways to optimize and expand its capabilities.</p>
</li>
</ul>
<h2 id="current-setup-bill-of-materials-bom">Current Setup: Bill of Materials (BOM)</h2>
<table>
  <thead>
      <tr>
          <th>Quantity</th>
          <th>Component</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td><strong>Server</strong></td>
          <td></td>
      </tr>
      <tr>
          <td>4</td>
          <td>Minisforum MS-01 i9 13.Gen</td>
      </tr>
      <tr>
          <td>3</td>
          <td>Asus NUC Pro i7 13.Gen</td>
      </tr>
      <tr>
          <td>1</td>
          <td>Asus NUC Pro i3 13.Gen</td>
      </tr>
      <tr>
          <td><strong>Network</strong></td>
          <td></td>
      </tr>
      <tr>
          <td>2</td>
          <td>Mikrotik CRS309-1G-8S+IN</td>
      </tr>
      <tr>
          <td>1</td>
          <td>MikroTik L009UiGS-RM</td>
      </tr>
      <tr>
          <td>1</td>
          <td>Mikrotik CRS326-4C +20G+2Q</td>
      </tr>
      <tr>
          <td>1</td>
          <td>Mikrotik RB5009UG+S+IN</td>
      </tr>
      <tr>
          <td><strong>Storage</strong></td>
          <td></td>
      </tr>
      <tr>
          <td>1</td>
          <td>Intel NUC Extreme i7 11.Gen</td>
      </tr>
      <tr>
          <td><strong>USV</strong></td>
          <td></td>
      </tr>
      <tr>
          <td>1</td>
          <td>APC Back-UPS Pro 1300VA BR1300MI</td>
      </tr>
      <tr>
          <td><strong>Firewall</strong></td>
          <td></td>
      </tr>
      <tr>
          <td>1</td>
          <td>Fortinet Fortigate F40</td>
      </tr>
      <tr>
          <td><strong>Other</strong></td>
          <td></td>
      </tr>
      <tr>
          <td>1</td>
          <td>21U Rack</td>
      </tr>
      <tr>
          <td>1</td>
          <td>DAC Cable / Ethernet Cable</td>
      </tr>
      <tr>
          <td>2</td>
          <td>Cable Management</td>
      </tr>
      <tr>
          <td>2</td>
          <td>Rack Mount MS-01</td>
      </tr>
      <tr>
          <td>1</td>
          <td>Rack Mount NUC</td>
      </tr>
      <tr>
          <td>2</td>
          <td>Air Vent</td>
      </tr>
      <tr>
          <td>4</td>
          <td>Rack PSU</td>
      </tr>
  </tbody>
</table>
<p>You can also find the detailed BOM with prices <a href="https://docs.google.com/spreadsheets/d/1XK32KJWiLBMlKLlPSKNDwBaX2mg4fmGFN2aEA3SIsEc/edit?pli=1&amp;gid=0#gid=0">here</a>, which I update regularly to reflect any changes in my setup.</p>
<h2 id="final-result">Final result</h2>
<p>
<figure><picture>
          <source srcset="/labv5/rackmount_hu1092622698464279345.webp" type="image/webp">
          <source srcset="/labv5/rackmount_hu15529659375378009796.jpg" type="image/jpeg">
          <img src="/labv5/rackmount_hu1092622698464279345.webp"alt="Rackmount"  width="1600"  height="1200" />
        </picture><figcaption>
            <p>Rackmount MS-01</p>
          </figcaption></figure>

<figure><picture>
          <source srcset="/labv5/rackmount2_hu12242692340557408737.webp" type="image/webp">
          <source srcset="/labv5/rackmount2_hu16189033617681113906.jpg" type="image/jpeg">
          <img src="/labv5/rackmount2_hu12242692340557408737.webp"alt="Rackmount2"  width="2250"  height="1500" />
        </picture><figcaption>
            <p>Rackmount MS-01</p>
          </figcaption></figure>

<figure><picture>
          <source srcset="/labv5/cable_hu10763548189357929237.webp" type="image/webp">
          <source srcset="/labv5/cable_hu12300581878864859744.jpg" type="image/jpeg">
          <img src="/labv5/cable_hu10763548189357929237.webp"alt="Rack"  width="1500"  height="1000" />
        </picture><figcaption>
            <p>Rack view top</p>
          </figcaption></figure>

<figure><picture>
          <source srcset="/labv5/rack_hu9852508572301551615.webp" type="image/webp">
          <source srcset="/labv5/rack_hu10464460824911337438.jpg" type="image/jpeg">
          <img src="/labv5/rack_hu9852508572301551615.webp"alt="Rack"  width="1200"  height="1600" />
        </picture><figcaption>
            <p>Rack view</p>
          </figcaption></figure></p>
]]></content>
		</item>
		
		<item>
			<title>How to use QoS in NSX</title>
			<link>https://sdn-warrior.org/posts/nsx-qos/</link>
			<pubDate>Tue, 10 Dec 2024 02:00:40 +0100</pubDate>
			
			<guid>https://sdn-warrior.org/posts/nsx-qos/</guid>
			<description><![CDATA[How to use QoS in NSX]]></description>
			<content type="html"><![CDATA[<h2 id="introduction">Introduction</h2>
<p>Quality of Service (QoS) is a critical aspect of network performance management, especially in complex environments where NSX is deployed. NSX provides powerful QoS capabilities at both the gateway and segment levels, enabling fine-grained control over traffic prioritization and bandwidth allocation. However, understanding the differences between these two levels of QoS implementation is essential for optimizing network performance.</p>
<p>In this article, we’ll delve into how QoS functions on the gateway versus the segment in NSX, explore their respective use cases, and provide insights into selecting the right approach for your network needs. Whether you&rsquo;re managing inter-tenant traffic or fine-tuning internal traffic flows, mastering these distinctions will empower you to make informed decisions and maximize the efficiency of your NSX deployment.</p>
<h2 id="qos-on-an-nsx-segment">QoS on an NSX Segment</h2>
<p>Quality of Service (QoS) on an NSX segment focuses on managing traffic flows within a specific segment, providing comprehensive control over bandwidth and traffic priorities. Unlike gateway-level QoS, which typically manages north-south traffic at the boundary of the network, segment-level QoS applies to all traffic associated with virtual machines (VMs) on the segment, regardless of direction.</p>
<p>To implement QoS at this level, you must first create a Segment Profile, which defines the QoS policies. This includes settings such as ingress and egress traffic shaping, bandwidth guarantees, and DSCP marking. Once configured, this profile is attached to the segment, ensuring that the specified QoS policies are applied consistently to all VMs on that segment.</p>

<figure><a href="qos1.png"><picture>
          <source srcset="/nsx-qos/qos1_hu9376800852532490140.webp" type="image/webp">
          <source srcset="/nsx-qos/qos1_hu16011686681575812025.jpg" type="image/jpeg">
          <img src="/nsx-qos/qos1_hu9376800852532490140.webp"alt="QoS Profile"  width="1157"  height="466" />
        </picture></a><figcaption>
            <p>QoS Profile</p>
          </figcaption></figure>

    <aside class="admonition info">
        <div class="admonition-title">
            <div class="icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-info">
      <circle cx="12" cy="12" r="10"></circle>
      <line x1="12" y1="16" x2="12" y2="12"></line>
      <line x1="12" y1="8" x2="12.01" y2="8"></line>
   </svg></div><b>Info</b>
        </div>
        <div class="admonition-content">A crucial aspect of segment-level QoS is that it impacts all traffic originating from or destined for VMs on the segment. For example, if a Segment Profile specifies a guaranteed bandwidth of 30 Mbps, each VM on that segment will have this guarantee for all traffic, whether it is east-west within the same data center or north-south to external networks.</div>
    </aside>
<h2 id="explanation-of-segment-qos-profile-parameters-in-nsx">Explanation of Segment QoS Profile Parameters in NSX</h2>
<h3 id="mode"><strong>Mode</strong></h3>
<p>Defines how DSCP (Differentiated Services Code Point) values are handled for traffic originating from or destined for a logical port.</p>
<ul>
<li>
<p><strong>Trusted Mode</strong>:</p>
<ul>
<li>The DSCP value from the <strong>inner packet header</strong> (original header) is copied to the <strong>outer IP header</strong> (tunnel header) for IP/IPv6 traffic.</li>
<li>For non-IP/IPv6 traffic, the default DSCP value (0) is used for the outer IP header.</li>
<li>Supported only on <strong>overlay-based logical ports</strong>.</li>
</ul>
</li>
<li>
<p><strong>Untrusted Mode</strong>:</p>
<ul>
<li>For <strong>overlay-based logical ports</strong>, the configured DSCP value is applied to the outer IP header, regardless of the inner packet type.</li>
<li>For <strong>VLAN-based logical ports</strong>, the configured DSCP value is applied to the IP/IPv6 packets&rsquo; outer IP header.</li>
<li>DSCP values can range from 0 to 63.</li>
</ul>
</li>
</ul>

    <aside class="admonition info">
        <div class="admonition-title">
            <div class="icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-info">
      <circle cx="12" cy="12" r="10"></circle>
      <line x1="12" y1="16" x2="12" y2="12"></line>
      <line x1="12" y1="8" x2="12.01" y2="8"></line>
   </svg></div><b>Info</b>
        </div>
        <div class="admonition-content">DSCP settings affect only tunneled traffic and do not apply to traffic within the same hypervisor.</div>
    </aside>
<h3 id="priority"><strong>Priority</strong></h3>
<p>Specifies the <strong>DSCP priority value</strong>, which determines the level of importance for packets. The DSCP priority values range from <strong>0 to 63</strong>, with higher values indicating higher priority traffic.</p>
<h3 id="class-of-service-cos"><strong>Class of Service (CoS)</strong></h3>
<p>Defines the <strong>CoS value</strong>, applicable to VLAN-based logical ports.</p>
<ul>
<li>CoS groups similar traffic types and assigns a service priority level for each type.</li>
<li>Lower-priority traffic may experience reduced throughput or be dropped to ensure better performance for higher-priority traffic.</li>
<li>CoS can also be configured for VLAN ID 0 packets.</li>
<li>The CoS value ranges from <strong>0 to 7</strong>, where <strong>0</strong> indicates best-effort service.</li>
</ul>
<h3 id="ingress">Ingress</h3>
<p>Configures traffic shaping for <strong>outbound traffic</strong> from the VM to the logical network.</p>
<ul>
<li><strong>Average Bandwidth</strong>: The average rate of outbound traffic to prevent network congestion.</li>
<li><strong>Peak Bandwidth</strong>: The maximum traffic rate allowed to support bursts.</li>
<li><strong>Burst Size</strong>: Defines the maximum data size for a traffic burst, calculated as:</li>
</ul>
$$
\frac{\text{Peak Bandwidth (in bits per second)} \times \text{Burst Duration (in seconds)}}{8} = \text{Burst Size (in Bytes)}
$$<p>For example, with an average bandwidth of 30 Mbps, a peak of 60 Mbps, and a burst duration of 0.1 seconds, the burst size would be:</p>
$$
\frac{{60000000} \text{(bits per second)} \times {0.1} \text{(seconds)}}{8} = 750000 \text{ bytes}
$$<p>Default value is 0, which disables rate limiting.</p>
<h3 id="ingress-broadcast">Ingress Broadcast</h3>
<p>Configures traffic shaping for broadcast traffic sent from the VM to the logical network.
Works similarly to the general ingress settings, allowing custom limits for average bandwidth, peak bandwidth, and burst size for broadcast traffic.
Default value is 0, which disables rate limiting for ingress broadcast traffic.</p>
<h3 id="egress">Egress</h3>
<p>Configures traffic shaping for inbound traffic from the logical network to the VM.
Allows setting limits on the average bandwidth, peak bandwidth, and burst size for inbound traffic.
Default value is 0, which disables rate limiting on egress traffic.
By configuring these parameters effectively, you can ensure traffic prioritization, manage congestion, and optimize bandwidth usage for both overlay-based and VLAN-based logical ports in NSX environments.</p>
<h2 id="test-scenario-evaluating-qos-at-the-segment-level">Test Scenario: Evaluating QoS at the segment level</h2>
<p>To demonstrate the differences between a setup with and without QoS, I have created a test environment consisting of two T1 routers, each connected to its own segment and hosting VMs for testing. Both T1 routers are connected to the same Tier-0 (T0) router, providing a shared Internet connection for testing north-south traffic scenarios.</p>

<figure><a href="topo.png"><picture>
          <source srcset="/nsx-qos/topo_hu6475854221281022681.webp" type="image/webp">
          <source srcset="/nsx-qos/topo_hu2349874414644969993.jpg" type="image/jpeg">
          <img src="/nsx-qos/topo_hu6475854221281022681.webp"alt="NSX Enviroment"  width="912"  height="936" />
        </picture></a><figcaption>
            <p>NSX Test Enviroment</p>
          </figcaption></figure>
<p>This test specifically focuses on <strong>QoS at the segment level</strong>, with the primary goal of limiting the VMs on the segment <code>LS-10.10.20.1</code> to a maximum bandwidth of <strong>30 Mbps</strong> using a QoS profile.</p>
<h3 id="t1-router-1-t1-bgp-no-qos"><strong>T1 Router 1: T1-BGP No QoS</strong></h3>
<ul>
<li><strong>Segment</strong>: <code>LS-10.10.10.1</code></li>
<li><strong>QoS Policy</strong>: None applied</li>
<li><strong>VM</strong>: <code>Alpine01</code> IP Adress <code>10.10.10.10</code>
<ul>
<li>Running an instance of iPerf to act as a traffic generator and receiver.</li>
</ul>
</li>
</ul>
<p>This router and segment represent a baseline configuration without any QoS policies, allowing for a comparison of unshaped and unprioritized traffic.</p>
<h3 id="t1-router-2-t1-bgp-qos"><strong>T1 Router 2: T1-BGP QoS</strong></h3>
<ul>
<li><strong>Segment</strong>: <code>LS-10.10.20.1</code></li>
<li><strong>QoS Policy</strong>: A custom QoS profile is applied to this segment, specifically configured to limit bandwidth to 30 Mbps for all associated VMs.</li>
<li><strong>VM</strong>: <code>Alpine02</code> IP Adresse <code>10.10.20.10</code>
<ul>
<li>Equipped with iPerf for traffic generation and reception.</li>
<li>Includes a browser for additional testing and validation purposes.</li>
</ul>
</li>
</ul>
<h3 id="purpose-of-the-test"><strong>Purpose of the Test</strong></h3>
<p>The primary goal of this test is to validate <strong>QoS at the segment level</strong>, focusing on the following:</p>
<ul>
<li>Verifying that VMs connected to <code>LS-10.10.20.1</code> are effectively limited to a bandwidth of 30 Mbps for egress traffic.</li>
<li>Demonstrating that the QoS profile, configured as ingress-only, limits traffic originating from <code>Alpine02</code> to other VMs or the Internet, while traffic from <code>Alpine01</code> to <code>Alpine02</code> remains unrestricted.</li>
<li>Comparing traffic behavior between a segment with and without an applied QoS profile.</li>
<li>Assessing performance consistency under traffic shaping policies.</li>
<li>Measuring the impact of the 30 Mbps limit on both east-west and north-south traffic.</li>
</ul>
<p>These behaviors will be demonstrated using iPerf measurements, highlighting the effectiveness and boundaries of the configured QoS profile.
By analyzing the test results, we can confirm the effectiveness of the QoS profile in limiting segment-level bandwidth and understand its implications for overall network performance.</p>
<h3 id="first-test-iperf-test-from-alpine02-to-alpine03">First Test: iPerf Test from Alpine02 to Alpine03</h3>
<h4 id="test-configuration"><strong>Test Configuration</strong></h4>
<ul>
<li><strong>Source</strong>: Alpine02 (<code>10.10.20.10</code>) connected to the segment <code>LS-10.10.20.1</code> with the QoS profile applied (ingress limited to 30 Mbps).</li>
<li><strong>Destination</strong>: Alpine03 (<code>10.10.10.10</code>) connected to the segment <code>LS-10.10.10.1</code> with no QoS policy applied.</li>
<li><strong>Tool</strong>: iPerf3</li>
<li><strong>Command</strong>: <code>iperf3 -c 10.10.10.10</code></li>
</ul>
<h4 id="result-summary"><strong>Result Summary</strong></h4>
<ul>
<li><strong>Average Sender Bitrate</strong>: 32.6 Mbps</li>
<li><strong>Average Receiver Bitrate</strong>: 30.2 Mbps</li>
<li><strong>Key Observation</strong>: The sender&rsquo;s bitrate fluctuates around the 30 Mbps mark, as expected due to the QoS ingress limitation applied to the LS-10.10.20.1 segment. Receiver bitrate is consistent with the QoS configuration, confirming that the profile effectively limits traffic from Alpine02 to Alpine03.</li>
</ul>
<pre tabindex="0"><code>alpine02:~# iperf3 -c 10.10.10.10
Connecting to host 10.10.10.10, port 5201
[  5] local 10.10.20.10 port 40468 connected to 10.10.10.10 port 5201
[ ID] Interval           Transfer     Bitrate         Retr  Cwnd
[  5]   0.00-1.00   sec  6.12 MBytes  51.3 Mbits/sec    0    522 KBytes       
[  5]   1.00-2.00   sec  4.25 MBytes  35.7 Mbits/sec    0    522 KBytes       
[  5]   2.00-3.00   sec  3.25 MBytes  27.3 Mbits/sec    0    522 KBytes       
[  5]   3.00-4.00   sec  3.12 MBytes  26.2 Mbits/sec    0    522 KBytes       
[  5]   4.00-5.00   sec  4.25 MBytes  35.7 Mbits/sec    0    522 KBytes       
[  5]   5.00-6.00   sec  3.12 MBytes  26.2 Mbits/sec    0    522 KBytes       
[  5]   6.00-7.00   sec  4.25 MBytes  35.6 Mbits/sec    0    522 KBytes       
[  5]   7.00-8.00   sec  3.12 MBytes  26.2 Mbits/sec    2    365 KBytes       
[  5]   8.00-9.00   sec  3.25 MBytes  27.3 Mbits/sec    0    365 KBytes       
[  5]   9.00-10.00  sec  4.12 MBytes  34.6 Mbits/sec    0    365 KBytes       
- - - - - - - - - - - - - - - - - - - - - - - - -
[ ID] Interval           Transfer     Bitrate         Retr
[  5]   0.00-10.00  sec  38.9 MBytes  32.6 Mbits/sec    2        sender
[  5]   0.00-10.00  sec  36.0 MBytes  30.2 Mbits/sec             receiver
</code></pre><h3 id="second-test-iperf-test-from-alpine01-to-alpine02">Second Test: iPerf Test from Alpine01 to Alpine02</h3>
<h4 id="test-configuration-1"><strong>Test Configuration</strong></h4>
<ul>
<li><strong>Source</strong>: Alpine01 (<code>10.10.10.10</code>) connected to the segment <code>LS-10.10.10.1</code> with no QoS policy applied.</li>
<li><strong>Destination</strong>: Alpine02 (<code>10.10.20.10</code>) connected to the segment <code>LS-10.10.20.1</code> with the QoS profile applied (ingress limited to 30 Mbps).</li>
<li><strong>Tool</strong>: iPerf3</li>
<li><strong>Command</strong>: <code>iperf3 -c 10.10.20.10</code></li>
</ul>
<h4 id="result-summary-1"><strong>Result Summary</strong></h4>
<ul>
<li><strong>Average Sender Bitrate</strong>: 2.25 Gbps</li>
<li><strong>Average Receiver Bitrate</strong>: 2.25 Gbps</li>
<li><strong>Key Observation</strong>: The traffic from Alpine01 to Alpine02 is not limited by the QoS profile, as expected. This confirms the QoS profile applies only to ingress traffic on the <code>LS-10.10.20.1</code> segment.</li>
</ul>
<pre tabindex="0"><code>alpine01:~# iperf3 -c 10.10.20.10
Connecting to host 10.10.20.10, port 5201
[  5] local 10.10.10.10 port 50482 connected to 10.10.20.10 port 5201
[ ID] Interval           Transfer     Bitrate         Retr  Cwnd
[  5]   0.00-1.00   sec   269 MBytes  2.25 Gbits/sec  150   2.08 MBytes       
[  5]   1.00-2.00   sec   268 MBytes  2.25 Gbits/sec    0   2.19 MBytes       
[  5]   2.00-3.00   sec   268 MBytes  2.25 Gbits/sec  172   1.21 MBytes       
[  5]   3.00-4.00   sec   267 MBytes  2.24 Gbits/sec  150    997 KBytes       
[  5]   4.00-5.00   sec   267 MBytes  2.24 Gbits/sec   11    673 KBytes       
[  5]   5.00-6.00   sec   268 MBytes  2.24 Gbits/sec    0    928 KBytes       
[  5]   6.00-7.00   sec   268 MBytes  2.25 Gbits/sec    0   1.10 MBytes       
[  5]   7.00-8.00   sec   268 MBytes  2.25 Gbits/sec    0   1.27 MBytes       
[  5]   8.00-9.00   sec   269 MBytes  2.25 Gbits/sec    0   1.42 MBytes       
[  5]   9.00-10.00  sec   268 MBytes  2.25 Gbits/sec    6   1.11 MBytes       
- - - - - - - - - - - - - - - - - - - - - - - - -
[ ID] Interval           Transfer     Bitrate         Retr
[  5]   0.00-10.00  sec  2.62 GBytes  2.25 Gbits/sec  489             sender
[  5]   0.00-10.01  sec  2.62 GBytes  2.25 Gbits/sec                  receiver

iperf Done.
</code></pre>
    <aside class="admonition attention">
        <div class="admonition-title">
            <div class="icon"><svg xmlns="http://www.w3.org/2000/svg" class="feather feather-link" width="24" height="24" viewBox="0 0 24 24"
      fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
      <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path>
      <path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path>
   </svg></div><b>Note on QoS Profiles and Perspective</b>
        </div>
        <div class="admonition-content"><p>When working with QoS profiles on a segment, it is important to understand that the traffic shaping perspective is always <strong>from the segment&rsquo;s point of view</strong>. For example:</p>
<ul>
<li>A profile that shapes <strong>ingress traffic</strong> is applied to traffic <strong>entering the segment</strong>.</li>
<li>From the VM&rsquo;s perspective, this same traffic is considered <strong>egress traffic</strong> (leaving the VM).
This distinction can initially be confusing but is crucial for correctly interpreting and configuring QoS policies in NSX environments.</li>
</ul></div>
    </aside>
<h4 id="qos-profiles-apply-to-all-vms-on-the-segment"><strong>QoS Profiles Apply to All VMs on the Segment</strong></h4>
<p>QoS profiles applied at the segment level are effective for <strong>all VMs connected to that segment</strong>. In our example, this means:</p>
<ul>
<li>Every VM connected to the segment <code>LS-10.10.20.1</code> is limited to a maximum <strong>outgoing traffic rate of 30 Mbps</strong>.</li>
<li>The QoS profile ensures this bandwidth limit is enforced uniformly, regardless of the specific VM or traffic destination.</li>
</ul>
<p>This behavior highlights the segment-wide scope of QoS policies, making them a powerful tool for managing traffic flow consistently across all connected VMs.</p>
<h2 id="qos-on-t1-gateway-level">QoS on T1 Gateway Level</h2>
<p>To evaluate QoS at the Tier-1 Gateway level, the test conditions remain the same as in the segment-level QoS tests. However, the QoS profile will now be applied directly to the T1 Gateway. Before proceeding, the QoS profile is removed from the segment <code>LS-10.10.20.1</code>.</p>
<h2 id="qos-profile-configuration-on-t1-gateway"><strong>QoS Profile Configuration on T1 Gateway</strong></h2>
<p>For the T1 Gateway, the QoS profile is applied with the following characteristics:</p>
<ul>
<li><strong>Type</strong>: Ingress</li>
<li><strong>Committed Bandwidth</strong>: 30 Mbps</li>
<li><strong>Burst Size</strong>: Configured based on constraints (explained below).</li>
</ul>
<p>Unlike segment-level QoS, the T1 Gateway QoS profile allows only the configuration of <strong>Committed Bandwidth</strong> and <strong>Burst Size</strong> in bytes. The direction (Ingress or Egress) is explicitly specified when applying the profile to the gateway.</p>
<h3 id="limitations-of-gateway-qos-profiles"><strong>Limitations of Gateway QoS Profiles</strong></h3>
<ul>
<li><strong>Supported only on Tier-1 Gateways</strong>:
<ul>
<li>QoS profiles can only be applied to T1 Gateways, not to Tier-0 Gateways or any other components.</li>
</ul>
</li>
<li><strong>Applies only to North-South Traffic</strong>:
<ul>
<li>QoS policies on Tier-1 Gateways are limited to north-south traffic and do not affect overlay segments or service interfaces connected to the gateway.</li>
</ul>
</li>
<li><strong>Requires Active-Standby Mode</strong>:
<ul>
<li>The T1 Gateway must be in active-standby mode with an NSX Edge cluster for the QoS profile to function.</li>
</ul>
</li>
<li><strong>Not Supported for Distributed Routing</strong>:
<ul>
<li>Gateways configured for distributed routing cannot have QoS profiles applied.</li>
</ul>
</li>
</ul>
<h3 id="burst-size-calculation"><strong>Burst Size Calculation</strong></h3>
<p>The calculation of the <strong>Burst Size</strong> for a T1 Gateway is more complex due to additional constraints. The Burst Size must satisfy the following:</p>
<ol>
<li>
<p><strong>Token Refill per Interval</strong>:</p>
\[B \geq \frac{R \times 1000000 \times I}{1000 \times 8} \]<p>
Where:</p>
</li>
</ol>
<ul>
<li>\( B \): Burst Size in Bytes</li>
<li>\( R \): Committed Bandwidth in Mbps</li>
<li>\( I \): Refill Interval in milliseconds (e.g., 1 ms)</li>
</ul>
<ol start="2">
<li><strong>Minimum Refill Interval</strong>:
\[ B \geq \frac{R \times 1000000 \times 1}{1,000 \times 8} \]</li>
</ol>
<ul>
<li>The minimum interval \( I \) is 1 ms to account for dataplane CPU usage.</li>
</ul>
<ol start="3">
<li><strong>MTU Constraint</strong>:
\[ B \geq MTU \] of the Service Router (SR) port.</li>
</ol>
<ul>
<li>The Burst Size must accommodate at least one full MTU-size packet.</li>
</ul>
<p>The effective Burst Size must satisfy all three constraints. Therefore, the configured Burst Size is determined as:</p>
\[
B = \text{Max} \left( \frac{R \times 1000000 \times I}{1000 \times 8}, \frac{R \times 1000000 \times 1}{1000 \times 8}, MTU \right)
\]<h3 id="burst-size-calculation-example">Burst Size Calculation Example</h3>
<h4 id="parameters"><strong>Parameters</strong></h4>
<ul>
<li>\( R \): <strong>Committed Bandwidth</strong> = 30 Mbps</li>
<li>\( I \): <strong>Refill Interval</strong> = 1000 ms (1 second)</li>
<li>\( MTU \): <strong>Maximum Transmission Unit</strong> = 1500 bytes</li>
</ul>
<p>The Burst Size \( B \) must satisfy the following constraints:</p>
<h4 id="1-token-refill-per-interval"><strong>1. Token Refill per Interval</strong></h4>
\[
B \geq \frac{R \times 1000000 \times I}{1000 \times 8}
\]<p>
Substitute the values:
</p>
\[
B \geq \frac{30 \times 1000000 \times 1000}{1000 \times 8}
\]\[
B \geq \frac{30000000000}{8000}
\]\[
B \geq 3750000 \, \text{bytes}
\]<h4 id="2-minimum-refill-interval"><strong>2. Minimum Refill Interval</strong></h4>
\[
B \geq \frac{R \times 1000000 \times 1000}{1000 \times 8}
\]<p>
This calculation remains the same as in the previous case since \( I = 1000 \, \text{ms} \):
</p>
\[
B \geq 3750000 \, \text{bytes}
\]<h4 id="3-mtu-constraint"><strong>3. MTU Constraint</strong></h4>
\[
B \geq MTU
\]\[
B \geq 1500 \, \text{bytes}
\]<h4 id="final-burst-size"><strong>Final Burst Size</strong></h4>
<p>The Burst Size \( B \) must satisfy <strong>all three constraints</strong>, so:
</p>
\[
B = \text{Max}(3750000, 3750000, 1500)
\]\[
B = 3750000 \, \text{bytes}
\]<h3 id="result"><strong>Result</strong></h3>
<p>The minimum Burst Size required is <strong>3750000 bytes</strong> to satisfy all constraints with the given parameters.</p>

    <aside class="admonition info">
        <div class="admonition-title">
            <div class="icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-info">
      <circle cx="12" cy="12" r="10"></circle>
      <line x1="12" y1="16" x2="12" y2="12"></line>
      <line x1="12" y1="8" x2="12.01" y2="8"></line>
   </svg></div><b>Minimum Refill Interval</b>
        </div>
        <div class="admonition-content">Use the <code>get dataplane</code> command from the NSX Edge CLI to retrieve the time interval, Qos_wakeup_interval_ms. The default value for Qos_wakeup_interval_ms is 50ms. However, this value is automatically adjusted by the dataplane based on the QoS configuration. In my lab, the QoS_wakeup_interval is relatively high, which is partly due to my hardware and the fact that it is a nested lab. In production environments, this value is typically lower.</div>
    </aside>
<h3 id="implementation-for-this-test"><strong>Implementation for This Test</strong></h3>
<ul>
<li>The T1 Gateway is configured with a 30 Mbps Committed Bandwidth and a Burst Size that satisfies the constraints above.</li>
<li>The profile is applied in <strong>Ingress</strong> mode to test incoming north-south traffic through the T1 Gateway.</li>
</ul>

<figure><a href="t1qos.png"><picture>
          <source srcset="/nsx-qos/t1qos_hu14467825683727384270.webp" type="image/webp">
          <source srcset="/nsx-qos/t1qos_hu5352979924153472324.jpg" type="image/jpeg">
          <img src="/nsx-qos/t1qos_hu14467825683727384270.webp"alt="T1 Qos Profile"  width="1433"  height="821" />
        </picture></a><figcaption>
            <p>T1 Qos Profile</p>
          </figcaption></figure>
<p>This setup will help analyze how traffic shaping and rate limiting function at the T1 Gateway level compared to the segment-level QoS.</p>
<h2 id="first-test-iperf-test-from-alpine01-to-alpine02-t1-gateway-qos">First Test: iPerf Test from Alpine01 to Alpine02 (T1 Gateway QoS)</h2>
<h3 id="test-configuration-2"><strong>Test Configuration</strong></h3>
<ul>
<li><strong>Source</strong>: Alpine01 (<code>10.10.10.10</code>) connected to the segment <code>LS-10.10.10.1</code> with no QoS profile applied.</li>
<li><strong>Destination</strong>: Alpine02 (<code>10.10.20.10</code>) connected to the segment <code>LS-10.10.20.1</code>.</li>
<li><strong>QoS Profile</strong>: Applied to the T1 Gateway with:
<ul>
<li><strong>Type</strong>: Ingress</li>
<li><strong>Committed Bandwidth</strong>: 30 Mbps</li>
<li><strong>Burst Size</strong>: Configured according to the calculated constraints.</li>
</ul>
</li>
<li><strong>Tool</strong>: iPerf3</li>
<li><strong>Command</strong>: <code>iperf3 -c 10.10.20.10</code></li>
</ul>
<pre tabindex="0"><code>alpine01:~# iperf3 -c 10.10.20.10 
Connecting to host 10.10.20.10, port 5201
[  5] local 10.10.10.10 port 33292 connected to 10.10.20.10 port 5201
[ ID] Interval           Transfer     Bitrate         Retr  Cwnd
[  5]   0.00-1.00   sec  7.25 MBytes  60.8 Mbits/sec  283   5.66 KBytes       
[  5]   1.00-2.00   sec  3.25 MBytes  27.3 Mbits/sec  369   5.66 KBytes       
[  5]   2.00-3.00   sec  3.00 MBytes  25.2 Mbits/sec  233   52.3 KBytes       
[  5]   3.00-4.00   sec  3.75 MBytes  31.5 Mbits/sec  392   7.07 KBytes       
[  5]   4.00-5.00   sec  3.12 MBytes  26.2 Mbits/sec  284   41.0 KBytes       
[  5]   5.00-6.00   sec  3.88 MBytes  32.5 Mbits/sec  356   8.48 KBytes       
[  5]   6.00-7.00   sec  3.25 MBytes  27.3 Mbits/sec  270   7.07 KBytes       
[  5]   7.00-8.00   sec  3.62 MBytes  30.4 Mbits/sec  315    110 KBytes       
[  5]   8.00-9.00   sec  3.50 MBytes  29.4 Mbits/sec  407   7.07 KBytes       
[  5]   9.00-10.00  sec  3.00 MBytes  25.2 Mbits/sec  245   12.7 KBytes       
- - - - - - - - - - - - - - - - - - - - - - - - -
[ ID] Interval           Transfer     Bitrate         Retr
[  5]   0.00-10.00  sec  37.6 MBytes  31.6 Mbits/sec  3154             sender
[  5]   0.00-10.00  sec  37.1 MBytes  31.1 Mbits/sec                  receiver

iperf Done.
</code></pre><h3 id="result-summary-2">Result Summary</h3>
<ul>
<li>Average Sender Bitrate: 31.6 Mbps</li>
<li>Average Receiver Bitrate: 31.1 Mbps</li>
<li>Total Retransmissions: 3154</li>
</ul>
<h2 id="second-test-iperf-test-from-alpine02-to-alpine03-t1-gateway-qos">Second Test: iPerf Test from Alpine02 to Alpine03 (T1 Gateway QoS)</h2>
<h3 id="test-configuration-3"><strong>Test Configuration</strong></h3>
<ul>
<li><strong>Source</strong>: Alpine02 (<code>10.10.20.10</code>) connected to the segment <code>LS-10.10.20.1</code> with no QoS profile applied.</li>
<li><strong>Destination</strong>: Alpine03 (<code>10.10.10.10</code>) connected to the segment <code>LS-10.10.10.1</code> with no QoS profile applied.</li>
<li><strong>QoS Profile</strong>: Applied to the T1 Gateway in <strong>Ingress</strong> mode, limiting traffic to 30 Mbps for ingress traffic  from T0 to the T1 gateway.</li>
<li><strong>Tool</strong>: iPerf3</li>
<li><strong>Command</strong>: <code>iperf3 -c 10.10.10.10</code></li>
</ul>
<pre tabindex="0"><code>alpine02:~# iperf3 -c 10.10.10.10
Connecting to host 10.10.10.10, port 5201
[  5] local 10.10.20.10 port 50200 connected to 10.10.10.10 port 5201
[ ID] Interval           Transfer     Bitrate         Retr  Cwnd
[  5]   0.00-1.00   sec   249 MBytes  2.09 Gbits/sec    5    615 KBytes       
[  5]   1.00-2.00   sec   267 MBytes  2.24 Gbits/sec    0    885 KBytes       
[  5]   2.00-3.00   sec   159 MBytes  1.34 Gbits/sec   73    290 KBytes       
[  5]   3.00-4.00   sec   248 MBytes  2.08 Gbits/sec    0    677 KBytes       
[  5]   4.00-5.00   sec   261 MBytes  2.19 Gbits/sec    0    928 KBytes       
[  5]   5.00-6.00   sec   264 MBytes  2.22 Gbits/sec    1    792 KBytes       
[  5]   6.00-7.00   sec   260 MBytes  2.18 Gbits/sec   26    441 KBytes       
[  5]   7.00-8.00   sec   257 MBytes  2.16 Gbits/sec    0    766 KBytes       
[  5]   8.00-9.00   sec   262 MBytes  2.20 Gbits/sec    7    585 KBytes       
[  5]   9.00-10.00  sec   262 MBytes  2.19 Gbits/sec    0    858 KBytes       
- - - - - - - - - - - - - - - - - - - - - - - - -
[ ID] Interval           Transfer     Bitrate         Retr
[  5]   0.00-10.00  sec  2.43 GBytes  2.09 Gbits/sec  112             sender
[  5]   0.00-10.00  sec  2.43 GBytes  2.09 Gbits/sec                  receiver
</code></pre><h3 id="result-summary-3">Result Summary</h3>
<ul>
<li>Average Sender Bitrate: 2.09 Gbps</li>
<li>Average Receiver Bitrate: 2.09 Gbps</li>
<li>Total Retransmissions: 112</li>
</ul>
<h2 id="third-test-iperf-test-from-alpine01-to-alpine02-concurrent-traffic">Third Test: iPerf Test from Alpine01 to Alpine02 (Concurrent Traffic)</h2>
<h3 id="test-configuration-4"><strong>Test Configuration</strong></h3>
<ul>
<li><strong>Source</strong>: Alpine01 (<code>10.10.10.10</code>) connected to the segment <code>LS-10.10.10.1</code> with no QoS profile applied.</li>
<li><strong>Destination</strong>: Alpine02 (<code>10.10.20.10</code>) connected to the segment <code>LS-10.10.20.1</code>.</li>
<li><strong>Additional Traffic</strong>: A second VM, connected to the same T1 Gateway as Alpine02, is concurrently receiving data to simulate shared bandwidth conditions.</li>
<li><strong>QoS Profile</strong>: Applied to the T1 Gateway in <strong>Ingress</strong> mode, limiting traffic to 30 Mbps for ingress traffic to the gateway.</li>
<li><strong>Tool</strong>: iPerf3</li>
<li><strong>Command</strong>: <code>iperf3 -c 10.10.20.10</code></li>
</ul>
<pre tabindex="0"><code>alpine01:~# iperf3 -c 10.10.20.10 
Connecting to host 10.10.20.10, port 5201
[  5] local 10.10.10.10 port 40898 connected to 10.10.20.10 port 5201
[ ID] Interval           Transfer     Bitrate         Retr  Cwnd
[  5]   0.00-1.00   sec  2.88 MBytes  24.1 Mbits/sec  475   8.48 KBytes       
[  5]   1.00-2.00   sec  2.50 MBytes  21.0 Mbits/sec  212   53.7 KBytes       
[  5]   2.00-3.00   sec  2.12 MBytes  17.8 Mbits/sec  233   9.90 KBytes       
[  5]   3.00-4.00   sec  2.12 MBytes  17.8 Mbits/sec  145   1.41 KBytes       
[  5]   4.00-5.00   sec  2.50 MBytes  21.0 Mbits/sec  237   14.1 KBytes       
[  5]   5.00-6.00   sec  1.62 MBytes  13.6 Mbits/sec  145   22.6 KBytes       
[  5]   6.00-7.00   sec  3.12 MBytes  26.2 Mbits/sec  331   5.66 KBytes       
[  5]   7.00-8.00   sec  1.00 MBytes  8.39 Mbits/sec  155   7.07 KBytes       
[  5]   8.00-9.00   sec  2.62 MBytes  22.0 Mbits/sec  142   86.3 KBytes       
[  5]   9.00-10.00  sec  3.12 MBytes  26.2 Mbits/sec  349   22.6 KBytes       
- - - - - - - - - - - - - - - - - - - - - - - - -
[ ID] Interval           Transfer     Bitrate         Retr
[  5]   0.00-10.00  sec  23.6 MBytes  19.8 Mbits/sec  2424             sender
[  5]   0.00-10.00  sec  22.4 MBytes  18.8 Mbits/sec                  receiver
</code></pre><h3 id="result-summary-4">Result Summary</h3>
<ul>
<li>Average Sender Bitrate: 19.8 Mbps</li>
<li>Average Receiver Bitrate: 18.8 Mbps</li>
<li>Total Retransmissions: 2424</li>
<li>The QoS profile at the T1 Gateway effectively limits total ingress bandwidth to 30 Mbps. However, the concurrent traffic from the second VM reduces the available bandwidth for Alpine01.</li>
</ul>

    <aside class="admonition attention">
        <div class="admonition-title">
            <div class="icon"><svg xmlns="http://www.w3.org/2000/svg" class="feather feather-link" width="24" height="24" viewBox="0 0 24 24"
      fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
      <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path>
      <path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path>
   </svg></div><b>Note on QoS Profiles and Perspective</b>
        </div>
        <div class="admonition-content"><p>For a QoS profile applied at the gateway level, the <strong>Ingress</strong> direction refers to traffic entering the Tier-1 (T1) Gateway from the Tier-0 (T0) Gateway. This means:</p>
<ul>
<li>From the perspective of a VM, this is indeed <strong>ingress traffic</strong>, as it refers to traffic arriving at the VM via the T1 Gateway.</li>
<li>The <strong>egress traffic</strong> of a VM (traffic leaving the VM) is not affected by an ingress QoS profile applied at the T1 Gateway.
This distinction ensures that the QoS profile at the gateway level is only applied to traffic coming from the T0 to the T1, without influencing outgoing traffic generated by the VM.</li>
</ul></div>
    </aside>
<h2 id="summary">Summary</h2>
<p>When working with QoS profiles in NSX, it is important to understand the key differences and use cases for <strong>Gateway QoS Profiles</strong> and <strong>Segment QoS Profiles</strong>:</p>
<h3 id="gateway-qos-profiles"><strong>Gateway QoS Profiles</strong></h3>
<ul>
<li><strong>Shared Bandwidth</strong>: The configured bandwidth applies to the <strong>total traffic</strong> for all VMs connected to the same T1 Gateway. This includes all segments attached to that gateway.</li>
<li><strong>Practical Use Case</strong>: Gateway QoS profiles are ideal for scenarios like test environments, where you want to limit the total available bandwidth across all VMs and segments.</li>
<li><strong>Traffic Direction</strong>: The QoS direction is critical:
<ul>
<li><strong>Ingress QoS</strong>: Limits traffic entering the T1 Gateway (from T0 to T1), affecting ingress traffic from the VM&rsquo;s perspective.</li>
<li><strong>Egress QoS</strong>: Limits traffic leaving the T1 Gateway (from T1 to T0), affecting egress traffic from the VM&rsquo;s perspective.</li>
</ul>
</li>
</ul>
<h3 id="segment-qos-profiles"><strong>Segment QoS Profiles</strong></h3>
<ul>
<li><strong>Individual Bandwidth Allocation</strong>: Each VM connected to the segment receives the bandwidth specified in the profile. VMs do <strong>not share</strong> the bandwidth; they each receive the assigned limit (assuming the total environment can provide the required bandwidth).</li>
<li><strong>Flexibility</strong>: Segment QoS profiles offer more granular control and can be used for more than just rate limiting. For example, they can prioritize or shape specific types of traffic.</li>
<li><strong>Traffic Direction</strong>: The direction in the profile (Ingress or Egress) must be carefully considered based on what you want to achieve.</li>
</ul>
<h3 id="key-considerations"><strong>Key Considerations</strong></h3>
<ul>
<li><strong>Shared vs. Dedicated Bandwidth</strong>: Use Gateway QoS Profiles when you want to manage total bandwidth collectively for all VMs. Use Segment QoS Profiles when you need to allocate specific bandwidth to individual VMs.</li>
<li><strong>Performance Impact</strong>: Avoid using Gateway QoS Profiles in scenarios requiring high performance and scalability. The active/standby limitation can create bottlenecks, making distributed T1 Gateways the better choice for performance-critical environments.</li>
</ul>
<p>By understanding these differences, you can effectively apply QoS profiles to achieve desired traffic shaping and bandwidth management goals in your NSX environment.</p>
]]></content>
		</item>
		
		<item>
			<title>NSX 4.2.1.1 Hotfix Update</title>
			<link>https://sdn-warrior.org/posts/nsx4_2_1_1/</link>
			<pubDate>Mon, 09 Dec 2024 14:55:00 +0100</pubDate>
			
			<guid>https://sdn-warrior.org/posts/nsx4_2_1_1/</guid>
			<description><![CDATA[short summary of the NSX update 4.2.1.1]]></description>
			<content type="html"><![CDATA[<p>The latest NSX update delivers a comprehensive set of fixes to enhance stability, performance, and security. Here’s a summary of the resolved issues and their impact:</p>
<h2 id="1-enhanced-stability-for-virtual-environments">1. Enhanced Stability for Virtual Environments</h2>
<ul>
<li>Loss of IP Bindings after VMotion (Issue 3453866): Addressed the removal of IP bindings and logical ports associated with VMs during vMotion events.</li>
<li>Critical ESXi Errors with UENS (Issue 3456283): Fixed intermittent PSODs caused by control priority filter lookups, ensuring smoother ESXi operations.</li>
<li>Portgroup Creation Issue (Issue 3458111): Resolved the creation of additional portgroups during full sync, preventing potential vCenter crashes.</li>
<li>Transport Zone Reference Issue (Issue 3454291): Fixed transport zone profile mismatches, restoring vMotion and service functionality.</li>
</ul>
<h2 id="2-improved-network-performance">2. Improved Network Performance</h2>
<ul>
<li>TCP Packet Drops in EDP (Issue 3457047): Resolved issues causing TCP connection drops when using Enhanced Datapath configurations.</li>
<li>Packet Reordering with LRO (Issue 3456533): Fixed packet reordering issues when HW Large Receive Offload is enabled, improving TCP throughput.</li>
<li>Reduced Traffic Performance with UENS and LRO (Issue 3456289): Addressed performance degradation in VSAN workloads.</li>
</ul>
<h2 id="3-robust-security-and-monitoring">3. Robust Security and Monitoring</h2>
<ul>
<li>NSX UI Alarm for Metrics Delivery Failure (Issue 3456663): Fixed authentication issues following certificate changes to restore metrics delivery.</li>
<li>IDPS and TLS Prevention (Issue 3458040): Enhanced malicious traffic prevention by resolving decryption issues with IDPS.</li>
<li>IDPS Events and Certificate Verification (Issue 3458038): Restored the flow of IDPS events to Security Intelligence by fixing Kafka channel errors.</li>
</ul>
<h2 id="4-stability-in-upgrades-and-configurations">4. Stability in Upgrades and Configurations</h2>
<ul>
<li>NSX Manager Slowness (Issue 3453882): Resolved slowness and instability in NSX Manager post-upgrade.</li>
<li>Edge Node IP Table Rules (Issue 3452795): Ensured proper application of IP table rules on Edge nodes.</li>
<li>NSX Configuration Realization (Issue 3452794): Fixed issues preventing configuration realization on Transport Nodes.</li>
</ul>
<h2 id="5-enhancements-in-distributed-firewall-and-flow-management">5. Enhancements in Distributed Firewall and Flow Management</h2>
<ul>
<li>DFW Rules During Upgrade (Issue 3450247): Mitigated periods where DFW rules were disabled during the upgrade process.</li>
<li>Flow Exporter Alarms (Issues 3429787, 3456644): Fixed alarms and restored flow export functionality for Security Intelligence.</li>
</ul>
<h2 id="6-overlay-and-connectivity-improvements">6. Overlay and Connectivity Improvements</h2>
<ul>
<li>Overlay Segment Connectivity (Issue 3450019): Addressed connectivity loss in Overlay Segments when Edge TEP groups were enabled.</li>
</ul>
<h2 id="conclusion">Conclusion</h2>
<p>This NSX update resolves critical issues to improve operational reliability, security, and performance in virtual environments. For a seamless experience, upgrading to this release is highly recommended. As always, thorough testing in a staging environment before deployment in production is advised.</p>
]]></content>
		</item>
		
		<item>
			<title>iSCSI Tuning</title>
			<link>https://sdn-warrior.org/posts/iscsi-tuning/</link>
			<pubDate>Sun, 08 Dec 2024 12:21:20 +0100</pubDate>
			
			<guid>https://sdn-warrior.org/posts/iscsi-tuning/</guid>
			<description><![CDATA[Optimizing iSCSI Performance in an Unraid Environment]]></description>
			<content type="html"><![CDATA[<p>In my setup, I use iSCSI in combination with Unraid to create a DIY block storage solution. Unraid, with its flexibility, serves as the foundation, and I utilize the Linux iSCSI implementation installed via a plugin to enable block-level storage.</p>
<p>For my setup, I use an Intel NUC of the 13th generation, equipped with two 2.5G network adapters. These provide the necessary connectivity for storage traffic. I configured two VMkernel (VMK) adapters specifically for iSCSI traffic, ensuring redundancy and optimized throughput.</p>
<p>To further enhance performance, I’ve implemented several optimizations, including fine-tuning settings on my ESXi servers.</p>
<h2 id="optimize-maxiosizekb">Optimize MaxIoSizeKB</h2>
<p>One such optimization involves adjusting the maximum I/O size for iSCSI traffic.</p>

    <aside class="admonition info">
        <div class="admonition-title">
            <div class="icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-info">
      <circle cx="12" cy="12" r="10"></circle>
      <line x1="12" y1="16" x2="12" y2="12"></line>
      <line x1="12" y1="8" x2="12.01" y2="8"></line>
   </svg></div><b>Info</b>
        </div>
        <div class="admonition-content">By default, VMware ESXi uses a <em><strong>MaxIoSizeKB</strong></em> value of 128 KB.</div>
    </aside>
<p>While this is sufficient for many setups, it may not be optimal for environments
like mine, where jumbo frames are enabled across the network. Larger packets perform better in such a configuration, reducing overhead and increasing throughput.</p>
<p>To take advantage of my network&rsquo;s capabilities, I increased the <em><strong>MaxIoSizeKB</strong></em> parameter to 512 KB</p>
<p>To configure this, I ran the following command on my ESXi host:</p>
<pre tabindex="0"><code>esxcli system settings advanced set -o /ISCSI/MaxIoSizeKB -i 512
</code></pre><p>This change allows the iSCSI initiator to send larger I/O requests, improving data transfer efficiency in my jumbo-frame-enabled network. With this configuration, I noticed a significant improvement in performance, as the network could handle larger blocks of data more effectively.</p>

    <aside class="admonition attention">
        <div class="admonition-title">
            <div class="icon"><svg xmlns="http://www.w3.org/2000/svg" class="feather feather-link" width="24" height="24" viewBox="0 0 24 24"
      fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
      <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path>
      <path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path>
   </svg></div><b>Attention</b>
        </div>
        <div class="admonition-content">For this change to take effect, a host reboot is required. After restarting the ESXi server, the new value will be applied, enabling the iSCSI initiator to send larger I/O requests.</div>
    </aside>
<p>After the reboot, you can verify that the change has been successfully applied by running the following command:</p>
<pre tabindex="0"><code>esxcli system settings advanced list -o /ISCSI/MaxIoSizeKB
</code></pre><p>The output should look like this:</p>
<pre tabindex="0"><code>[root@esxnuc1:~] esxcli system settings advanced list -o /ISCSI/MaxIoSizeKB
   Path: /ISCSI/MaxIoSizeKB
   Type: integer
   Int Value: 512
   Default Int Value: 128
   Min Value: 128
   Max Value: 512
   String Value: 
   Default String Value: 
   Valid Characters: 
   Description: Maximum Software iSCSI I/O size (in KB) (REQUIRES REBOOT!)
   Host Specific: false
   Impact: reboot
</code></pre><h2 id="optimize-multipathing">Optimize multipathing</h2>
<p>To optimize performance, I configured Round Robin as the multipathing policy for my iSCSI volumes on the ESXi server. This ensures better load distribution and failover capabilities. The configuration can be applied via the ESXi CLI as follows:</p>
<ul>
<li>List all connected storage devices to identify the target naa or eui identifier:</li>
</ul>
<pre tabindex="0"><code>esxcli storage nmp device list
</code></pre><ul>
<li>Output</li>
</ul>
<pre tabindex="0"><code>naa.60014058f1117188efe49cb8b5de2273
   Device Display Name: LIO-ORG iSCSI Disk (naa.60014058f1117188efe49cb8b5de2273)
   Storage Array Type: VMW_SATP_ALUA
   Storage Array Type Device Config: {implicit_support=on; explicit_support=on; explicit_allow=on; alua_followover=on; action_OnRetryErrors=on; {TPG_id=0,TPG_state=AO}}
   Path Selection Policy: VMW_PSP_MRU
   Path Selection Policy Device Config: {policy=iops,iops=1000,bytes=10485760,useANO=0; lastPathIndex=0: NumIOsPending=0,numBytesPending=0}
   Path Selection Policy Device Custom Config: policy=iops;iops=1000;bytes=10485760;samplingCycles=16;latencyEvalTime=180000;useANO=0;
   Working Paths: vmhba64:C1:T0:L1, vmhba64:C0:T0:L1
   Is USB: false
</code></pre>
    <aside class="admonition info">
        <div class="admonition-title">
            <div class="icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-info">
      <circle cx="12" cy="12" r="10"></circle>
      <line x1="12" y1="16" x2="12" y2="12"></line>
      <line x1="12" y1="8" x2="12.01" y2="8"></line>
   </svg></div><b>Info</b>
        </div>
        <div class="admonition-content"><!-- raw HTML omitted --> The default multipath policy is &ldquo;Most Recently Used&rdquo; (VM_PSP_MRU).</div>
    </aside>
<ul>
<li>Set the multipathing policy for the desired iSCSI device to RoundRobin:</li>
</ul>
<pre tabindex="0"><code>esxcli storage nmp device set --device &lt;DeviceIdentifier&gt; --psp VMW_PSP_RR
</code></pre><ul>
<li>Verify that the policy has been successfully applied:</li>
</ul>
<pre tabindex="0"><code>esxcli storage nmp device list | grep &lt;DeviceIdentifier&gt;
</code></pre>
    <aside class="admonition info">
        <div class="admonition-title">
            <div class="icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-info">
      <circle cx="12" cy="12" r="10"></circle>
      <line x1="12" y1="16" x2="12" y2="12"></line>
      <line x1="12" y1="8" x2="12.01" y2="8"></line>
   </svg></div><b>Info</b>
        </div>
        <div class="admonition-content">Replace <!-- raw HTML omitted --> with the actual identifier of your iSCSI device (naa.xxxxxxxxxxxxxxxxxxxxxxxxxxxxxx).&quot;</div>
    </aside>
<ul>
<li>Output after chages</li>
</ul>
<pre tabindex="0"><code>naa.60014058f1117188efe49cb8b5de2273
   Device Display Name: LIO-ORG iSCSI Disk (naa.60014058f1117188efe49cb8b5de2273)
   Storage Array Type: VMW_SATP_ALUA
   Storage Array Type Device Config: {implicit_support=on; explicit_support=on; explicit_allow=on; alua_followover=on; action_OnRetryErrors=on; {TPG_id=0,TPG_state=AO}}
   Path Selection Policy: VMW_PSP_RR
   Path Selection Policy Device Config: {policy=iops,iops=1000,bytes=10485760,useANO=0; lastPathIndex=0: NumIOsPending=0,numBytesPending=0}
   Path Selection Policy Device Custom Config: policy=iops;iops=1000;bytes=10485760;samplingCycles=16;latencyEvalTime=180000;useANO=0;
   Working Paths: vmhba64:C1:T0:L1, vmhba64:C0:T0:L1
   Is USB: false
</code></pre>
    <aside class="admonition info">
        <div class="admonition-title">
            <div class="icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-info">
      <circle cx="12" cy="12" r="10"></circle>
      <line x1="12" y1="16" x2="12" y2="12"></line>
      <line x1="12" y1="8" x2="12.01" y2="8"></line>
   </svg></div><b>Info</b>
        </div>
        <div class="admonition-content">the multipath policy must now be VMW_PSP_RR</div>
    </aside>
<p>To further optimize path usage and load distribution, I adjusted the IOPS parameter for the Round Robin policy. By default, ESXi switches storage paths after 1000 I/O operations, but I used the following command snippet to change this behavior to switch after every single I/O:</p>
<pre tabindex="0"><code>for i in `esxcfg-scsidevs -c |awk &#39;{print $1}&#39; | grep naa.xxxx`; do 
   esxcli storage nmp psp roundrobin deviceconfig set --type=iops --iops=1 --device=$i
done
</code></pre><p>Where .xxxx matches the first few characters of your NAA IDs. Reducing the IOPS value from 1000 to 1 means the ESXi host will alternate between available paths much more frequently. In practice, this can help evenly distribute the workload across all paths, potentially improving overall responsiveness and performance.</p>
<p>However, when combined with changes like increasing MaxIoSizeKB, the outcome can vary. In some cases, this adjustment may yield better results, while in others it could degrade performance. Therefore, it’s crucial to test these parameters individually for each storage environment to determine the most effective configuration.</p>
<h2 id="why-jumbo-frames-matter-bonus">Why Jumbo Frames Matter (bonus)</h2>
<p>Jumbo frames allow Ethernet frames larger than the standard 1500 bytes to be transmitted, reducing the total number of frames required to send the same amount of data. This results in lower CPU overhead and better performance, particularly in high-bandwidth and storage-intensive environments. However, it’s essential to ensure that every device in the network path—NICs, switches, and storage systems—supports and is configured for jumbo frames for optimal performance.</p>
<p>To verify that jumbo frames are functioning correctly in your environment, you can use the <em><strong>vmkping</strong></em> command on your ESXi host:</p>
<pre tabindex="0"><code>vmkping -I vmk1 -s 8973 -d 192.168.67.250
</code></pre>
    <aside class="admonition info">
        <div class="admonition-title">
            <div class="icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-info">
      <circle cx="12" cy="12" r="10"></circle>
      <line x1="12" y1="16" x2="12" y2="12"></line>
      <line x1="12" y1="8" x2="12.01" y2="8"></line>
   </svg></div><b> vmkping parameters</b>
        </div>
        <div class="admonition-content"><ul>
<li>Replace <em><strong>vmk1</strong></em> with the VMkernel adapter used for iSCSI.</li>
<li><em><strong>-s 8972</strong></em> sets the packet size to match the jumbo frame size (8972 bytes, including headers).</li>
<li><em><strong>-d</strong></em> enables the Don&rsquo;t Fragment flag to ensure the packet isn&rsquo;t fragmented along the way.</li>
</ul>
</div>
    </aside>
<p>If you see the error:  <em><strong>sendto() failed (Message too long)</strong></em> this indicates that the packet size is too large to be transmitted without fragmentation. For a setup with an MTU of 9000 configured on the distributed or standard switch, a packet size of 8972 bytes should work correctly. If the error occurs, check your vswitch settings, your physical switches and your iSCSI target.</p>
<h2 id="conclusion">Conclusion</h2>
<p>By increasing the MaxIoSizeKB to 512 KB, verifying jumbo frame functionality with vmkping, and enabling Round Robin, I optimized my iSCSI setup to leverage the full potential of my 2.5G network.</p>

<figure><picture>
          <source srcset="/iscsi-tuning/test_hu12020423995241410634.webp" type="image/webp">
          <source srcset="/iscsi-tuning/test_hu16349894886800222160.jpg" type="image/jpeg">
          <img src="/iscsi-tuning/test_hu12020423995241410634.webp"alt="CriytalDiskMark Benchmark"  width="479"  height="351" />
        </picture><figcaption>
            <p>CriytalDiskMark Benchmark</p>
          </figcaption></figure>
<p>In my tests with CrystalDiskMark, I observed that both network adapters showed significant performance improvements as a result of these optimizations. These adjustments allow my Unraid and iSCSI configuration to deliver a robust and high-performance block storage solution tailored to my workloads.</p>
<h2 id="disclaimer">Disclaimer</h2>
<p>The settings and configurations described in this article are specific to my environment and were tested extensively within my setup. While these adjustments significantly improved performance for my use case, they may not be universally applicable. It’s essential to test these settings in your environment before implementing them, as results may vary depending on hardware, network, and workload specifics. These optimizations are not intended as a blanket recommendation.</p>
]]></content>
		</item>
		
		<item>
			<title>MAC Learning is your friend</title>
			<link>https://sdn-warrior.org/posts/mac-learning/</link>
			<pubDate>Wed, 27 Nov 2024 19:54:18 +0100</pubDate>
			
			<guid>https://sdn-warrior.org/posts/mac-learning/</guid>
			<description><![CDATA[Why you should use MAC Learning]]></description>
			<content type="html"><![CDATA[<h2 id="intro">Intro</h2>
<p>When working with nested ESXi environments, understanding the interplay between MAC Learning, Promiscuous Mode, and Forged Transmits is critical. These settings significantly affect how traffic flows in virtualized networks, especially in scenarios involving virtualized hypervisors or advanced network configurations.</p>
<ul>
<li>
<p>MAC Learning: Think of it as a switch-like behavior in your virtual environment. It optimizes network traffic by ensuring that each virtual machine (VM) receives only the packets meant for its MAC address.
Without MAC learning, when the ESXi VM&rsquo;s vNIC connects to a switch port, it only contains a static MAC address.
<a href="https://docs.vmware.com/en/VMware-vSphere/8.0/vsphere-networking/GUID-E0246B3D-9FB1-4976-8217-5C085863EA9A.html">(more Information about MAC learning)</a></p>
</li>
<li>
<p>Promiscuous Mode: On the other hand, this allows a VM or virtual switch to capture all traffic on a port group, whether addressed to it or not. It&rsquo;s a useful feature for troubleshooting and monitoring but comes with potential security and performance implications.</p>
</li>
<li>
<p>Forged Transmits: Forged Transmits plays a complementary role in this configuration. It ensures that traffic originating from a VM with a source MAC address different from its assigned MAC address is allowed to leave the virtual switch. This is crucial in nested environments.</p>
</li>
</ul>
<h2 id="lab-environment">Lab environment</h2>
<p>In this lab environment, I am using two Minisforum MS-01 workstations, each equipped with ESXi 8.0.3 as the hypervisor. These compact systems provide a balance of performance and energy efficiency, fitting perfectly into my goal of maintaining a powerful yet quiet setup.</p>
<p>Each workstation is interconnected via dual 10 Gb/s network links, ensuring high-speed communication with minimal latency. This setup is particularly advantageous for simulating complex network scenarios and nested virtualization environments.</p>
<p>On each workstation, a nested ESXi host is deployed. These nested hosts act as virtualized hypervisors for a future VCF deployment.</p>
<h2 id="the-problems-ive-caused">The problems I&rsquo;ve caused</h2>
<p>In my previous lab setups, Promiscuous Mode was my go-to solution for nested virtualization. It was reliable, simple to configure, and worked flawlessly for years. While I was aware of the security risks associated with it, in a controlled homelab environment, those risks were not a significant concern.</p>
<p>However, everything changed when I upgraded my lab to dual 10 Gb/s network links and, powered by the i9 CPU, gained the ability to run multiple nested ESXi hosts on a single physical machine.
One of the first challenges I encountered was during the configuration of a vSAN port group for my nested ESXi hosts. This port group was configured to use Active/Active load balancing across both 10 Gb/s uplinks on the MS-01 workstations.
Almost immediately, I noticed unexpected performance issues. Nested VMs were experiencing slow network speeds, and vSAN operations were significantly hindered. Initially, I struggled to pinpoint the root cause. Given my past success with Promiscuous Mode, I didn’t suspect it could be contributing to the problem.</p>
<p><a href="https://cybernils.net/2024/11/26/the-effect-of-using-mac-learning-in-esxi-nested-labs/">This article</a> by my fellow vExpert colleague Nils Kristiansen inspired me to delve deeper into the topic.</p>
<h2 id="why-promiscuous-mode-became-a-problem">Why Promiscuous Mode Became a Problem</h2>
<p>The performance degradation stemmed from how traffic was handled with Promiscuous Mode in a dual-uplink, Active/Active configuration:</p>
<ul>
<li>
<p>Broadcasting Traffic Across Both Uplinks: Promiscuous Mode caused the virtual switch to deliver all traffic to every uplink, regardless of the destination. With two high-speed uplinks in an Active/Active configuration, this created excessive overhead, saturating the uplinks and causing packet drops.</p>
</li>
<li>
<p>vSAN’s High Sensitivity to Latency: vSAN traffic is highly dependent on low latency and consistent performance. The unnecessary broadcast of packets interfered with its ability to operate efficiently.</p>
</li>
<li>
<p>Nested Virtualization Amplified the Problem: Nested ESXi hosts added another layer of complexity. The inner VMs were sending and receiving traffic that the parent ESXi host’s virtual switch struggled to handle efficiently under Promiscuous Mode.</p>
</li>
</ul>
<h2 id="ok-but-how-bad-is-the-performance">OK, but how bad is the performance?</h2>
<p>To quantify the performance issues, I turned to iPerf3, a reliable tool for measuring network throughput that is conveniently included in ESXi 8. Using iPerf3, I conducted a series of tests to better understand the extent of the performance degradation.</p>
<h3 id="performance-measurement-1-both-physical-nics-active-nested-hosts-on-the-same-physical-host">Performance Measurement 1: Both Physical NICs Active, Nested Hosts on the Same Physical Host</h3>
<p>For the first test, I configured both pNICs (10 Gb/s) as active in an Active/Active load balancing setup and placed both nested ESXi hosts on the same physical host. Additionally, Promiscuous Mode was enabled on the port group to ensure traffic could flow properly between the nested hosts.</p>
<p><img src="test1.png" alt="Test 1"></p>
<h3 id="results">Results</h3>
<pre tabindex="0"><code>[ ID] Interval           Transfer     Bitrate         Retr
[  5]   0.00-10.00  sec  1.35 GBytes  1.16 Gbits/sec    0            sender  
[  5]   0.00-10.00  sec  1.35 GBytes  1.16 Gbits/sec                 receiver 
</code></pre><h3 id="performance-measurement-2-single-nic-active-nested-hosts-on-the-same-physical-host">Performance Measurement 2: Single NIC Active, Nested Hosts on the Same Physical Host</h3>
<p>For the second test, I modified the setup to use only one physical NIC (pNIC) while keeping both nested ESXi hosts on the same physical host. Promiscuous Mode was still enabled on the port group to ensure traffic routing between the nested hosts. By disabling the second uplink, the traffic path was simplified, reducing potential conflicts.</p>
<p><img src="test2.png" alt="Test 2"></p>
<h3 id="results-1">Results</h3>
<pre tabindex="0"><code>[ ID] Interval           Transfer     Bitrate         Retr
[  5]   0.00-10.00  sec  11.4 GBytes  9.82 Gbits/sec    0            sender
[  5]   0.00-10.01  sec  11.4 GBytes  9.80 Gbits/sec                 receiver
</code></pre><h3 id="performance-measurement-3-mac-learning-and-forged-transmits-dual-uplinks-nested-hosts-on-the-same-physical-host">Performance Measurement 3: MAC Learning and Forged Transmits, Dual Uplinks, Nested Hosts on the Same Physical Host</h3>
<p>For the third test, I switched to using MAC Learning and Forged Transmits, while keeping both physical NICs (pNICs) active in the Active/Active load balancing configuration. Both nested ESXi hosts were still located on the same physical host. This configuration was designed to optimize traffic handling without relying on Promiscuous Mode</p>
<p><img src="test3.png" alt="Test 3"></p>
<h3 id="results-2">Results</h3>
<pre tabindex="0"><code>[ ID] Interval           Transfer     Bitrate         Retr  
[  5]   0.00-10.00  sec  24.2 GBytes  20.8 Gbits/sec    0            sender  
[  5]   0.00-10.01  sec  24.2 GBytes  20.8 Gbits/sec                 receiver  
</code></pre><h3 id="performance-measurement-4-mac-learning-and-forged-transmits-single-uplink-nested-hosts-on-the-same-physical-host">Performance Measurement 4: MAC Learning and Forged Transmits, Single Uplink, Nested Hosts on the Same Physical Host</h3>
<p>For the fourth test, I used a single uplink (pNIC) with both nested ESXi hosts on the same ESXi server. MAC Learning and Forged Transmits were enabled to optimize traffic handling.
The throughput was 20.7 Gbits/sec, almost identical to Test 3. This confirms that, since the traffic did not need to traverse the physical network infrastructure, the single uplink configuration with MAC Learning and Forged Transmits performed just as efficiently, without the overhead of Promiscuous Mode.</p>
<p><img src="test4.png" alt="Test 4"></p>
<h3 id="results-3">Results</h3>
<pre tabindex="0"><code>[ ID] Interval           Transfer     Bitrate         Retr  
[  5]   0.00-10.00  sec  24.2 GBytes  20.8 Gbits/sec    0            sender  
[  5]   0.00-10.01  sec  24.2 GBytes  20.8 Gbits/sec                 receiver  
</code></pre><h3 id="further-performance-measurements-and-security-considerations">Further Performance Measurements and Security Considerations</h3>
<p>Additional performance tests revealed that the difference between Promiscuous Mode and MAC Learning was minimal or even non-existent when the nested hosts were placed on two different physical hosts.
The traffic between the nested VMs did not significantly differ whether Promiscuous Mode or MAC Learning was enabled, indicating that both configurations performed similarly in a multi-host environment.</p>

    <aside class="admonition warning">
        <div class="admonition-title">
            <div class="icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-alert-circle">
      <circle cx="12" cy="12" r="10"></circle>
      <line x1="12" y1="8" x2="12" y2="12"></line>
      <line x1="12" y1="16" x2="12.01" y2="16"></line>
   </svg></div><b>It&#39;s important to note the security implications of using Promiscuous Mode</b>
        </div>
        <div class="admonition-content">Enabling Promiscuous Mode on a network interface allows all traffic to be sent to the VM, even traffic not intended for it, which can expose sensitive data or potentially allow malicious activity.
Because of this security concern, Promiscuous Mode should only be used temporarily, and for troubleshooting purposes, in production environments.
It is recommended to disable it once the issue is resolved to maintain a secure network setup.</div>
    </aside>
<h3 id="side-effect-of-promiscuous-mode-duplicate-packets">Side Effect of Promiscuous Mode: Duplicate Packets</h3>
<p>Enabling Promiscuous Mode on a network interface can lead to duplicate packets when both the source and destination are on the same ESXi host. In this mode, the virtual machine receives all traffic, including its own outbound packets, causing unnecessary duplication. This can result in performance degradation due to increased CPU usage and network inefficiencies.</p>
<pre tabindex="0"><code>[root@esxnuc04:/usr/lib/vmware/vsan/bin] vmkping -I vmk1 192.168.69.203
PING 192.168.69.203 (192.168.69.203): 56 data bytes
64 bytes from 192.168.69.203: icmp_seq=0 ttl=64 time=0.356 ms
64 bytes from 192.168.69.203: icmp_seq=0 ttl=64 time=0.423 ms (DUP!)
64 bytes from 192.168.69.203: icmp_seq=0 ttl=64 time=0.426 ms (DUP!)
64 bytes from 192.168.69.203: icmp_seq=0 ttl=64 time=0.429 ms (DUP!)
64 bytes from 192.168.69.203: icmp_seq=1 ttl=64 time=0.249 ms
64 bytes from 192.168.69.203: icmp_seq=1 ttl=64 time=0.274 ms (DUP!)
64 bytes from 192.168.69.203: icmp_seq=1 ttl=64 time=0.277 ms (DUP!)
64 bytes from 192.168.69.203: icmp_seq=1 ttl=64 time=0.281 ms (DUP!)
64 bytes from 192.168.69.203: icmp_seq=2 ttl=64 time=0.261 ms
</code></pre><h2 id="sidequest-using-iperf-on-esxi-803">Sidequest: Using iPerf on ESXi 8.0.3</h2>
<p>To use iPerf for network performance testing on ESXi 8.0.3, you&rsquo;ll need to follow a few steps to enable and configure the necessary settings.</p>
<ul>
<li>Step 1: Disable the ESXi firewall temporarily
First, disable the ESXi firewall to allow the iPerf tool to operate without restrictions:</li>
</ul>
<pre tabindex="0"><code>esxcli network firewall set --enabled false
</code></pre><ul>
<li>Step 2: Allow executing iPerf
Next, set the system to allow execution of non-installed binaries (such as iPerf), which are not part of the default ESXi installation:</li>
</ul>
<pre tabindex="0"><code>localcli system settings advanced set -o /User/execInstalledOnly -i 0
</code></pre><ul>
<li>Step 3: Execute iPerf (Client example)
Once you&rsquo;ve set the necessary configuration, you can execute iPerf to test the network performance. Use the following command to run iPerf as a client (-c) and specify the target IP address (e.g., 192.168.69.203):</li>
</ul>
<pre tabindex="0"><code>./usr/lib/vmware/vsan/bin/iperf3 -c 192.168.69.203
</code></pre><ul>
<li>Step 4: Re-enable the firewall
Once you’ve finished testing, remember to re-enable the firewall for security reasons:</li>
</ul>
<pre tabindex="0"><code>esxcli network firewall set --enabled true
</code></pre><ul>
<li>Step 5: Restrict execution of non-installed binaries
To revert the system to its default behavior and restrict the execution of non-installed binaries, run the following command:</li>
</ul>
<pre tabindex="0"><code>localcli system settings advanced set -o /User/execInstalledOnly -i 1
</code></pre><h2 id="why-mac-learning-and-forged-transmits-replace-promiscuous-mode-in-nested-environments">Why MAC Learning and Forged Transmits Replace Promiscuous Mode in Nested Environments</h2>
<p>In a typical nested ESXi environment, each inner VM sends packets with its unique MAC address, which the virtual switch on the parent ESXi host does not recognize by default. This creates a challenge because:</p>
<ul>
<li>Without Promiscuous Mode, the switch drops packets destined for or originating from these MAC addresses.</li>
<li>Without Forged Transmits, packets from inner VMs with &ldquo;forged&rdquo; source MAC addresses are also dropped.</li>
</ul>
<h3 id="by-enabling-mac-learning-and-forged-transmits">By enabling MAC Learning and Forged Transmits:</h3>
<p><em><strong>MAC Learning</strong></em> ensures that the virtual switch learns the inner VMs’ MAC addresses dynamically, so it can correctly forward traffic to them without requiring Promiscuous Mode.
<em><strong>Forged Transmits</strong></em> ensures that traffic from inner VMs with different source MAC addresses is allowed to leave the parent VM&rsquo;s vNIC.</p>

    <aside class="admonition info">
        <div class="admonition-title">
            <div class="icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-info">
      <circle cx="12" cy="12" r="10"></circle>
      <line x1="12" y1="16" x2="12" y2="12"></line>
      <line x1="12" y1="8" x2="12.01" y2="8"></line>
   </svg></div><b>Conclusion</b>
        </div>
        <div class="admonition-content">The combination of MAC Learning and Forged Transits removes the need for promiscuous mode, while maintaining:
Better performance, as traffic is only sent where needed.
Stronger security, since traffic is not broadcast unnecessarily.</div>
    </aside>
<p>MAC Learning with Forged Transmits is a significant performance gamechanger, especially when running multiple nested VMs on a single physical ESXi host.
However, it&rsquo;s important to note that MAC Learning with Forged Transmits requires a Distributed Switch. If you&rsquo;re using a Standard Switch, you&rsquo;ll still need to rely on Promiscuous Mode to achieve similar functionality.</p>
]]></content>
		</item>
		
		<item>
			<title>Unraid - A Storage Journey</title>
			<link>https://sdn-warrior.org/posts/unraid-storage/</link>
			<pubDate>Tue, 19 Nov 2024 23:00:00 +0100</pubDate>
			
			<guid>https://sdn-warrior.org/posts/unraid-storage/</guid>
			<description><![CDATA[How i use Unraid]]></description>
			<content type="html"><![CDATA[<h2 id="my-custom-unraid-storage-build---flexibility-simplicity-and-future-proofing">My Custom Unraid Storage Build - Flexibility, Simplicity, and Future-Proofing</h2>
<p>As a passionate homelaber, I enjoy not only using technology but also understanding and customizing it to suit my needs. My Unraid storage system is one of my longest-running projects, continuously evolving to meet the demands of my homelab.</p>
<p>After thorough research, I decided to go with Unraid – an operating system renowned for its simplicity, flexibility, and scalability. These key features were the deciding factors for me:</p>
<ul>
<li>Easy Expansion: Unraid allows combining drives of different sizes and expanding the array later without replacing all disks at once.</li>
<li>Docker Integration: The ability to run Docker containers directly on Unraid unlocks immense potential for personal projects and applications.</li>
<li>Versatility: Whether it’s managing data, running a media server, or hosting virtual machines, Unraid offers the freedom to adapt the system to your needs.</li>
</ul>
<p>In this blog post, I’ll share my experience and guide you through how I’ve planned, built, and continuously improved my Unraid storage system. It’s a perfect solution for anyone seeking a scalable, cost-effective setup without sacrificing performance or ease of use.</p>
<h2 id="the-beginning-my-first-steps-with-unraid">The Beginning: My First Steps with Unraid</h2>
<p>Unraid is a commercial product that initially caught my attention due to its unique approach to storage management. Historically, Unraid licenses were available for a one-time purchase, providing lifetime access to its features. Today, however, users can choose between a subscription model or a lifetime license, offering flexibility depending on individual needs.</p>
<p>One of the standout features of Unraid is that it boots directly from a USB stick. This design choice not only simplifies installation but also makes it incredibly easy to replace hardware. Simply move the USB stick to a new machine, and the system is ready to run without the need for extensive reconfiguration.</p>
<p>My first Unraid “server” was far from conventional: a Lenovo notebook powered by an old Intel Dual-Core processor. To build my initial array, I used external USB disks – a true makeshift setup but perfect for testing the waters. For three weeks, this setup served as my proof of concept (POC), allowing me to explore Unraid’s capabilities and ensure it met my needs before committing to more suitable hardware.</p>
<p>This early experimentation confirmed that Unraid was the right choice for my homelab, and I soon began planning and acquiring the components for my first proper build.</p>
<h2 id="building-a-3-tier-performance-storage-system">Building a 3-Tier Performance Storage System</h2>
<p>As my Unraid setup evolved, I implemented a 3-tier performance storage system to meet the varying demands of my homelab. Each tier is tailored for a specific purpose, optimizing the balance between speed, capacity, and efficiency:</p>
<ul>
<li>Tier 1: The Unraid Array (Slow Storage)
At the foundation of my storage system is the Unraid Array, which serves as the slowest but most capacious tier. Unlike traditional RAID, an Unraid Array does not stripe data across disks. Instead, each disk holds individual files, while parity disks provide fault tolerance for data recovery. This unique design allows mixing drives of different sizes, making upgrades straightforward and cost-effective.
My Unraid Array is hosted in an external USB 3.2 storage shelf, which presents each drive individually to Unraid. The shelf delivers enough bandwidth to operate all six 6TB enterprise SATA drives at full speed, ensuring reliable performance even during intensive data access.</li>
</ul>

    <aside class="admonition warning">
        <div class="admonition-title">
            <div class="icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-alert-circle">
      <circle cx="12" cy="12" r="10"></circle>
      <line x1="12" y1="8" x2="12" y2="12"></line>
      <line x1="12" y1="16" x2="12.01" y2="16"></line>
   </svg></div><b>A Quick Warning About Using Unraid with USB Disk Shelves</b>
        </div>
        <div class="admonition-content"><p>It’s important to note that Unraid does not officially recommend running the array on a USB disk shelf, as USB connections can sometimes lead to instability or performance issues. However, my personal experience has shown that it can work reliably with the right hardware.</p>
<p>In my setup, I use a TerraMaster D6-320 USB 3.2 disk shelf, paired with a high-quality USB controller like those found in devices such as Intel NUCs. This combination has proven stable and capable of delivering full-speed performance for all six enterprise SATA drives in my array.
While this setup works well for me, I recommend testing thoroughly in your environment to ensure stability and compatibility before committing to a similar configuration.</p>
</div>
    </aside>
<ul>
<li>
<p>Tier 2: Consumer NVMe Drives (Fast Cache and Docker Storage)
The second performance tier consists of consumer-grade NVMe drives, configured in a btrfs pool within Unraid. This configuration not only allows advanced features like snapshots but also supports RAID levels within the pool, providing a balance between performance and redundancy.
This tier is designed to handle tasks requiring high-speed I/O, such as hosting Docker containers. Keeping Docker data on the NVMe tier ensures that the Unraid Array doesn’t need to spin up unnecessarily, prolonging the life of the disks and improving system responsiveness.
The NVMe drives also serve as a fast cache for incoming data. Files uploaded to the NAS during the day are stored on the NVMe tier and then moved to the slower Unraid Array overnight—except for Docker data, which always remains on the NVMe storage to maintain optimal performance.
Unraid’s flexibility makes it easy to decide whether specific shares or data should stay on the NVMe pool or be automatically moved to the Array on a scheduled basis. This level of control ensures you can optimize storage placement to suit your workload, balancing speed and capacity seamlessly.</p>
</li>
<li>
<p>Tier 3: Enterprise NVMe via iSCSI (Fast and Durable Storage)
The top tier features a 4TB enterprise NVMe drive, designed for high-speed and durable performance under constant load. This storage tier is shared with my homelab servers via iSCSI Multichannel, utilizing a 2x10Gb Intel X710 NIC for redundancy and maximum throughput.
This tier provides fast, reliable storage for workloads that demand consistent performance, such as virtual machines or other critical applications in my homelab. By leveraging enterprise-grade hardware and robust networking, this storage layer ensures low-latency access and can handle the demands of continuous use without compromising reliability.</p>
</li>
</ul>
<h2 id="current-setup">Current Setup</h2>
<p>My Unraid server is built on a Intel NUC Extreme 11th Gen i7 with 64GB of RAM, offering a powerful and compact platform for my homelab. The storage setup includes 2x 1TB consumer-grade NVMe drives for fast cache and 4TB enterprise-grade NVMe for ultra-reliable, high-performance storage.</p>
<p>The Unraid Array has a total capacity of 42TB, with 33.4TB usable for data storage. This provides ample space for both my active projects and long-term storage needs.</p>
<p>On the software side, I host 29 Docker container services and 4 virtual machines, including critical services such as my Active Directory (AD), Certificate Authority (CA), and a Veeam Proxy for file backups. This setup allows for a highly efficient and flexible environment that supports a wide range of use cases while maintaining reliable performance.</p>
<p><img src="unraid2.jpg" alt="Gui"></p>
<h2 id="performance">Performance</h2>
<p>The performance of my Unraid setup was measured using CrystalDiskMark with a 16GB test file (on a Windows 11 VM) to evaluate both sequential and random read and write speeds, as well as IOPS (Input/Output Operations Per Second) of my iSCSI Storage (Tier 3). The results highlight the impressive capabilities of the system:</p>
<p>Read Performance:</p>
<ul>
<li>Sequential Read (Q8T1): 1.993 GB/s | IOPS: 1900.35</li>
<li>Sequential Read (Q1T1): 0.782 GB/s | IOPS: 746.21</li>
<li>Random Read 4K (Q32T1): 0.322 GB/s | IOPS: 78,651.61</li>
<li>Random Read 4K (Q1T1): 0.021 GB/s | IOPS: 5,149.17</li>
</ul>
<p>Write Performance:</p>
<ul>
<li>Sequential Write (Q8T1): 1.247 GB/s | IOPS: 1,189.37</li>
<li>Sequential Write (Q1T1): 0.802 GB/s | IOPS: 764.48</li>
<li>Random Write 4K (Q32T1): 0.298 GB/s | IOPS: 72,776.61</li>
<li>Random Write 4K (Q1T1): 0.036 GB/s | IOPS: 8,835.45</li>
</ul>
<p>These performance metrics demonstrate both the high throughput and responsiveness of the NVMe storage.
The sequential read and write speeds are excellent for large file transfers, while the random IOPS (especially at Q32T1) indicate the drive’s ability to handle a high volume of small, random data requests.
Despite the lower random read/write speeds at Q1T1, the system still shows strong overall performance for a variety of tasks.</p>
<h2 id="understanding-the-crystaldiskmark-test-parameters">Understanding the CrystalDiskMark Test Parameters</h2>
<p>In CrystalDiskMark, several key parameters define how the storage device is tested. Here’s a breakdown of what each test represents:</p>
<p>Q8T1: This stands for Queue Depth 8, Thread 1. It simulates a scenario where 8 data requests are queued up, but only 1 thread (or process) is handling those requests. This type of test is useful for measuring the performance of the storage device when handling multiple sequential tasks at once, but not with excessive parallelism.</p>
<p>Q1T1: This stands for Queue Depth 1, Thread 1. Here, only 1 data request is in the queue, and a single thread handles it. This test represents the performance when a single request is being processed at a time, simulating typical user scenarios where one operation is occurring without significant multitasking.</p>
<p>Q32T1: This stands for Queue Depth 32, Thread 1. In this case, there are 32 queued data requests with a single thread handling them. This test simulates heavy workloads where many data requests are pending, but only one thread is processing them. It can show how the device handles stress under larger, more sustained read operations.</p>
<h3 id="sequential-vs-random-read-tests">Sequential vs. Random Read Tests</h3>
<p>Sequential Read: This test measures how fast the storage device can read large, contiguous chunks of data, like streaming a video or transferring large files. It simulates real-world scenarios where large files need to be read from the storage at a steady rate.</p>
<p>Sequential Read (Q8T1): 1.993 GB/s – This high performance indicates the drive can handle multiple large file read operations quickly, with 8 data requests queued up.
Sequential Read (Q1T1): 0.782 GB/s – This is slower than the Q8T1 test because only one request is processed at a time, simulating less intensive operations.</p>
<p>Random Read: This test measures the performance when the drive has to read small, non-contiguous chunks of data from different parts of the storage. This type of test is more representative of workloads like database operations or running small applications that frequently access different data blocks.</p>
<p>Random Read 4K (Q32T1): 0.322 GB/s – With 32 queued requests and 4KB blocks, this performance shows how the system handles multiple random reads.
Random Read 4K (Q1T1): 0.021 GB/s – Here, only one small request is being handled at a time, leading to slower performance because random 4K reads are typically slower due to the overhead of accessing many different locations on the disk.</p>
<p>These tests give a complete picture of the drive&rsquo;s performance under different scenarios: from high-speed, sequential reads (large files) to more intensive, random access (small files), and with varying levels of workload concurrency.</p>
<h2 id="bom-bill-of-materials">BOM (Bill of Materials)</h2>
<ul>
<li>NUC11DBBi7 , Version M17027-404</li>
<li>2x 32 GB RAM Kingston KF3200C20S4 SODIMM DDR4 Synchronous 3200 MHz</li>
<li>TerraMaster D6-320 USB 3.2(Gen2)</li>
<li>3x TOSHIBA_MG09ACA18TE 18 TB Enterprise SATA</li>
<li>1x TOSHIBA_MG08ADA600E 6 TB Enterpise SATA (to change)</li>
<li>2x Samsung 970 EVO Plus 1TB</li>
<li>1x Samsung MZ1L23T8HBLA-00A07 4 TB Enterprise NVMe 110mm</li>
<li>1x Intel X710 2x 10 Gb/s</li>
<li>1x Good USB Stick (32GB)</li>
</ul>
<p><img src="das.jpg" alt="DAS Disk Array">
<img src="unraid.jpg" alt="Unraid"></p>
<h3 id="fun-fact-my-unraid-server-has-underglow-lighting">Fun fact: My Unraid server has underglow lighting!</h3>
]]></content>
		</item>
		
		<item>
			<title>How to get most out of your Nuc </title>
			<link>https://sdn-warrior.org/posts/nuc/</link>
			<pubDate>Sun, 17 Nov 2024 11:57:43 +0100</pubDate>
			
			<guid>https://sdn-warrior.org/posts/nuc/</guid>
			<description><![CDATA[Performance tuning for NUCs]]></description>
			<content type="html"><![CDATA[<h2 id="first-things-first">First things first</h2>
<p>Get a second NIC. The Intel NUC Pro has an IO expansion and supports an additional NIC.
Unfortunately, these are relatively difficult to get in Germany, but it&rsquo;s worth the effort.

    <aside class="admonition info">
        <div class="admonition-title">
            <div class="icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-info">
      <circle cx="12" cy="12" r="10"></circle>
      <line x1="12" y1="16" x2="12" y2="12"></line>
      <line x1="12" y1="8" x2="12.01" y2="8"></line>
   </svg></div><b>Search for</b>
        </div>
        <div class="admonition-content">ASUS NUC LAN and USB Expansion Module (90AR0000-P00010)</div>
    </aside></p>
<p>
<figure><picture>
          <source srcset="/NIC_hu6903087498817470336.webp" type="image/webp">
          <source srcset="/NIC_hu4204005753805133783.jpg" type="image/jpeg">
          <img src="/NIC_hu6903087498817470336.webp"alt="Image of an IO expansion"  width="1200"  height="800" />
        </picture><figcaption>
            <p>IO expansion</p>
          </figcaption></figure>
vSphere 8 supports the cards natively and you don&rsquo;t have to install any drivers.
It also supports jumbo frames, which is relevant for NSX Labs.
It is recommended to use a 2.5 GB managed switch. I am using a Mikrotik with the wonderful name <code>CRS326-4C +20G+2Q</code>.</p>

    <aside class="admonition tip">
        <div class="admonition-title">
            <div class="icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-sun">
      <circle cx="12" cy="12" r="5"></circle>
      <line x1="12" y1="1" x2="12" y2="3"></line>
      <line x1="12" y1="21" x2="12" y2="23"></line>
      <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
      <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
      <line x1="1" y1="12" x2="3" y2="12"></line>
      <line x1="21" y1="12" x2="23" y2="12"></line>
      <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
      <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
   </svg></div><b>Practical tip</b>
        </div>
        <div class="admonition-content">My experience with 2.5 Gb/s Lan has shown that it makes sense to set the ports to a fixed speed in the hypervisor and on the switch, otherwise I kept having network failures.</div>
    </aside>
<h2 id="memory-tiering">Memory Tiering</h2>
<p>Memory Tiering is very new in ESXi vSphere 8.0U3 and is still a Tech Preview.
With memory tiering, you can use up to 400% of the physical RAM. This requires a fast NVMe.
I would recommend a PCIe4 NVMe with at least 5000 MB/s read/write.
Memory Tiering stores very cold (unused RAM pages) and cold RAM pages (less than 20% used) on the NVMe (Memory Tier).
There is a wonderful <a href="https://www.vmware.com/explore/video-library/video/6360757998112" title="Explore USA">Explore Session</a> on this.</p>
<p>To enable memory tiering, you have to enter the following commands via the ESX Cli:</p>
<ul>
<li>This command turns on memory tiering</li>
</ul>
<pre tabindex="0"><code>esxcli system settings kernel set -s MemoryTiering -v TRUE
</code></pre><ul>
<li>This command selects the NVMe</li>
</ul>
<pre tabindex="0"><code>esxcli system tierdevice create -d /vmfs/devices/disks/&lt;Your NVME&gt;
</code></pre><ul>
<li>Enter the factor here (0-400%).</li>
</ul>
<pre tabindex="0"><code>esxcli system settings advanced set -o /Mem/TierNvmePct -i 400
</code></pre><p>After a reboot, you have the selected amount of additional memory.</p>

    <aside class="admonition warning">
        <div class="admonition-title">
            <div class="icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-alert-circle">
      <circle cx="12" cy="12" r="10"></circle>
      <line x1="12" y1="8" x2="12" y2="12"></line>
      <line x1="12" y1="16" x2="12.01" y2="16"></line>
   </svg></div><b>Attention</b>
        </div>
        <div class="admonition-content">The selected disk is no longer available to the ESXi.
The minimum capacity must match the selected factor.
If the disk is larger, it will still be used entirely for memory tiering.
My recommendation is to use 1 TB NVMe with 64 GB of physical RAM and 400% as the factor.
ESXi will use the NVMe evenly so that the disk doesn&rsquo;t break as quickly.</div>
    </aside>
<h2 id="using-pe-cores">Using P/E Cores</h2>
<p>Intel has introduced the big.little CPU architecture from the 12th generation of their consumer CPUs. This leads to some problems with ESXi. If the efficiency cores are activated, the ESXi starts with a PSOD (Purble Screen of Death).
Fortunately, there are a few workarounds here.</p>
<ul>
<li>Disable the E cores in the BIOS</li>
</ul>
<p>This means that you can use hyperthreading and the P Cores. However, you are clearly wasting potential here. That&rsquo;s why we don&rsquo;t want to.</p>
<ul>
<li>Use P and E cores and sacrifice hyperthreading for them</li>
</ul>
<p>My tests showed that I got significantly more performance out of my 13th generation i7 if I didn&rsquo;t use hyperthreading and only used “real” CPU cores, even if the E cores have a lower clock rate.
<a href="https://williamlam.com/2023/01/video-of-esxi-install-workaround-for-fatal-cpu-mismatch-on-feature-for-intel-12th-gen-cpus-and-newer.html">William Lam</a> has written very detailed blog articles about this, I link to him here for more information, as this article was actually only intended to be a short summary.</p>
<p>We actually only need two ESX CLI commands to make it all work.</p>
<ul>
<li>With this command, we prevent the PSOD from occurring when the ESXi boots.</li>
</ul>
<pre tabindex="0"><code>esxcli system settings kernel set -s cpuUniformityHardCheckPanic -v FALSE
</code></pre><ul>
<li>With this command, we prevent the ESXi from getting a PSOD when the VMs are switched on.</li>
</ul>
<pre tabindex="0"><code>esxcli system settings kernel set -s ignoreMsrFaults -v TRUE
</code></pre>
    <aside class="admonition tip">
        <div class="admonition-title">
            <div class="icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-sun">
      <circle cx="12" cy="12" r="5"></circle>
      <line x1="12" y1="1" x2="12" y2="3"></line>
      <line x1="12" y1="21" x2="12" y2="23"></line>
      <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
      <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
      <line x1="1" y1="12" x2="3" y2="12"></line>
      <line x1="21" y1="12" x2="23" y2="12"></line>
      <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
      <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
   </svg></div><b>Practical tip</b>
        </div>
        <div class="admonition-content">When reinstalling an ESXi server, I always switch off the E Cores, which saves me from having to manipulate the boot loader. After I have allowed memory tiering and the E/P Cores via the ESX CLI, I switch the E/P Cores back on in the BIOS.</div>
    </aside>
<p>If everything is correct, an ESX NUC of the 13th generation looks like this.

<figure><picture>
          <source srcset="/nuc_hu16244612542378356649.webp" type="image/webp">
          <source srcset="/nuc_hu988947436421053682.jpg" type="image/jpeg">
          <img src="/nuc_hu16244612542378356649.webp"alt="NUC i7"  width="1098"  height="458" />
        </picture><figcaption>
            <p>NUC i7 13th Gen with Memory Tiering and P/E Cores</p>
          </figcaption></figure></p>
]]></content>
		</item>
		
		<item>
			<title>Homelab V4</title>
			<link>https://sdn-warrior.org/posts/labv4/</link>
			<pubDate>Sat, 16 Nov 2024 20:00:00 +0000</pubDate>
			
			<guid>https://sdn-warrior.org/posts/labv4/</guid>
			<description><![CDATA[Homelab v4]]></description>
			<content type="html"><![CDATA[<h2 id="ready-for-vcf">Ready for VCF</h2>
<p>I have done a huge redesign of my Homelab.
To better test VCF scenarios, 3 new Minisforum MS-01 have been added.
These have a 13th generation i9 and are equipped with fast NVMes for memory tiering.
They also have 2x10G and 2x2.5G networking on board for various VM workloads.
Furthermore, I converted my storage from NFS to iSCSI with multipathing, which gets even more performance out of my self-built Unraid.
I manage about 2 GB/s read / 1.2 GB GB/s write and 78K IOPS (Random 4K with 32Q) in a Windows 11 VM.</p>

<figure><picture>
          <source srcset="/bench1_hu2966482509308598586.webp" type="image/webp">
          <source srcset="/bench1_hu2660173491905647916.jpg" type="image/jpeg">
          <img src="/bench1_hu2966482509308598586.webp"alt="Disk Performance iSCSI Multipathing"  width="483"  height="351" />
        </picture><figcaption>
            <p>Disk Performance iSCSI Multipathing</p>
          </figcaption></figure>

<figure><picture>
          <source srcset="/bench2_hu3593698816181082095.webp" type="image/webp">
          <source srcset="/bench2_hu10297252823232288012.jpg" type="image/jpeg">
          <img src="/bench2_hu3593698816181082095.webp"alt="IOPS iSCSI Multipathing"  width="483"  height="356" />
        </picture><figcaption>
            <p>IOPS iSCSI Multipathing</p>
          </figcaption></figure>
<p>Pretty impressive for my setup. I still have to customize the rack a bit so that I can add the 10G Mikrotik switch and clean up the VLANs from old labs.
I&rsquo;m already planning a further expansion stage though.\</p>
]]></content>
		</item>
		
		<item>
			<title>NSX Integration Fortigate</title>
			<link>https://sdn-warrior.org/posts/nsx-integration-fortigate/</link>
			<pubDate>Mon, 26 Aug 2024 19:49:23 +0100</pubDate>
			
			<guid>https://sdn-warrior.org/posts/nsx-integration-fortigate/</guid>
			<description><![CDATA[NSX and Fortigate]]></description>
			<content type="html"><![CDATA[<h2 id="nsx-integration-for-fortinet-fortigate-firewalls">NSX integration for Fortinet Fortigate Firewalls</h2>
<p>Modern SDN solutions are flexible, fast and effective. The rules of the classic perimeter firewall should be exactly the same. To make life easier, Fortinet has an NSX integration that allows us to write our perimeter rules to dynamic NSX groups.</p>
<h2 id="first-things-first">First things first</h2>
<p>The Fortinet NSX integration works via a so-called external connector. For this purpose, the Fortigate contacts the NSX Manager at regular intervals and updates the previously imported groups.
This allows us to use dynamic groups that were previously created in NSX using tags, for example.</p>
<p>First we need to configure our connector. To do this, go to the Fortigate at <em><strong>Security Fabric / External Connectors</strong></em> and click on <em><strong>Create New</strong></em>.</p>
<p><img src="01.webp" alt="Fortigate Dialog"></p>
<p>Here we need to enter our NSX Manager, if we have an NSX Manager Cluster then of course the Cluster VIP or FQDN is needed.
We can define an update interval, this determines how long it takes to update the groups on the Fortigate.
In my lab I chose 30 seconds, depending on the environment lower or higher values may make sense. In a productive environment, the certificate should always be verified.
In my homelab environment I deliberately turned this off.</p>
<p><img src="02_External-Connector.webp" alt="External Connector"></p>
<h2 id="importing-the-dynamic-nsx-groups">Importing the dynamic NSX groups</h2>
<p>The groups need to be imported via the Fortigate CLI. This is relatively easy to do for all groups and specifically for individual groups.
Groups imported this way will be automatically updated in the future. If new groups are configured in NSX, they must be imported via the CLI if they play a role in the rules.</p>

    <aside class="admonition info">
        <div class="admonition-title">
            <div class="icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-info">
      <circle cx="12" cy="12" r="10"></circle>
      <line x1="12" y1="16" x2="12" y2="12"></line>
      <line x1="12" y1="8" x2="12.01" y2="8"></line>
   </svg></div><b>Fortigate CLI</b>
        </div>
        <div class="admonition-content"><code>execute nsx group import &lt;SDN Connector&gt; &lt;VDOM&gt; &lt;group&gt;</code></div>
    </aside>
<p>If you want to import all NSX groups, you need to omit the group name in the CLI call. In the screenshot you can see me importing the <em><strong>dFG_AlpineLinux</strong></em> NSX group.
This uses an NSX tag to combine all VMs of type Alpine Linux into one security group.</p>
<p><img src="03_Group-Import.png" alt="Group-Import"></p>
<p>In the Fortigate, you can now find the group under <em><strong>Policy &amp; Objects / Addresses</strong></em> and use it like any other group in firewall policies. The NSX groups can be used not only for firewall rules, but also for policy-based routing via the SD-WAN feature.</p>
<p><img src="04_FW-Groups.webp" alt="Firewall Groups"></p>
<p>In my example, I am prohibiting the Alpine Linux VMs from accessing the Internet. The current realised group assignment can be checked at any time via <em><strong>Policy &amp; Objects&gt; / Addresses</strong></em> and a double click on the group.</p>
<p><img src="05_matched-adress.webp" alt="Matched Adrewss"></p>
<h2 id="delete-groups">Delete groups</h2>
<p>Groups need to be deleted manually. The easiest way to do this is via the Fortigate CLI. To do this, execute the following CLI command:</p>

    <aside class="admonition info">
        <div class="admonition-title">
            <div class="icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-info">
      <circle cx="12" cy="12" r="10"></circle>
      <line x1="12" y1="16" x2="12" y2="12"></line>
      <line x1="12" y1="8" x2="12.01" y2="8"></line>
   </svg></div><b>Fortigate CLI</b>
        </div>
        <div class="admonition-content"><code>execute nsx group delete &lt;VDOM&gt; &lt;filter&gt;</code></div>
    </aside>
<p>If you want to delete all groups, you can simply leave the filter empty. If a group is used in a firewall policy, it cannot be deleted and you will receive a message that the group is in use.</p>
<h2 id="testing-the-solution">Testing the solution</h2>
<p>To do this, I log on to the Alpine2 VM and check the current IP. The VM has currently been assigned 172.31.2.10. We can also find this on the Fortigate in our dFG_AlpineLinux group. I am trying to send an ICMP to the Internet, which is blocked by the Fortigate firewall as expected.</p>
<p><img src="06_test-1.webp" alt="First Test"></p>
<p>Next, I remove the Alpine Linux tag in the NSX, which ensures that the VM is no longer realised in the dFG_Alpine Linux group on the Fortigate after 30 seconds at the latest.</p>
<p><img src="07_test-2.webp" alt="Second Test"></p>
<p>Finally, I repeated my ping test. As expected, Internet access is now without any problems.</p>
<p><img src="08_icmp.png" alt="Test Number three"></p>
<h2 id="remarks">Remarks</h2>
<p>If the connection to NSX Manager is interrupted, group membership remains at the last synchronised state. This means that in highly dynamic environments, too much or too little traffic may be allowed or blocked. For this reason, the SDN connection should always be monitored. All group changes are saved in the Log SDN Connector Log of the Fortigate.</p>
<h2 id="use-cases">Use cases</h2>
<p>One conceivable scenario would be to enable a dynamic firewall for VMs that are allowed to access the Internet. This can be done in NSX using a tag and a group. Every VM that does not have a tag and is therefore not in the group will be blocked at the Fortigate perimeter firewall.</p>
<p><img src="09_firewall_rule.webp" alt="Firewall Rules"></p>
<p>The firewall rule allows everything that does not go into RFC1918 networks (private IP range). Of course, this is only a simple example and more complex setups are possible.</p>
<h2 id="additional-information">Additional information</h2>
<p><a href="https://docs.fortinet.com/document/fortigate/7.4.4/administration-guide/753961/public-and-private-sdn-connectors">Fortinet Documentation: Public and private SDN connectors</a></p>
]]></content>
		</item>
		
		<item>
			<title>NSX Identity Firewall – A Case Study With the Flavour VDI</title>
			<link>https://sdn-warrior.org/posts/nsx-idfw-vdi/</link>
			<pubDate>Fri, 02 Aug 2024 08:34:23 +0100</pubDate>
			
			<guid>https://sdn-warrior.org/posts/nsx-idfw-vdi/</guid>
			<description><![CDATA[IDFW with NSX and Windows Clients]]></description>
			<content type="html"><![CDATA[
    <aside class="admonition info">
        <div class="admonition-title">
            <div class="icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-info">
      <circle cx="12" cy="12" r="10"></circle>
      <line x1="12" y1="16" x2="12" y2="12"></line>
      <line x1="12" y1="8" x2="12.01" y2="8"></line>
   </svg></div><b>Disclaimer</b>
        </div>
        <div class="admonition-content">There are of course other ways of using the Identity Firewall. This is the way I have used it with my customers so far. Of course, the whole thing is colored by personal preferences, experiences and customer requirements so take this as an idea for your own implementations.</div>
    </aside>
<h2 id="intro">Intro</h2>
<p>A customer of mine has the use case that his entire environment must be micro-segmented. Of course, this does not stop at the VDI environment. Since my customer uses non-persistent VDIs (the VMs are deleted after each logoff), no tags can be used for the security groups. After deleting the VM, the tag would also be removed and a new VDI would have no tags and would thus be isolated. This can be solved with automation or with the Identity Firewall (a combination of both solutions is also conceivable and makes sense). In the first step, my customer opted for the Identity Firewall because it allows generic VDIs to be used and authorisations to resources can be conveniently controlled via the customer AD. In addition, each user/user group receives individual firewall rules, which corresponds to the need-to-know principle.</p>
<h3 id="you-can-use-the-identity-firewall-in-2-ways">You can use the Identity Firewall in 2 ways:</h3>
<ul>
<li>Variant 1 would be with VMware Tools and Guest Introspection.</li>
<li>Variant 2 would be with log scraping and, for example, Aria Operations for Logs as a log server.</li>
</ul>
<p>I use variant 1 because the implementation means less effort in my customer setup and we only want to protect the VDIs with Idendity Firewall.</p>
<h3 id="limitations">Limitations</h3>
<ul>
<li>No User /Group ID Support for Federation.</li>
<li>No direct integration with VDI and RDSH.</li>
<li>User-ID based rules are supported for only firewall rules.</li>
<li>No User-ID based policy for IDS/IPS and TLS Inspection.</li>
<li>Multi-User (RDSH) does not support Server Message Block (SMP) protocol.</li>
</ul>
<h3 id="supported-protocols">Supported protocols</h3>
<ul>
<li>Single user (VDI, or Non-RDSH Server) use case support – TCP, UDP</li>
<li>Multi-User (RDSH) use case support – TCP, UDP</li>
</ul>
<h3 id="supported-clients">Supported clients</h3>
<ul>
<li>Windows 8,10,11</li>
<li>Windows Server 2012 – 2022</li>
</ul>
<h3 id="what-is-needed">What is needed?</h3>
<ul>
<li>NSX in the VDI cluster</li>
<li>AD infrastructure</li>
<li>VMware tools</li>
<li>VMware Aria Operations for Logs (optional)</li>
</ul>
<h2 id="first-things-first">First things first</h2>
<p>Identity based groups can only be used as a source for a firewall rule. In addition, the rules only come into effect after a successful logon to the client. Thus, normal dFW rules must be written for all communication that happens before the user logs on. This applies, for example, to Windows AD communication. The NSX Manager synchronises cyclically with the domain controllers. The default interval is 180 minutes. If changes are made to the group membership at short notice, a manual sync can be performed. Alternatively, the sync interval can also be shortened or extended. Using network introspection, the NSX Manager recognises when a user logs on to a client and can perform matching with the AD groups and thus add the client dynamically to the security group of the IDFW firewall rule.</p>

<figure><picture>
          <source srcset="/idfw/00_idfw_hu13340240812954672917.webp" type="image/webp">
          <source srcset="/idfw/00_idfw_hu2021098600553032142.jpg" type="image/jpeg">
          <img src="/idfw/00_idfw_hu13340240812954672917.webp"alt="Idendity Firewall function"  width="1782"  height="1164" />
        </picture><figcaption>
            <p>Idendity Firewall function</p>
          </figcaption></figure>
<h2 id="getting-started">Getting started</h2>
<p>Firstly, I start by customising the golden images of the VDI and installing NSX Guest Introspection. These are not installed by default and have to be installed explicitly. You can find them under VMware Device Driver – NSX Network Introspection. File Introspection is installed automatically.</p>

<figure><picture>
          <source srcset="/idfw/01_vmwaretools_hu10766174744744931261.webp" type="image/webp">
          <source srcset="/idfw/01_vmwaretools_hu2019103790225014772.jpeg" type="image/jpeg">
          <img src="/idfw/01_vmwaretools_hu10766174744744931261.webp"alt="VMware Tools"  width="555"  height="417" />
        </picture><figcaption>
            <p>VMware Tools</p>
          </figcaption></figure>
<p>Once Guestintrospection has been successfully installed, we no longer need to do anything on our Windows clients. Next, the domain must be integrated.</p>
<p>This is done in the NSX Manager under System -&gt; Identity Firewall AD. Several domains can be entered so that multi-tenant setups can also be realised. The NSX Manager requires firewall activations and must be able to reach the domain controllers via LDAP or LDAPS. I strongly recommend the use of LDAPS. These settings can also be used to perform a manual sync or check the synchronisation status.</p>

<figure><a href="02_nsx_idfw.jpeg"><picture>
          <source srcset="/idfw/02_nsx_idfw_hu5496888141303419910.webp" type="image/webp">
          <source srcset="/idfw/02_nsx_idfw_hu10295445750369093658.jpeg" type="image/jpeg">
          <img src="/idfw/02_nsx_idfw_hu5496888141303419910.webp"alt="Idetity firewall AD settings"  width="1452"  height="465" />
        </picture></a><figcaption>
            <p>Idetity firewall AD settings (click to enlarge)</p>
          </figcaption></figure>
<p>Under LDAP Server you can set several domain controllers for the previously set up domain. The protocol used is also selected here. I use the Domain Administrator in my lab. In a productive environment, an LDAP bind user should always be used.</p>

<figure><a href="03.jpeg"><picture>
          <source srcset="/idfw/03_hu16733942988963753118.webp" type="image/webp">
          <source srcset="/idfw/03_hu2839745583667809871.jpeg" type="image/jpeg">
          <img src="/idfw/03_hu16733942988963753118.webp"alt="LDAP Server settings"  width="1160"  height="460" />
        </picture></a><figcaption>
            <p>LDAP Server settings (click to enlarge)</p>
          </figcaption></figure>

    <aside class="admonition attention">
        <div class="admonition-title">
            <div class="icon"><svg xmlns="http://www.w3.org/2000/svg" class="feather feather-link" width="24" height="24" viewBox="0 0 24 24"
      fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
      <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path>
      <path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path>
   </svg></div><b>Attention</b>
        </div>
        <div class="admonition-content">If LDAPS is used, NSX imports the SHA thumbprint of the domain controller certificate. As the certificate is usually renewed automatically after 2 years at the latest, NSX loses the trust with the domain controller. In this case, the trust must be established manually. To do this, delete the thumbprint and reconnect to the bind user.  It has proven to be practical to monitor certificate expiry times and to enter at least 2 domain controllers that exchange their certificates with a 4-week time lag. If the trust fails completely, no more identity rules are applied and the default firewall rule comes into effect. In practice, this should be an any/any default drop and log rule.</div>
    </aside>
<p>After we have successfully setup and synchronised our domain, we only need to activate the Identity Firewall. By default, this feature is disabled (it is a free feature that is available with the NSX Firewall VCF add-on). To activate the Identity Firewall, go to Security -&gt; Distributed Firewall -&gt; Settings -&gt; Identity Firewall Settings and activate the identity firewall service button. Then we select the cluster on which we want to activate the service. Now we can get started.</p>

<figure><a href="04.webp"><picture>
          <source srcset="/idfw/04_hu12560464345806440685.webp" type="image/webp">
          <source srcset="/idfw/04_hu11069272111777865810.jpg" type="image/jpeg">
          <img src="/idfw/04_hu12560464345806440685.webp"alt="Distributed Firewall Settings"  width="1699"  height="792" />
        </picture></a><figcaption>
            <p>Distributed Firewall Settings (click to enlarge)</p>
          </figcaption></figure>
<h2 id="identity-firewall-rules">Identity Firewall Rules</h2>
<p>A general recommendation that applies to all firewalls is to think about a naming concept. At my customer, we have different name prefixes for the various security groups in NSX or in the other firewalls. For my part, I prefer the following naming convention, for example:</p>
<p>Distributed firewall groups start with dFW_XXX, an LDAP backed security group with dFWU_XXX (the U stands for user). For a group that contains an NSX segment it would be dFWS_XXX and for the gateway firewall a gWF_XXX and so on.</p>
<p>So we create our first LDAP user group. As with any group, we can do this either when creating the rules or in the inventory under Groups. The process is the same as for a normal Distributed Firewall Group, except that we don’t use tags but Distinguished Names, which can be conveniently selected or filtered from the synchronised AD elements.</p>

<figure><a href="05.webp"><picture>
          <source srcset="/idfw/05_hu16308509706422833611.webp" type="image/webp">
          <source srcset="/idfw/05_hu13581943069617237554.jpg" type="image/jpeg">
          <img src="/idfw/05_hu16308509706422833611.webp"alt="Security Groups"  width="1164"  height="977" />
        </picture></a><figcaption>
            <p>Security Groups (click to enlarge)</p>
          </figcaption></figure>
<p>I also need at least one segment group and at least one group containing my target servers. The target servers are assigned to their groups via tags. The same applies to the segment group. To do this, I set one or more tags on the overlay segment and use this tag as a condition for group membership.</p>

<figure><a href="06.webp"><picture>
          <source srcset="/idfw/06_hu566842021385844307.webp" type="image/webp">
          <source srcset="/idfw/06_hu1670820342853564742.jpg" type="image/jpeg">
          <img src="/idfw/06_hu566842021385844307.webp"alt="Member selection"  width="1156"  height="964" />
        </picture></a><figcaption>
            <p>Member selection (click to enlarge)</p>
          </figcaption></figure>
<p>Our goal will be that we authorise our users assigned to dFWU_UserGroup1 to access our fileserver with SMB and the users of the group dFWU_UserGroup2 must not receive any authorisation. I have two domain users in my lab, User1 is in the AD group assigned to dFWU_UserGroup1 and User2 is only assigned to dFWU_UserGroup2.</p>
<h2 id="creating-the-firewall-rules">Creating the firewall rules</h2>
<p>For each identity firewall rule that allows traffic from a group of users to a destination, there must be a corresponding distributed firewall rule that allows traffic from a group of computers to the same destination specified in the identity firewall rule. We therefore need two firewall rules.</p>

<figure><a href="07.webp"><picture>
          <source srcset="/idfw/07_hu14371369714500250916.webp" type="image/webp">
          <source srcset="/idfw/07_hu7421683099691664086.jpg" type="image/jpeg">
          <img src="/idfw/07_hu14371369714500250916.webp"alt="Firewall Rules"  width="1630"  height="226" />
        </picture></a><figcaption>
            <p>Firewall Rules (click to enlarge)</p>
          </figcaption></figure>
<p>The first rule is pretty straight forward, as source we have our dFWU_UserGroup1, the target is our dFG_Fileserver and the service is SMB. The Applied To Field is even more important than usual for the Identity Firewall. We may only apply this rule to our VDIs. Since my customer has different pools that are named according to a specific naming scheme, I can further restrict the scope based on the computer name. Each pool has different rules and we only want the rules to be realised on VMs where they are needed. The second rule is a bit more interesting. As a source, we have our VDI segment or segments. As in the first rule, the target is our file server. Logically, the service is also the same.</p>

    <aside class="admonition attention">
        <div class="admonition-title">
            <div class="icon"><svg xmlns="http://www.w3.org/2000/svg" class="feather feather-link" width="24" height="24" viewBox="0 0 24 24"
      fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
      <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path>
      <path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path>
   </svg></div><b>Attention</b>
        </div>
        <div class="admonition-content">It is important that the Apply To field can only be on the file servers or on the target that we want to enable. If we were to use the dFG_VDI_GroupA or the dFGS_VDI group here, for example, then the entire Identity Firewall Rule is cancelled out!</div>
    </aside>
<h2 id="testing-and-verifying">Testing and verifying</h2>
<p>The test is considered successful if User1 on the TestVDI can establish a successful SMB connection to the file server. If User2 is used instead of User1, the traffic to the file server must be blocked by the firewall.</p>
<p>For testing, I log in to my VDI with the credentials of User1 and perform a TestNetConnection with Powershell. This is a simple and quick way to test TCP connections. I also open a share on the file server.</p>

<figure><a href="08.webp"><picture>
          <source srcset="/idfw/08_hu11227237490731539297.webp" type="image/webp">
          <source srcset="/idfw/08_hu11206926558301644657.jpg" type="image/jpeg">
          <img src="/idfw/08_hu11227237490731539297.webp"alt="Network test user 1"  width="1072"  height="627" />
        </picture></a><figcaption>
            <p>Network test user 1 (click to enlarge)</p>
          </figcaption></figure>
<p>The test was successful, both the TNC command and the actual opening of the file share worked. Now I’m running the same test on the same VDI (after it was recreated, because non-persistent VDI), only this time I’m using User2, which has no explicit firewall rules and is therefore blocked by my default cleanup rule. As expected, the traffic was successfully blocked.</p>

<figure><a href="09.png"><picture>
          <source srcset="/idfw/09_hu2338535551923653157.webp" type="image/webp">
          <source srcset="/idfw/09_hu1977019721799240822.jpg" type="image/jpeg">
          <img src="/idfw/09_hu2338535551923653157.webp"alt="Network test user 2"  width="993"  height="499" />
        </picture></a><figcaption>
            <p>Network test user 2 (click to enlarge)</p>
          </figcaption></figure>
<h2 id="lessons-learned">Lessons learned</h2>
<p>This is where I would like to add a few more thoughts on the subject. Troubleshooting is more difficult in practice than I thought. Tools such as NSX Traceflow cannot be used because you cannot add an AD user to the request. This means that the traffic in the traceflow is always dropped or the identity rule is maybe configured incorrectly.</p>
<p>But there is light at the end of the tunnel. In NSX 4.X there is a session view of the active IDFW user session under Security -&gt; Security Overview -&gt; Configuration. All active sessions, UserIDs and VMs are displayed here, as well as the source of the information.</p>

<figure><a href="10.png"><picture>
          <source srcset="/idfw/10_hu10916967050054961034.webp" type="image/webp">
          <source srcset="/idfw/10_hu10504170639179592038.jpg" type="image/jpeg">
          <img src="/idfw/10_hu10916967050054961034.webp"alt="Active Sessions"  width="979"  height="278" />
        </picture></a><figcaption>
            <p>Active Sessions (click to enlarge)</p>
          </figcaption></figure>
<p>Next tip would be to always check the sync status with the AD. Ask your AD admin when the user was added to the group. If the user has several accounts, ask for the user name used. Experience has shown that this is where most problems occur.</p>
<p>Use a syslog server and check exactly with which rule ID the traffic was discarded. Have all deny rules logged.</p>
<p>Not all rules can be implemented as Identity Firewall Rules. The Windows domain basic communication can only be enabled via a classic set of rules, as no Identity Firewall rules are active for the VM without an active user session.</p>
<h2 id="important-things-to-know">Important things to know</h2>
<p>Never install Guest Introspection on a target. If a user has remote desktop permissions on the target and guest introspection is active there, then the target receives all of the user’s firewall rules. This can lead to unwanted firewall permissions.</p>
<p>If targets outside of NSX are addressed, such as a NAS or legacy infrastructure, a second rule is not required (unless the gateway firewall is also used). In this case, the distributed firewall will only check the traffic at the source VM.</p>
<p>Any change on a domain, including a domain name change, will trigger a full sync with Active Directory. Because a full sync can take a long time, i recommend syncing during off-peak or non-business hours.</p>
<p>MutiUser setups only work with RDSH (Remote Desktop Session Hosts) which requires a special configuration. Otherwise, if several users are logged on to a client at the same time, this leads to unwanted behavior and, in the worst case, to unwanted firewall permissions.</p>
<h2 id="conclusion">Conclusion</h2>
<p>The Identity Firewall is a wonderful extension of the Distributed firewall and should be treated as such. Used correctly, it provides a very nice way to manage and delegate firewall permissions dynamically and centrally for individual users or user groups. It enables generic VDIs that can be used for different purposes depending on the user. This can reduce the number of VDI pools required, which in turn makes it easier to manage the customers VDI environment. The RBAC concept is even more tightly bound to the firewall policies and AD tiering can also be enforced via the firewall. And best of all, this great feature is included in the NSX Firewall license. I would recommend every NSX firewall administrator to take a closer look at the Identity Firewall.</p>
<h2 id="additional-resources">Additional resources</h2>
<p><a href="https://docs.vmware.com/en/VMware-NSX/4.1/administration/GUID-9CD3FC21-9ED4-4FB3-9E19-67A7C4D1F53E.html">VMware Docs Idendity Firewall</a></p>
]]></content>
		</item>
		
		<item>
			<title>NSX 4.X Certificate exchange of the NSX Manager</title>
			<link>https://sdn-warrior.org/posts/nsx-cert-exchange/</link>
			<pubDate>Fri, 05 Apr 2024 23:22:00 +0100</pubDate>
			
			<guid>https://sdn-warrior.org/posts/nsx-cert-exchange/</guid>
			<description><![CDATA[Exchange your NSX Manager certificates]]></description>
			<content type="html"><![CDATA[<h1 id="nsx-4x-certificate-exchange-of-the-nsx-manager">NSX 4.X Certificate exchange of the NSX Manager</h1>
<h1 id="certificate-creation">Certificate creation</h1>
<p>First of all, we need a CSR request. This can be created with OPENSSL. It is important that the key is also exported. You can either create 4 individual certificates (VIP and the three manager nodes) or a SAN certificate with all DNS and IP names of the manager nodes. The easiest way is to carry out the request on a manager node. To do this, I create an openssl config file with VIM.</p>
<pre tabindex="0"><code>[req]
default_bits = 4096
default_md = sha256
days = 365
distinguished_name = req_distinguished_name
req_extensions = v3_req
prompt = no
 
[req_distinguished_name]
C   = DE
ST  = RLP
L   = NW
O   = Land RLP
OU  = sdnwarrior
CN  = nsxm0001.lab.home
emailAddress = mail@lab.home
 
[v3_req]
subjectAltName = @sans
 
[sans]
DNS.1 = nsxm0001.lab.home
DNS.2 = nsxm0002.lab.home
DNS.3 = nsxm0003.lab.home
DNS.4 = nsxm0004.lab.home
IP.1 = 192.168.12.110
IP.2 = 192.168.12.111
IP.3 = 192.168.12.112
IP.4 = 192.168.12.113
</code></pre><p>The CSR is generated with the following command:</p>
<pre tabindex="0"><code>openssl req -new -newkey rsa:4096 -nodes -keyout nsxm0001.key -out nsxm0001.csr -config opnssl.cnf
</code></pre><p>Two files are generated, a private key file and the actual request, which must be submitted to the CA.</p>

    <aside class="admonition attention">
        <div class="admonition-title">
            <div class="icon"><svg xmlns="http://www.w3.org/2000/svg" class="feather feather-link" width="24" height="24" viewBox="0 0 24 24"
      fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
      <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path>
      <path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path>
   </svg></div><b>Attention</b>
        </div>
        <div class="admonition-content">The CA must issue the certificate with the extension basicConstraints = cA:FALSE, otherwise the certificate cannot be used. With a Windows CA, this must be explicitly permitted in the template. If the extension is missing, the certificate validation will fail with an error message that the certificate key does not match the certificate.</div>
    </aside>
<h2 id="import-certificate">Import certificate</h2>
<p>The certificate can be imported in the NSX Manager under System &gt; Certificates &gt; Import. Here it must be ensured that the service certificate slider is set to NO. The complete certificate chain is also required. The certificate chain must be in the industry standard order of ‘certificate – intermediate – root.</p>

<figure><picture>
          <source srcset="/nsx-cert/01_hu8747713429439574210.webp" type="image/webp">
          <source srcset="/nsx-cert/01_hu1064617936367254404.jpg" type="image/jpeg">
          <img src="/nsx-cert/01_hu8747713429439574210.webp"alt="NSX Cert"  width="582"  height="924" />
        </picture><figcaption>
            <p>Import NSX Cert</p>
          </figcaption></figure>
<p>After the import, the certificate can be validated using an API request.
API calls may vary depending on the NSX-T versions, in my example NSX version 4.1.2.3 is used.</p>
<pre tabindex="0"><code>GET https://&lt;nsx-mgr&gt;/api/v1/trust- management/certificates/&lt;cert-id&gt;?action=validate
</code></pre><h2 id="exchange-of-certificates">Exchange of certificates</h2>
<p>An API request must be executed for each manager node and for the VIP. This requires the certificate ID and the manager node ID. Both can be copied from the WebGUI or requested via API Get Requests.</p>
<p>The following API call is used to exchange the Manager Node certificate:</p>
<pre tabindex="0"><code>POST /api/v1/trust-management/certificates/&lt;cert- id&gt;?action=apply_certificate&amp;service_type=API&amp;node_id=&lt;node- id&gt;
</code></pre><p>The following API call is used to exchange the cluster VIP certificate:</p>
<pre tabindex="0"><code>POST /api/v1/trust-management/certificates/&lt;cert- id&gt;?action=apply_certificate&amp;service_type=MGMT_CLUSTER
</code></pre><p>After replacing the certificates, you should close all browser windows and log in to the NSX Manager again. The certificate should now have been successfully replaced.</p>
<h2 id="further-resources">Further resources:</h2>
<p><a href="https://docs.vmware.com/en/VMware-NSX-T-Data-Center/3.2/administration/GUID-50C36862-A29D-48FA-8CE7-697E64E10E37.html">VMware Administration Handbook</a></p>
]]></content>
		</item>
		
	</channel>
</rss>
